{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andandandand/practical-computer-vision/blob/main/notebooks/Part_2_Zero_Shot_Classification_Aerial_Images_CLIP_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellUniqueIdByVincent": "2554b",
        "id": "5JsshEd-R9nD"
      },
      "source": [
        "# Intro to Dataset Curation with FiftyOne and CLIP (Part 2 of 2)\n",
        "\n",
        "In this notebook we explore the use of:\n",
        "\n",
        "* FiftyOne's dataset curation SDK and visualization app\n",
        "* Multimodal embeddings (text + image) from the [CLIP](https://arxiv.org/pdf/2103.00020) model to produce labels for images (aka Zero-shot classification)\n",
        "\n",
        "We label dataset of aerial images from Google Earth View. This is the output of deduplication that we performed [in the previous notebook](https://github.com/andandandand/practical-computer-vision/blob/main/notebooks/Intro_Dataset_Curation_Deduplicate_Aerial_Images.ipynb).\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/clip_ensemble_labels.png?raw=true)\n",
        "\n",
        "We will use an [ensemble of CLIP models](https://docs.voxel51.com/tutorials/zero_shot_classification.html) to produce majority vote labels for the images.\n",
        "\n",
        "* OpenAI's base CLIP model\n",
        "* CLIPA\n",
        "* DFN\n",
        "* EVA-CLIP\n",
        "* MetaCLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install FiftyOne and openCLIP\n"
      ],
      "metadata": {
        "id": "Au2nLBNYSOwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone==1.5.2 > /dev/null"
      ],
      "metadata": {
        "id": "S2Kmj1sXSFCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open_clip_torch==2.32.0 > /dev/null"
      ],
      "metadata": {
        "id": "8YyttcBaU33p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "FUC6LcQhSKTy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellUniqueIdByVincent": "11255",
        "id": "d8OdDCy1R9nE"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "tqapJbH6SYRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "id": "FjvcXfJySdVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define paths for local folders"
      ],
      "metadata": {
        "id": "E36u1r-qSz2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellUniqueIdByVincent": "d4927",
        "id": "sZXGUMgFR9nE"
      },
      "outputs": [],
      "source": [
        "parent_path = Path(\"/gdrive/MyDrive/fiftyone-getting-started-datasets/aerial-images\")\n",
        "os.listdir(parent_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aerial_images_path = parent_path / 'aerial_images_without_duplicates/data'\n",
        "len(os.listdir(aerial_images_path))"
      ],
      "metadata": {
        "id": "AMAb_KcrTQVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJLaQP23R9nE"
      },
      "outputs": [],
      "source": [
        "# Check if dataset exists and delete it (dataset names are unique in FiftyOne)\n",
        "dataset_name = \"aerial-images-tagged\"\n",
        "\n",
        "if dataset_name in fo.list_datasets():\n",
        "    print(f\"Dataset '{dataset_name}' exists. Deleting...\")\n",
        "    fo.delete_dataset(dataset_name)\n",
        "    print(f\"Dataset '{dataset_name}' deleted.\")\n",
        "else:\n",
        "    print(f\"Dataset '{dataset_name}' does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellUniqueIdByVincent": "cecd0",
        "id": "6ugUj4QKR9nE"
      },
      "outputs": [],
      "source": [
        "# Create the dataset\n",
        "dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=aerial_images_path,\n",
        "    dataset_type=fo.types.ImageDirectory,\n",
        "    name=dataset_name,\n",
        "    persistent=True\n",
        ")\n",
        "\n",
        "dataset.compute_metadata(overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellUniqueIdByVincent": "a6ae5",
        "id": "4SBkTAMSR9nE"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellUniqueIdByVincent": "188a3",
        "id": "JNKPVrFWR9nE"
      },
      "outputs": [],
      "source": [
        "text_prompt = 'an aerial photo of'\n",
        "\n",
        "classes = ['a river',\n",
        "           'a river in the jungle',\n",
        "           'a river next to an urban area',\n",
        "           'a river delta merging with the sea',\n",
        "           'a body of water',\n",
        "           'a body of water next to an urban area',\n",
        "           'a jungle',\n",
        "           'a forest',\n",
        "           'a river in a forest',\n",
        "           'a farmland',\n",
        "           'a coast',\n",
        "           'a desert',\n",
        "           \"a harbor next to a desert\",\n",
        "           'a desert next to a river',\n",
        "           'a desert next to a body of water',\n",
        "           'a desert next to an urban area',\n",
        "           'a desert next to a coast',\n",
        "           'a desert next to a forest',\n",
        "           'terrain covered by snow',\n",
        "           'a city',\n",
        "           'an airport',\n",
        "           'a sports stadium',\n",
        "           'an urban area',\n",
        "           'a city next to the coast',\n",
        "           'military planes parked next to each other',\n",
        "           'containers in a harbor',\n",
        "           'ships in the ocean',\n",
        "           'the ocean',\n",
        "           'a beach',\n",
        "           'a beach next to an urban area',\n",
        "           'a mountainscape',\n",
        "           'a refinery',\n",
        "           'ships and containers in a harbor',\n",
        "           'ships and boats in a harbor, next to an urban area',\n",
        "           'dense vegetation next to a desert',\n",
        "           'an island',\n",
        "           'a harbor next to an urban area',\n",
        "           'antartica or artic area, ice and water',\n",
        "           'railroad tracks',\n",
        "           'a train station',\n",
        "           'a highway',\n",
        "           'farming terraces',\n",
        "           'an oil rig in the sea']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellUniqueIdByVincent": "c45fa",
        "id": "xxqAgMglR9nF"
      },
      "outputs": [],
      "source": [
        "clip_model = foz.load_zoo_model(\n",
        "    \"clip-vit-base32-torch\",\n",
        "    text_prompt=text_prompt,\n",
        "    classes=classes,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjZUrvc5R9nF"
      },
      "outputs": [],
      "source": [
        "print(f\"The model is loaded on {clip_model._device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellUniqueIdByVincent": "81917",
        "id": "to9IjmnKR9nF"
      },
      "outputs": [],
      "source": [
        "dataset.apply_model(\n",
        "    model=clip_model,\n",
        "    label_field=\"clip_zero_shot_classification\",\n",
        "    # This is how many samples we will show to the model at once\n",
        "    batch_size=32,\n",
        "    store_logits=True,\n",
        "    progress_bar=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellUniqueIdByVincent": "ce1f8",
        "id": "_A1BpHrQR9nF"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset, auto=False)\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellUniqueIdByVincent": "d68ff",
        "id": "nlIN2dDKR9nF"
      },
      "source": [
        "## CLIP-variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9rjsWANR9nF"
      },
      "outputs": [],
      "source": [
        "open_clip_args = {\n",
        "    \"clipa\": {\n",
        "        \"clip_model\": 'hf-hub:UCSC-VLAA/ViT-L-14-CLIPA-datacomp1B',\n",
        "        \"pretrained\": '',\n",
        "        },\n",
        "    \"dfn\": {\n",
        "        \"clip_model\": 'ViT-B-16-quickgelu',\n",
        "        \"pretrained\": 'dfn2b',\n",
        "        },\n",
        "    \"eva02_clip\": {\n",
        "        \"clip_model\": 'EVA02-B-16',\n",
        "        \"pretrained\": 'merged2b_s8b_b131k',\n",
        "        },\n",
        "    \"metaclip\": {\n",
        "        \"clip_model\": 'ViT-B-32-quickgelu',\n",
        "        \"pretrained\": 'metaclip_400m',\n",
        "        },\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te_EWKViR9nF"
      },
      "outputs": [],
      "source": [
        "for name, args in open_clip_args.items():\n",
        "    print(f\"Applying {name} model\")\n",
        "    clip_model = args[\"clip_model\"]\n",
        "    pretrained = args[\"pretrained\"]\n",
        "    model = foz.load_zoo_model(\n",
        "        \"open-clip-torch\",\n",
        "        clip_model=clip_model,\n",
        "        pretrained=pretrained,\n",
        "        classes=classes,\n",
        "        store_logits=True,\n",
        "        batch_size=32,\n",
        "        text_promopt=text_prompt\n",
        "    )\n",
        "\n",
        "    dataset.apply_model(model, label_field=name, save_logits=True)\n",
        "    session.refresh()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_XYBRc5R9nF"
      },
      "outputs": [],
      "source": [
        "session.view = dataset.view()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr64yZhgR9nF"
      },
      "outputs": [],
      "source": [
        "dataset.get_field_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qADxNNWgR9nF"
      },
      "outputs": [],
      "source": [
        "sample = dataset.first()\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPKzQ1hlR9nF"
      },
      "outputs": [],
      "source": [
        "\n",
        "predictions_fields = ['clip_zero_shot_classification', 'clipa', 'dfn', 'eva02_clip', 'metaclip']\n",
        "for sample in dataset:\n",
        "    sample_labels = []\n",
        "    confidences = []\n",
        "    for prediction_field in predictions_fields:\n",
        "       sample_labels.append(sample[prediction_field].label)\n",
        "       confidences.append(sample[prediction_field].confidence)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    labels_array = np.array(sample_labels)\n",
        "    confidences_array = np.array(confidences)\n",
        "\n",
        "    # Find unique labels and their counts\n",
        "    unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "\n",
        "    # Find the maximum count and get all labels with that count\n",
        "    max_count = np.max(counts)\n",
        "    most_common_mask = counts == max_count\n",
        "    most_common_labels = unique_labels[most_common_mask]\n",
        "\n",
        "    most_common_label = most_common_labels[0]\n",
        "    #print(f\"Most common label: {most_common_label}\")\n",
        "\n",
        "    # Get indices for ONLY the first most common label\n",
        "    indices = np.where(labels_array == most_common_label)[0]\n",
        "\n",
        "    # Calculate mean confidence for this specific label only\n",
        "    conf_mean = np.mean(confidences_array[indices])\n",
        "\n",
        "    # Save the most common label and its mean confidence as a Classification\n",
        "    sample['most_common_label'] = fo.Classification(label=most_common_label, confidence=conf_mean)\n",
        "    sample.save()\n",
        "\n",
        "session.refresh()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMEZL0lZR9nF"
      },
      "source": [
        "## Visualize the dataset with consensus labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U55FP8XuR9nF"
      },
      "outputs": [],
      "source": [
        "# Launch the FiftyOne app to visualize the dataset\n",
        "session.view = dataset.view()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZlyu--DR9nF"
      },
      "outputs": [],
      "source": [
        "# Export to disk\n",
        "export_dir = str(parent_path / \"data/tagged_aerial_images\")\n",
        "dataset.export(\n",
        "    export_dir=export_dir,\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    export_media=True,  # Include media files,\n",
        "    overwrite=True  # Overwrite existing files if they exist\n",
        ")\n",
        "print(f\"Dataset exported to: {export_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGz7793zR9nF"
      },
      "outputs": [],
      "source": [
        "# Import from disk\n",
        "imported_dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=export_dir,\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9Qv0EdnR9nF"
      },
      "outputs": [],
      "source": [
        "# Test that your custom field works correctly\n",
        "print(\"Testing most_common_label field:\")\n",
        "for i, sample in enumerate(imported_dataset.take(3)):\n",
        "    if hasattr(sample, 'most_common_label'):\n",
        "        print(f\"Sample {i+1}: {sample.most_common_label.label} (conf: {sample.most_common_label.confidence:.3f})\")\n",
        "    else:\n",
        "        print(f\"Sample {i+1}: No most_common_label field\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTjDhl-GR9nF"
      },
      "source": [
        "## Suggested Exercise\n",
        "\n",
        "* Now that the images have been labeled and deduplicated, you can use the FiftyOne app to visualize the dataset. Try adding new features such as [clustering](https://docs.voxel51.com/tutorials/clustering.html) and [representativeness](https://docs.voxel51.com/brain.html#brain-image-representativeness) based on the embeddings."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "vincent": {
      "sessionId": "1a9452eaf56410e78d5466f1_2025-06-16T13-49-07-353Z"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}