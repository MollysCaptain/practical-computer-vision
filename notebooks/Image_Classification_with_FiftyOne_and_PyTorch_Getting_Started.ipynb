{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4337298cb93d4c69ae152b0ebce96b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1a1a85c56c747918d1045f40300c1f4",
              "IPY_MODEL_57df194fa5b54a5fb97ea4d3e6978ac7",
              "IPY_MODEL_52f963f35e3d49c5b66ee88eb5b81a25"
            ],
            "layout": "IPY_MODEL_5e4aee878da64108b2fd8a457807ae44"
          }
        },
        "a1a1a85c56c747918d1045f40300c1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb5295ef908c45279966b6ba0a2c1cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_4d135b9bc5a54cfd980f7a653f7d920e",
            "value": "Epochs completed: 100%| "
          }
        },
        "57df194fa5b54a5fb97ea4d3e6978ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c131465b27bb435aacf4829d686a5f75",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ae075891fdc431387c5b99052252054",
            "value": 500
          }
        },
        "52f963f35e3d49c5b66ee88eb5b81a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47c6c1158c804c4ca45fd296152ded7a",
            "placeholder": "​",
            "style": "IPY_MODEL_d7e09b2817e944c897a05c14116e49c2",
            "value": " 500/500 [00:04]"
          }
        },
        "5e4aee878da64108b2fd8a457807ae44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb5295ef908c45279966b6ba0a2c1cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d135b9bc5a54cfd980f7a653f7d920e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c131465b27bb435aacf4829d686a5f75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ae075891fdc431387c5b99052252054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47c6c1158c804c4ca45fd296152ded7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e09b2817e944c897a05c14116e49c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2e86d3c470e4ded988b11ed9b139e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4abff2f37a6f4214861f96df674c2ce5",
              "IPY_MODEL_5561f4460ab2483295489c0512f185c3",
              "IPY_MODEL_8b3287df74b04bcb9f83c02004ceff88"
            ],
            "layout": "IPY_MODEL_ec8999eb8ab448fb83fc6b717f1f7160"
          }
        },
        "4abff2f37a6f4214861f96df674c2ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08b1ca31aef24559b76f67b149f89ed5",
            "placeholder": "​",
            "style": "IPY_MODEL_4c151c74ac354bf7bef901dae0718b2f",
            "value": "Epochs completed: 100%| "
          }
        },
        "5561f4460ab2483295489c0512f185c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17ae3862843448da8b3e28fed43ccf35",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_429186289f9f41d2a9dea3d284ac42f5",
            "value": 200
          }
        },
        "8b3287df74b04bcb9f83c02004ceff88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c33abcb029f4115a631cc6f9bf6776b",
            "placeholder": "​",
            "style": "IPY_MODEL_754401b489744ce9a607d340b7acbd3f",
            "value": " 200/200 [00:08]"
          }
        },
        "ec8999eb8ab448fb83fc6b717f1f7160": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b1ca31aef24559b76f67b149f89ed5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c151c74ac354bf7bef901dae0718b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17ae3862843448da8b3e28fed43ccf35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "429186289f9f41d2a9dea3d284ac42f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c33abcb029f4115a631cc6f9bf6776b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "754401b489744ce9a607d340b7acbd3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andandandand/practical-computer-vision/blob/main/notebooks/Image_Classification_with_FiftyOne_and_PyTorch_Getting_Started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with Image Classification using FiftyOne and PyTorch\n",
        "\n",
        "## Zero-shot Classification with CLIP and Supervised Learning with LeNet-5\n",
        "\n",
        "\n",
        "### Who this Is for\n",
        "This tutorial is designed for computer vision practitioners and data scientists who want to master image classification workflows using FiftyOne. Whether you're new to computer vision or experienced with other tools, you'll learn how to leverage FiftyOne's powerful capabilities for dataset curation, model evaluation, and visual analysis.\n",
        "\n",
        "This tutorial is appropriate for any level of computer vision knowledge. By the end of this tutorial, you'll be able to quickly identify mislabeled samples, compare classification models, create meaningful embeddings, and seamlessly move between FiftyOne and PyTorch workflows.\n",
        "\n",
        "### Assumed Knowledge\n",
        "We assume familiarity with basic Python programming and fundamental machine learning concepts. Knowledge of PyTorch is helpful but not required,  we'll explain the key concepts as we go. This tutorial is recommended for beginners to intermediate practitioners in computer vision.\n",
        "\n",
        "### Time to complete\n",
        "90-120 minutes\n",
        "\n",
        "### Required packages\n",
        "FiftyOne, PyTorch, and several other packages are required. You can install them with:\n",
        "\n",
        "```bash\n",
        "pip install fiftyone==1.5.2 torch torchvision numpy\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JO75fXaFtdvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Content Overview\n",
        "\n",
        "### 1. MNIST Dataset Exploration with FiftyOne\n",
        "\n",
        "Understand the MNIST dataset structure. Load the test split into FiftyOne. Compute and visualize metadata. Explore data distributions using aggregations and the FiftyOne App.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Loading datasets from FiftyOne Dataset Zoo\n",
        "*   Computing image metadata\n",
        "*   Using FiftyOne aggregations for data statistics\n",
        "*   Visualizing dataset distributions\n",
        "\n",
        "### 2. Image Embeddings with CLIP\n",
        "\n",
        "Generate image embeddings for the test dataset using a pre-trained CLIP model. Visualize these high dimensional vectors in 2D using PCA and UMAP to understand image similarity.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Loading pre-trained models from FiftyOne Model Zoo\n",
        "*   Computing image embeddings with CLIP\n",
        "*   Assigning embeddings to dataset samples\n",
        "*   Dimensionality reduction: PCA and UMAP\n",
        "*   Visualizing embedding plots in FiftyOne\n",
        "\n",
        "### 3. Dataset Analysis using CLIP Embeddings\n",
        "\n",
        "Continue analysis of the CLIP embeddings on the test dataset. Explore dataset clustering concepts. Compute and examine sample uniqueness and representativeness based on CLIP embeddings.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Introduction to clustering with embeddings\n",
        "*   Creating a similarity index\n",
        "*   Identifying outliers and representative samples\n",
        "\n",
        "### 4. Zero-Shot Classification with CLIP\n",
        "\n",
        "Perform image classification on the test dataset using CLIP without task specific training. Evaluate CLIP's performance using FiftyOne's tools.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Zero-shot classification principles\n",
        "*   Text prompts for classification\n",
        "*   Applying a model to a FiftyOne dataset\n",
        "*   Evaluating classification results including accuracy and confusion matrix\n",
        "\n",
        "### 5. Supervised Classification: LeNet-5 with PyTorch\n",
        "\n",
        "Build and train a LeNet-5 convolutional neural network from scratch using PyTorch. Prepare the MNIST training data. Implement the training loop and validation procedures.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   LeNet-5 architecture\n",
        "*   Defining a subclass of PyTorch's `nn.Module`\n",
        "*   Splitting FiftyOne data for training and validation\n",
        "*   Creating custom PyTorch Datasets from FiftyOne views\n",
        "*   Data normalization through mean and standard deviation computation\n",
        "*   PyTorch DataLoaders\n",
        "*   Defining loss functions and optimizers\n",
        "*   Training loops and model checkpointing\n",
        "\n",
        "### 6. LeNet-5 Model Evaluation on Test Data\n",
        "\n",
        "Apply the trained LeNet-5 model to the MNIST test set. Store predictions in FiftyOne. Evaluate its performance. Analyze prediction characteristics, including hardness and mistakenness.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Applying a PyTorch model to a FiftyOne dataset\n",
        "*   Storing predictions, confidence, and logits\n",
        "*   Evaluating classification performance\n",
        "*   Analyzing prediction confidence distributions\n",
        "*   Computing sample hardness and mistakenness\n",
        "\n",
        "### 7. Analysis of LeNet-5 Learned Features using Training Data\n",
        "\n",
        "Extract embeddings from the trained LeNet-5 model using the training data. Compute and visualize these embeddings. Analyze uniqueness and representativeness based on LeNet's learned features. Identify misclassifications within the training set.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Extracting embeddings from intermediate PyTorch model layers\n",
        "*   Storing custom model embeddings in FiftyOne\n",
        "*   Visualizing custom embeddings with PCA and UMAP\n",
        "*   Analyzing uniqueness and representativeness of training samples\n",
        "*   Evaluating model performance on training data\n",
        "*   Identifying false positives and false negatives in training data\n",
        "\n",
        "### 8. Data Augmentation Concepts for MNIST\n",
        "\n",
        "Understand effective data augmentation strategies for the MNIST dataset. Learn about geometric transformations and elastic deformations. Discuss augmentations to avoid. This section provides a conceptual discussion.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Rationale for data augmentation\n",
        "*   Geometric and elastic transformations suitable for MNIST\n",
        "*   Best practices and pitfalls in augmentation"
      ],
      "metadata": {
        "id": "ISDyu809KcxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "IQnUrahBi1Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove > /dev/null if you encounter errors during installation\n",
        "!pip install fiftyone==1.5.2 > /dev/null"
      ],
      "metadata": {
        "id": "W9NFRcctjBMc"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FiftyOne Plug-ins"
      ],
      "metadata": {
        "id": "iG3_ZBDEjNwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also install FiftyOne plugins for model evaluation and data augmentation:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yRlSg9rPlT2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plug-in to evaluate the performance of our classification models\n",
        "!fiftyone plugins download \\\n",
        "    https://github.com/voxel51/fiftyone-plugins \\\n",
        "    --plugin-names @voxel51/evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzHKYfmnjRYa",
        "outputId": "dbcd24d2-a4a0-472d-a08a-4bef4cf3dc78"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading voxel51/fiftyone-plugins...\n",
            "\n",
            "Skipping existing plugin '@voxel51/evaluation'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plug-in for image augmentations\n",
        "!fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r0OvvAflvmV",
        "outputId": "a889ae27-fd10-4c8e-c46b-9710dd19bd08"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading jacobmarks/fiftyone-albumentations-plugin...\n",
            "\n",
            "Skipping existing plugin '@jacobmarks/albumentations_augmentation'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "oCOAC2gFi4ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.brain as fob\n",
        "import torch\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Fun\n",
        "from fiftyone import ViewField as F\n",
        "import fiftyone.utils.random as four\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, ConcatDataset"
      ],
      "metadata": {
        "id": "0Rj1VqYYi6J1"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The MNIST dataset\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/mnist_clean.png?raw=true)\n",
        "\n",
        "The Modified National Institute of Standards and Technology (MNIST) dataset stands as one of the most influential benchmarks in computer vision and machine learning history. Created by Yann LeCun and colleagues in 1998, MNIST transformed a collection of handwritten digits from American Census Bureau employees and high school students into a standardized machine learning challenge that has shaped decades of research.\n",
        "\n",
        "\n",
        "\n",
        "**Dataset Structure and Characteristics**\n",
        "\n",
        "The MNIST dataset contains 60,000 training images and 10,000 testing images of handwritten digits (0-9). These are grayscale images of size 28x28 pixels, with each pixel value ranging from 0 (black) to 255 (white). The images have been size-normalized and centered, making them ideal for learning fundamental computer vision concepts without the complexity of dealing with varying scales, rotations, or backgrounds found in natural images. You can inspect the samples on the test portion of the dataset through [try.fiftyone.ai](https://try.fiftyone.ai/datasets/mnist/samples).\n",
        "\n",
        "**Historical Significance and Impact**\n",
        "\n",
        "MNIST earned its status as the \"Hello World\" of computer vision for several reasons. First, it provided the research community with a common benchmark that was computationally tractable, even early personal computers could train models on MNIST in reasonable time. Second, its simplicity allowed researchers to focus on algorithmic innovations rather than data preprocessing challenges. Landmark achievements in deep learning, from early multilayer perceptrons to convolutional architectures like LeNet-5, were first demonstrated and validated on MNIST.\n",
        "\n",
        "The dataset served as a proving ground for fundamental concepts of modern computer vision: convolutional neural networks, regularization techniques, and optimization algorithms were all tested on these handwritten digits. Many techniques that seem obvious today, like data augmentation, dropout, and batch normalization, were first explored and validated using MNIST as a testbed.\n",
        "\n",
        "**Why MNIST Remains Relevant**\n",
        "\n",
        "While critics sometimes dismiss MNIST as \"too easy\" for modern standards, it continues to serve crucial educational and research purposes. For newcomers to computer vision, MNIST provides an ideal environment to understand core concepts without overwhelming complexity. The dataset is small enough to experiment with quickly, yet rich enough to demonstrate important phenomena like overfitting, the importance of data augmentation, and the impact of architectural choices.\n",
        "\n",
        "Moreover, MNIST's apparent simplicity can be deceptive. Achieving state-of-the-art performance >99.7% accuracy requires sophisticated techniques and careful attention to detail, making it an interesting benchmark for testing new methodologies.\n",
        "\n",
        "\n",
        "## MNIST in the Modern Era\n",
        "\n",
        "Today, MNIST serves as an excellent starting point for understanding how modern techniques like embeddings, zero-shot classification, and transfer learning work. While a model trained specifically on MNIST might achieve 99%+ accuracy, applying a general-purpose vision model like CLIP without any MNIST-specific training provides insights into how well these models generalize and what they've learned about visual patterns from their massive training datasets.\n",
        "\n",
        "This makes MNIST perfect for comparing traditional supervised learning approaches with modern pre-trained models, helping us understand the trade-offs between task-specific optimization and general-purpose visual understanding.\n",
        "\n",
        "\n",
        "## CLIP\n",
        "\n",
        "**CLIP (Contrastive Language-Image Pre-training)** is a vision-language model developed by OpenAI that learns to understand the relationship between images and text descriptions. Traditional computer vision models are trained on fixed sets of image categories, but CLIP was trained on 400 million image-text pairs from the internet, learning to match images with their captions. This training enables CLIP to perform \"zero-shot\" classification: the ability to classify images into categories it has not seen during training by comparing the image representation with text descriptions of potential classes. The model works by encoding both images and text into the same high-dimensional embedding space, where similar concepts cluster together, allowing it to determine which text description best matches a given image through similarity comparison.\n",
        "\n",
        "![](https://github.com/andandandand/images-for-colab-notebooks/blob/main/clip%20contrastive%20pre-training.png?raw=true)\n",
        "\n",
        "### CLIP vs MNIST\n",
        "\n",
        "In [OpenAI's 2021 CLIP paper](https://arxiv.org/abs/2103.00020), the comparison with MNIST revealed fascinating insights about zero-shot learning capabilities. While supervised models trained specifically on MNIST achieve near-perfect accuracy (>99.7%), OpenAI's best performing variant of CLIP (trained on about 400 million image-text pairs) achieved only 88% accuracy on these handwritten digits. This is interesting considering the model was not trained explicitly trained on any variant of this dataset. This comparison highlighted a shift in modern computer vision from specialized models that excel at narrow tasks to general models that perform reasonably well across diverse domains. The 11+ percentage point gap between supervised and zero-shot approaches on MNIST demonstrates both the power and limitations of general-purpose vision-language models, making MNIST an excellent case study for understanding the trade-offs between specialized optimization and general-purpose learning, particularly valuable for exploring modern approaches like few-shot learning, prompt engineering, and transfer learning strategies.\n",
        "\n",
        "Note that the variant of CLIP that we use in our experiments of this notebook, `\"clip-vit-base32-torch\"` (a Vit-B/32 model) is *not* the top performing variant of CLIP that is showcased on the original OpenAI paper. That would be \"ViT-L/14@336px\", a bigger vision transformer model. The Vit-B/32 base models remain interesting and widely used due to their lower number of parameters.\n",
        "\n",
        "\n",
        "**Known Issues and Research Opportunities**\n",
        "\n",
        "The original dataset contains several images where the ground truth labeling is ambiguous or questionable. The academic community has identified [several of these ambiguous samples](https://arxiv.org/abs/1912.05283). These labeling inconsistencies, while representing less than 0.1% of the dataset, provide opportunities to explore data quality assessment techniques.\n",
        "\n",
        "Finding and analyzing these edge cases teaches valuable lessons about real-world data challenges. In production systems, you'll inevitably encounter ambiguous samples, annotation errors, and edge cases. MNIST's imperfections make it an excellent sandbox for developing robust approaches to handle these issues. Let's see if we can find them using FiftyOne's powerful analysis capabilities!"
      ],
      "metadata": {
        "id": "hKx_LKY_W87E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot Classification vs \"Traditional\" Supervised Training\n",
        "\n",
        "**Zero-shot classification** leverages pre-trained models like CLIP that have learned rich visual representations from massive datasets, allowing them to classify images into \"interpolated\" categories they've not explicitly seen during training. These models understand the semantic relationship between images and text descriptions, enabling classification through natural language prompts like \"a photo of the digit 3\" without requiring any task-specific training data. In contrast, **traditional supervised training** requires labeled examples for each class you want to predict. You must provide the model with many of images of each digit along with their correct labels, then train the network to learn the mapping from pixel patterns to class labels through backpropagation. While supervised training often achieves higher accuracy on specific datasets and allows for domain-specific optimization, zero-shot approaches offer remarkable flexibility and can instantly work on new classification tasks without additional training time or computational resources."
      ],
      "metadata": {
        "id": "ws9PjUuSZhH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/andandandand/images-for-colab-notebooks/blob/main/clip%20zero-shot%20prediction.png?raw=true)"
      ],
      "metadata": {
        "id": "7qaJxdibqZeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where CLIP Fails: Specialized Domains\n",
        "\n",
        "**Understanding the Limitations of General-Purpose Vision Models**\n",
        "\n",
        "While CLIP excels at understanding natural images and common objects, it struggles with specialized domains that differ from its internet-scale training data. MNIST handwritten digits represent one such domain where CLIP's performance drops compared to task-specific models.\n",
        "\n",
        "**Domain-Specific Challenges:**\n",
        "\n",
        "**Limited Training Exposure**: CLIP trained on web-scraped image-text pairs, which contain fewer examples of handwritten digits compared to photographs of everyday objects. The model lacks deep exposure to the subtle variations in handwriting styles.\n",
        "\n",
        "**Scale and Context Mismatch**: CLIP expects high-resolution, colorful images with rich contextual information. MNIST's 28x28 grayscale digits provide minimal visual context that CLIP relies on for classification decisions.\n",
        "\n",
        "**Specialized Visual Features**: Handwritten digits require recognition of fine-grained stroke patterns, curves, and connections that differ from the broader visual concepts CLIP learned from natural images. A \"6\" versus \"9\" distinction depends on subtle orientation cues.\n",
        "\n",
        "**Text-Image Alignment**: CLIP's strength lies in matching images with descriptive captions. Simple digit classification lacks the rich semantic relationships between visual and textual information that CLIP exploits in other domains.\n",
        "\n",
        "**Performance Gap**: This explains why CLIP achieves only ~88% accuracy on MNIST while specialized CNNs reach >99%. The 11+ percentage point gap highlights the trade-off between general-purpose capabilities and domain-specific optimization.\n",
        "\n",
        "Other specialized domains where CLIP struggles include medical imaging, satellite imagery, microscopy, and industrial inspection: areas where domain expertise and task-specific training data prove more valuable than general visual understanding.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Bo7U6NjqTSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/clip_limitations.png?raw=true)"
      ],
      "metadata": {
        "id": "p3C20CqVqgRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the MNIST Dataset from FiftyOne's Dataset Zoo\n",
        "\n",
        "A FiftyOne dataset wraps together the annotations and image data into a unified, queryable structure that makes computer vision workflows seamless. Unlike traditional approaches where you might manage separate files for images and labels, FiftyOne treats each sample as a rich object containing the image itself, ground truth labels, metadata, and any predictions or embeddings you add later. This design enables powerful operations like filtering by class imbalance, visualizing prediction confidence, or finding samples with specific characteristics, all through a consistent API.\n",
        "\n",
        "Loading MNIST from [FiftyOne's Dataset Zoo](https://docs.voxel51.com/dataset_zoo/index.html) is straightforward:\n"
      ],
      "metadata": {
        "id": "S3J8_p_7ZsvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will load the test split from the dataset first\n",
        "test_dataset = foz.load_zoo_dataset(\"mnist\", split='test')\n",
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzg-FFDoTVWr",
        "outputId": "50bdcb7f-db9e-4dfd-e515-845d9931702f"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 'test' already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Split 'test' already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'mnist-test'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'mnist-test'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name:        mnist-test\n",
              "Media type:  image\n",
              "Num samples: 10000\n",
              "Persistent:  False\n",
              "Tags:        []\n",
              "Sample fields:\n",
              "    id:                             fiftyone.core.fields.ObjectIdField\n",
              "    filepath:                       fiftyone.core.fields.StringField\n",
              "    tags:                           fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:                       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
              "    created_at:                     fiftyone.core.fields.DateTimeField\n",
              "    last_modified_at:               fiftyone.core.fields.DateTimeField\n",
              "    ground_truth:                   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    clip_embeddings:                fiftyone.core.fields.VectorField\n",
              "    clip_zero_shot_classification:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    lenet_classification:           fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    hardness:                       fiftyone.core.fields.FloatField\n",
              "    mistakenness:                   fiftyone.core.fields.FloatField\n",
              "    retrained_lenet_classification: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    lenet_eval:                     fiftyone.core.fields.BooleanField\n",
              "    retrained_lenet_eval:           fiftyone.core.fields.BooleanField\n",
              "    clip_zero_shot_eval:            fiftyone.core.fields.BooleanField"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We launch the FiftyOne app to visualize the test set."
      ],
      "metadata": {
        "id": "b3UYLNkgyKNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session = fo.launch_app(test_dataset, auto=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD7xgYznyGZR",
        "outputId": "20ac6bb2-8aaa-4e41-846b-dfc109e9c234"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.session.session:Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `compute_metadata()` we add the size in bytes, the image file type, the width and height of the image, and the number of channels to our dataset."
      ],
      "metadata": {
        "id": "ym7vp063WUm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.compute_metadata()"
      ],
      "metadata": {
        "id": "nKBLQnntVila"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do [aggregations](https://docs.voxel51.com/user_guide/using_aggregations.html) on the dataset to explore the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "YTH-cijUZfAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the [bounds](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.bounds) aggregation to compute the [min, max] range of a numeric field of a dataset. And [mean()](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.bounds) and [std()](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.std) to compute the mean and standard deviation of it.\n",
        "\n"
      ],
      "metadata": {
        "id": "xr_v-8_yZ041"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.bounds(\"metadata.size_bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yphCNxUxY-BW",
        "outputId": "be9e89d5-508f-4c86-901a-251f32563ebf"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(483, 1033)"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.mean(\"metadata.size_bytes\"), test_dataset.std(\"metadata.size_bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXWLJSuYZV0-",
        "outputId": "61bf8589-5b9b-4b76-8029-a6602041113a"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768.6065, 84.01331833554713)"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try filtering by label and visualizing the metadata of the MNIST images through the FiftyOne app.\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/filtering_by_label_mnist_w_metadata.png?raw=true)"
      ],
      "metadata": {
        "id": "yOmjhEc6X4Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Rm8fbYruVVZI",
        "outputId": "572a069b-8393-439b-e232-03fb0f9e6de8"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualize the distributions of `metadata.size_bytes` and `ground_truth.label`. Both of these are relevant when we decide the batch size on which to run our models and to evaluate the balance of classes in our dataset.\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/distribtution_size_bytes.png?raw=true)\n",
        "\n",
        "For this try clicking on the `+` symbol next to samples, select `Histograms` and then `metadata.size_bytes` and `ground_truth.label` from the dropdown menu. The `Split horizontally button` will allow you to see the panel alongside the image data.\n",
        "\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/ground_truh_distribution_mnist.png?raw=true)"
      ],
      "metadata": {
        "id": "XF-aubVNe3JX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Image Embeddings with CLIP\n",
        "\n",
        "**Image embeddings** are high-dimensional vector representations that capture the semantic and aesthetic content of images in a vector format that machine learning models can understand and compare.\n",
        "\n",
        "Think of embeddings as a way to translate visual concepts into vectors where similar images will correspond to similar embedding vectors, while visually or semantically different images will have more distant vectors in a high-dimensional space.\n",
        "\n",
        "OpenAI's CLIP model creates particularly powerful embeddings because it was trained to understand the relationship between images and their matching text descriptions, enabling it to capture rich semantic meaning. In FiftyOne, creating embeddings with CLIP is straightforward:\n",
        "\n",
        "1. We obtain the model through [`foz.load_zoo_model(\"\"clip-vit-base32-torch\"\")`](https://docs.voxel51.com/model_zoo/models.html#clip-vit-base32-torch) and pass it to the GPU (if we have it available).\n",
        "2. We use the [`compute_embeddings()`](https://docs.voxel51.com/api/fiftyone.brain.internal.core.elasticsearch.html#fiftyone.brain.internal.core.elasticsearch.ElasticsearchSimilarityIndex.compute_embeddings) method:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bQQPA9HTXVsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = foz.load_zoo_model(\"clip-vit-base32-torch\",\n",
        "                                device=device)\n",
        "print(f\"The model is loaded on {clip_model._device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST1OahV5kvgi",
        "outputId": "0d194b91-a4d5-4f30-ff77-dc9e38d97dd2"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model is loaded on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of parameters in the model\n",
        "total_params = sum(p.numel() for p in clip_model._model.parameters())\n",
        "\n",
        "print(f\"The CLIP model has {total_params:,} parameters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhHT2scrakDC",
        "outputId": "06540b01-3f1e-4b32-8295-4fb457774c00"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The CLIP model has 151,277,313 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will take about 3 min on a Google Colab instance with GPU enabled"
      ],
      "metadata": {
        "id": "R1Z9t8CBoXKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_embeddings = test_dataset.compute_embeddings(model=clip_model,\n",
        "                                        batch_size=512,\n",
        "                                        num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-va5BZtpFw6",
        "outputId": "0a776a98-4d06-4de9-d406-3f60b23abf70"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████| 10000/10000 [33.9s elapsed, 0s remaining, 307.1 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████| 10000/10000 [33.9s elapsed, 0s remaining, 307.1 samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The embeddings are a NumPy array.\n",
        "# Each embedding is a 512-dimensional vector.\n",
        "type(clip_embeddings), clip_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSyH8a7M0IrT",
        "outputId": "2d637544-03b1-45f9-de27-63eef53af62f"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, (10000, 512))"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, sample in enumerate(test_dataset):\n",
        "    sample[\"clip_embeddings\"] = clip_embeddings[index]\n",
        "    sample.save()"
      ],
      "metadata": {
        "id": "wKDE9MR80UJ7"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We see that the first sample now has the field 'embeddings' attached to it\n",
        "test_dataset.first().clip_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDSCqkEss_K4",
        "outputId": "c0e49bcf-87f9-45c3-b43c-5eb7449c9145"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512,)"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a 2D Projection of the Embeddings for Visualization"
      ],
      "metadata": {
        "id": "gjHRCURddjR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_visualization = fob.compute_visualization(test_dataset,\n",
        "                                              method=\"pca\",\n",
        "                                              embeddings=\"clip_embeddings\",\n",
        "                                              num_dims=2,\n",
        "                                              brain_key=\"pca_visualization_clip_embeds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XacGyC6OrtdR",
        "outputId": "ec066296-4480-4dbb-ce2a-f3009880ad1e"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating visualization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.visualization:Generating visualization...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "umap_visualization = fob.compute_visualization(test_dataset,\n",
        "                                              method=\"umap\",\n",
        "                                              embeddings=\"clip_embeddings\",\n",
        "                                              num_dims=2,\n",
        "                                              brain_key=\"umap_visualization_clip_embeds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483,
          "referenced_widgets": [
            "4337298cb93d4c69ae152b0ebce96b7f",
            "a1a1a85c56c747918d1045f40300c1f4",
            "57df194fa5b54a5fb97ea4d3e6978ac7",
            "52f963f35e3d49c5b66ee88eb5b81a25",
            "5e4aee878da64108b2fd8a457807ae44",
            "cb5295ef908c45279966b6ba0a2c1cb3",
            "4d135b9bc5a54cfd980f7a653f7d920e",
            "c131465b27bb435aacf4829d686a5f75",
            "2ae075891fdc431387c5b99052252054",
            "47c6c1158c804c4ca45fd296152ded7a",
            "d7e09b2817e944c897a05c14116e49c2"
          ]
        },
        "id": "4mq4hg9_ovbh",
        "outputId": "294d0f7e-08aa-4082-d6d8-6a6cb546d8fc"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating visualization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.visualization:Generating visualization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UMAP( verbose=True)\n",
            "Wed Jun 11 01:09:58 2025 Construct fuzzy simplicial set\n",
            "Wed Jun 11 01:09:58 2025 Finding Nearest Neighbors\n",
            "Wed Jun 11 01:09:58 2025 Building RP forest with 10 trees\n",
            "Wed Jun 11 01:09:58 2025 NN descent for 13 iterations\n",
            "\t 1  /  13\n",
            "\t 2  /  13\n",
            "\t 3  /  13\n",
            "\t 4  /  13\n",
            "\tStopping threshold met -- exiting after 4 iterations\n",
            "Wed Jun 11 01:09:59 2025 Finished Nearest Neighbor Search\n",
            "Wed Jun 11 01:09:59 2025 Construct embedding\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs completed:   0%|            0/500 [00:00]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4337298cb93d4c69ae152b0ebce96b7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tcompleted  0  /  500 epochs\n",
            "\tcompleted  50  /  500 epochs\n",
            "\tcompleted  100  /  500 epochs\n",
            "\tcompleted  150  /  500 epochs\n",
            "\tcompleted  200  /  500 epochs\n",
            "\tcompleted  250  /  500 epochs\n",
            "\tcompleted  300  /  500 epochs\n",
            "\tcompleted  350  /  500 epochs\n",
            "\tcompleted  400  /  500 epochs\n",
            "\tcompleted  450  /  500 epochs\n",
            "Wed Jun 11 01:10:04 2025 Finished embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "htl4-xTnt5xY",
        "outputId": "32dc705d-542e-48e1-fa1e-902660938fd1"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/image_embeddings_zero_cluster.gif?raw=true)"
      ],
      "metadata": {
        "id": "sJTHNOjydh7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that uniqueness and representativeness fields have been added to the dataset\n",
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbOsmfQgAoZh",
        "outputId": "f61a2f39-e9c4-4f2e-8be3-318d9a498124"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name:        mnist-test\n",
              "Media type:  image\n",
              "Num samples: 10000\n",
              "Persistent:  False\n",
              "Tags:        []\n",
              "Sample fields:\n",
              "    id:                             fiftyone.core.fields.ObjectIdField\n",
              "    filepath:                       fiftyone.core.fields.StringField\n",
              "    tags:                           fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:                       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
              "    created_at:                     fiftyone.core.fields.DateTimeField\n",
              "    last_modified_at:               fiftyone.core.fields.DateTimeField\n",
              "    ground_truth:                   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    clip_embeddings:                fiftyone.core.fields.VectorField\n",
              "    clip_zero_shot_classification:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    lenet_classification:           fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    hardness:                       fiftyone.core.fields.FloatField\n",
              "    mistakenness:                   fiftyone.core.fields.FloatField\n",
              "    retrained_lenet_classification: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    lenet_eval:                     fiftyone.core.fields.BooleanField\n",
              "    retrained_lenet_eval:           fiftyone.core.fields.BooleanField\n",
              "    clip_zero_shot_eval:            fiftyone.core.fields.BooleanField"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the FiftyOne app to explore the most unique and representative samples in the dataset."
      ],
      "metadata": {
        "id": "FzdQd-g3A248"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EDJ5HWQzAnEZ",
        "outputId": "9c524046-8278-42c2-a068-36496fa5a476"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering through Embeddings"
      ],
      "metadata": {
        "id": "qwoQi7T52naS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Clustering** transforms the high-dimensional embedding space into groups of similar images, helping us discover hidden patterns and structure in our dataset. Rather than manually browsing through thousands of images, clustering algorithms like K-means, HDBSCAN, or Gaussian Mixture Models automatically identify samples that share visual or semantic characteristics. This is particularly powerful when combined with CLIP embeddings, as the clusters often correspond to semantic concepts like \"outdoor scenes,\" \"close-up portraits,\" or \"nighttime images.\" FiftyOne provides a dedicated clustering plugin that makes this analysis seamless. Install it with:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cJeaax5-8Rww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fiftyone plugins download https://github.com/jacobmarks/clustering-plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNWq6eg43Fn_",
        "outputId": "2b669c64-9500-4777-de10-6750e277f165"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading jacobmarks/clustering-plugin...\n",
            "\n",
            "Skipping existing plugin '@jacobmarks/clustering'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once installed, you can access clustering functionality directly from the FiftyOne App by pressing the backtick key (`) and typing `compute_clusters`. The plugin offers multiple clustering algorithms and customizable parameters. This workflow is valuable for understanding the groupings within your data produced by the embedding model. Learn more about clustering workflows and advanced techniques in the [FiftyOne clustering tutorial](https://docs.voxel51.com/tutorials/clustering.html).\n"
      ],
      "metadata": {
        "id": "HXlzLzWZ860S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WNnlzTX93OnS",
        "outputId": "36449760-b606-48ab-be19-daf0a9e50a23"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot Classification"
      ],
      "metadata": {
        "id": "JloCmwLUXZ78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we understand how CLIP creates meaningful embeddings, we can leverage these representations for **zero-shot classification** - the ability to classify images into categories without any task-specific training. CLIP's vision-language training enables it to understand the semantic relationship between images and text descriptions, allowing us to classify MNIST digits simply by providing natural language descriptions of each class.\n",
        "\n",
        "The key insight is that CLIP learns to map both images and text into the same embedding space, where semantically similar concepts cluster together. By computing the similarity between an image's embedding and text embeddings for each class description, we can determine which category best matches the input image. This approach is remarkably flexible, we can change our classification categories simply by modifying the text prompts, without retraining the models.\n",
        "\n",
        "For MNIST digit classification, we'll use descriptive text prompts combined with the written names of each digit.\n"
      ],
      "metadata": {
        "id": "87uZ17UaeBcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We obtain the distinct labels of the dataset\n",
        "dataset_classes = sorted(test_dataset.distinct(\"ground_truth.label\"))\n",
        "dataset_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8Jk1qpak2Q9",
        "outputId": "ecd1fa07-fe8a-4b68-afd1-91b0958195b5"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0 - zero',\n",
              " '1 - one',\n",
              " '2 - two',\n",
              " '3 - three',\n",
              " '4 - four',\n",
              " '5 - five',\n",
              " '6 - six',\n",
              " '7 - seven',\n",
              " '8 - eight',\n",
              " '9 - nine']"
            ]
          },
          "metadata": {},
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Effect of the Text Prompt"
      ],
      "metadata": {
        "id": "uLYZ287EUJQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With CLIP, the `text_prompt` parameter can have a significant effect on the accuracy of the model, as it affects the text embedding for the label.\n",
        "\n",
        "## Experiment\n",
        "\n",
        "Try modifying the `text_prompt` field with:\n",
        "\n",
        "* \"The handwritten digit \"\n",
        "* \"A grayscale image of the number \"\n",
        "* \"A pixel illustration of the digit \"\n",
        "* \"A low resolution image of the number\"\n",
        "* \"An MNIST digit \"\n",
        "\n",
        "And observe the effect on the accuracy, precision and recall on our classification report."
      ],
      "metadata": {
        "id": "R4oAWHCawr7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model = foz.load_zoo_model(\n",
        "    \"clip-vit-base32-torch\",\n",
        "    text_prompt=\"A photo of \",\n",
        "    classes=dataset_classes,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "O6Q7NqbJdYCL"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The CLIP model preprocesses the input data\n",
        "clip_model.preprocess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBgS_90dWgbf",
        "outputId": "b2f18fc6-d334-448b-db43-37051ea8233f"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice that the CLIP model was originally trained on much larger images and that\n",
        "# these where RGB, in order to make the MNIST data work with it, it has been transformed and scaled\n",
        "clip_model._transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rMuKMCQWP9u",
        "outputId": "ddb89a89-0574-45eb-b33c-bfbdc29d4b43"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    <fiftyone.utils.torch.ToPILImage object at 0x7add7db74410>\n",
              "    Resize(size=[224, 224], interpolation=bilinear, max_size=None, antialias=True)\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `store_logits=True` parameter is important as it preserves the model's confidence scores for each prediction, enabling us to analyze prediction uncertainty and identify samples where the model was unsure. After applying the model, we can compute comprehensive evaluation metrics and visualize the results using FiftyOne's evaluation framework:"
      ],
      "metadata": {
        "id": "8mG-ECXjiQ7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.apply_model(\n",
        "    model=clip_model,\n",
        "    label_field=\"clip_zero_shot_classification\",\n",
        "    # We need to store the logits to compute the \"mistakenness\" value\n",
        "    store_logits=True,\n",
        "    # This is how many samples we will show to the model at once\n",
        "    batch_size=256,\n",
        "    progress_bar=True,\n",
        "     # Use more running threads if you have multiple cpus\n",
        "    num_workers=os.cpu_count()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9F4bKQheb_-",
        "outputId": "8371b2da-7613-425c-9259-9d250d24560c"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████| 10000/10000 [37.7s elapsed, 0s remaining, 271.2 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████| 10000/10000 [37.7s elapsed, 0s remaining, 271.2 samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "kACq6EVGf5RR",
        "outputId": "258973a8-3c06-4e80-e542-13db85d81d86"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating CLIP's Classification Performance\n",
        "\n",
        "With our zero-shot CLIP predictions generated, we need to evaluate how well the model performs compared to the ground truth labels. FiftyOne provides powerful evaluation capabilities that go beyond simple accuracy metrics, allowing us to understand where and why our model succeeds or fails.\n",
        "\n",
        "[FiftyOne's Model Evaluation Panel](https://docs.voxel51.com/user_guide/app.html#model-evaluation-panel-sub-new ) provides interactive confusion matrices, per-class metrics, and the ability to drill down into specific failure modes. This visual analysis helps identify systematic errors, class imbalances, and edge cases that pure numerical metrics might miss, enabling targeted improvements to your classification pipeline.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "plmWGrpTXfSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_evaluation_results = test_dataset.evaluate_classifications(\n",
        "    \"clip_zero_shot_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"clip_zero_shot_eval\")\n",
        "\n",
        "session.refresh()\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "y04n9H8WhAUf",
        "outputId": "316b6901-fe5d-4e95-f8e0-a0430637ba2e"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clip_evaluation_results.print_report(classes=dataset_classes, digits=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaXb016KpLcd",
        "outputId": "94abcacb-45ce-4d3f-e4e6-ccf3162d418a"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    0 - zero      0.256     0.999     0.408       980\n",
            "     1 - one      0.145     0.033     0.053      1135\n",
            "     2 - two      1.000     0.016     0.031      1032\n",
            "   3 - three      0.901     0.386     0.541      1010\n",
            "    4 - four      0.513     0.177     0.263       982\n",
            "    5 - five      0.600     0.161     0.254       892\n",
            "     6 - six      0.076     0.255     0.117       958\n",
            "   7 - seven      0.613     0.635     0.624      1028\n",
            "   8 - eight      0.178     0.115     0.140       974\n",
            "    9 - nine      0.000     0.000     0.000      1009\n",
            "\n",
            "    accuracy                          0.275     10000\n",
            "   macro avg      0.428     0.278     0.243     10000\n",
            "weighted avg      0.427     0.275     0.241     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The confusion matrix has a lot activity outside of the main diagonal\n",
        "clip_evaluation_results.plot_confusion_matrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "TioSsNHjS_sw",
        "outputId": "d138e65b-bcce-4657-a0d9-2c5bda722ab1"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fiftyone/core/plots/plotly.py:1591: UserWarning:\n",
            "\n",
            "Interactive plots are currently only supported in Jupyter notebooks. Support outside of notebooks and in Google Colab and Databricks will be included in an upcoming release. In the meantime, you can still use this plot, but note that (i) selecting data will not trigger callbacks, and (ii) you must manually call `plot.show()` to launch a new plot that reflects the current state of an attached session.\n",
            "\n",
            "See https://docs.voxel51.com/user_guide/plots.html#working-in-notebooks for more information.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"8198f42e-1616-4906-a087-753e9ae33a86\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8198f42e-1616-4906-a087-753e9ae33a86\")) {                    Plotly.newPlot(                        \"8198f42e-1616-4906-a087-753e9ae33a86\",                        [{\"mode\":\"markers\",\"opacity\":0.1,\"x\":[0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9],\"y\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"type\":\"scatter\",\"uid\":\"d723d424-7fe0-42e6-86d5-d1c2e4730fa3\"},{\"colorscale\":[[0.0,\"rgb(255,245,235)\"],[0.125,\"rgb(254,230,206)\"],[0.25,\"rgb(253,208,162)\"],[0.375,\"rgb(253,174,107)\"],[0.5,\"rgb(253,141,60)\"],[0.625,\"rgb(241,105,19)\"],[0.75,\"rgb(217,72,1)\"],[0.875,\"rgb(166,54,3)\"],[1.0,\"rgb(127,39,4)\"]],\"hoverinfo\":\"skip\",\"showscale\":false,\"z\":[[229,16,0,0,6,4,394,38,322,0],[147,47,0,1,6,3,647,10,112,1],[192,64,0,0,47,1,66,653,5,0],[603,3,0,0,0,3,244,0,105,0],[58,0,0,0,1,144,684,2,3,0],[162,0,0,0,174,1,592,30,23,0],[25,0,0,390,40,75,416,30,34,0],[378,89,16,42,55,0,141,303,8,0],[1049,37,0,0,10,9,15,0,15,0],[979,0,0,0,0,0,0,0,1,0]],\"zmax\":1049,\"zmin\":0,\"type\":\"heatmap\",\"uid\":\"ede2e1c3-650f-4cb5-a113-6f91d5272227\"},{\"colorbar\":{\"len\":1,\"lenmode\":\"fraction\"},\"colorscale\":[[0.0,\"rgb(255,245,235)\"],[0.125,\"rgb(254,230,206)\"],[0.25,\"rgb(253,208,162)\"],[0.375,\"rgb(253,174,107)\"],[0.5,\"rgb(253,141,60)\"],[0.625,\"rgb(241,105,19)\"],[0.75,\"rgb(217,72,1)\"],[0.875,\"rgb(166,54,3)\"],[1.0,\"rgb(127,39,4)\"]],\"hovertemplate\":\"\\u003cb\\u003ecount: %{z}\\u003c\\u002fb\\u003e\\u003cbr\\u003eground_truth: %{y}\\u003cbr\\u003eclip_zero_shot_classification: %{x}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"opacity\":0.25,\"z\":[[229,16,0,0,6,4,394,38,322,0],[147,47,0,1,6,3,647,10,112,1],[192,64,0,0,47,1,66,653,5,0],[603,3,0,0,0,3,244,0,105,0],[58,0,0,0,1,144,684,2,3,0],[162,0,0,0,174,1,592,30,23,0],[25,0,0,390,40,75,416,30,34,0],[378,89,16,42,55,0,141,303,8,0],[1049,37,0,0,10,9,15,0,15,0],[979,0,0,0,0,0,0,0,1,0]],\"zmax\":1049,\"zmin\":0,\"type\":\"heatmap\",\"uid\":\"e4c8375a-1851-43f7-8874-39e69bbd453f\"}],                        {\"clickmode\":\"event\",\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(237,237,237)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(51,51,51)\"},\"error_y\":{\"color\":\"rgb(51,51,51)\"},\"marker\":{\"line\":{\"color\":\"rgb(237,237,237)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(51,51,51)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(51,51,51)\"},\"baxis\":{\"endlinecolor\":\"rgb(51,51,51)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(51,51,51)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"colorscale\":{\"sequential\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"sequentialminus\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]]},\"colorway\":[\"#F8766D\",\"#A3A500\",\"#00BF7D\",\"#00B0F6\",\"#E76BF3\"],\"font\":{\"color\":\"rgb(51,51,51)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"rgb(237,237,237)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"rgb(237,237,237)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"bgcolor\":\"rgb(237,237,237)\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"bgcolor\":\"rgb(237,237,237)\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"}},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\"}}},\"xaxis\":{\"constrain\":\"domain\",\"range\":[-0.5,9.5],\"tickmode\":\"array\",\"ticktext\":[\"0 - zero\",\"1 - one\",\"2 - two\",\"3 - three\",\"4 - four\",\"5 - five\",\"6 - six\",\"7 - seven\",\"8 - eight\",\"9 - nine\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9]},\"yaxis\":{\"constrain\":\"domain\",\"range\":[-0.5,9.5],\"scaleanchor\":\"x\",\"scaleratio\":1,\"tickmode\":\"array\",\"ticktext\":[\"9 - nine\",\"8 - eight\",\"7 - seven\",\"6 - six\",\"5 - five\",\"4 - four\",\"3 - three\",\"2 - two\",\"1 - one\",\"0 - zero\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9]},\"margin\":{\"r\":0,\"t\":30,\"l\":0,\"b\":0},\"title\":{}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8198f42e-1616-4906-a087-753e9ae33a86');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Classes where the Model is Performing Worst"
      ],
      "metadata": {
        "id": "_Exkmk50l_Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nines_view = test_dataset.filter_labels(\"ground_truth\",\n",
        "                                        F(\"$ground_truth.label\") == \"9 - nine\"\n",
        "                                       )"
      ],
      "metadata": {
        "id": "W0WTxH3smMkE"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = fo.launch_app(nines_view, auto=False)\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "oFZbuxx-wv5L",
        "outputId": "8542125c-384c-4e93-dea5-55cb9e0d549d"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.session.session:Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Clear the clip_model from GPU memory\n",
        "# Delete the model variable\n",
        "del clip_model\n",
        "\n",
        "# Run Python's garbage collector\n",
        "gc.collect()\n",
        "\n",
        "# Empty the CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"CUDA device memory from clip_model should be cleared.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0uw_4muPB-8",
        "outputId": "dc31dc6c-d969-4e60-fd93-1fbcfe955ba5"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA device memory from clip_model should be cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Custom Convolutional Neural Networks in PyTorch (Two Versions of LeNet-5)\n",
        "\n",
        "\n",
        "While zero-shot classification with CLIP demonstrates the power of modern pre-trained models, understanding how to build and train convolutional neural networks from scratch remains fundamental to computer vision. **LeNet-5**, proposed by Yann LeCun in 1998, represents one of the earliest and most influential CNN architectures. Despite its age, LeNet-5 perfectly illustrates core CNN concepts including convolutional layers, pooling operations, and the transition from feature extraction to classification.\n",
        "\n",
        "LeNet-5's architecture is elegantly simple yet effective: it uses alternating convolutional and pooling layers to extract hierarchical features, followed by fully connected layers for classification. The network learns low-level features like edges and curves in early layers, then combines these into higher-level digit patterns in deeper layers. This hierarchical feature learning principle underlies virtually all modern CNN architectures.\n",
        "\n",
        "For MNIST digit classification, LeNet-5 provides an excellent baseline to compare against CLIP's zero-shot performance. While CLIP leverages massive-scale pre-training and vision-language understanding, LeNet-5 demonstrates what's possible with task-specific supervised learning on a much smaller scale. Building this model from scratch in PyTorch teaches essential concepts about gradient-based optimization, backpropagation, and the relationship between network architecture and learning capacity.\n",
        "\n"
      ],
      "metadata": {
        "id": "NKrksi3nXk2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is [a great animation](https://youtu.be/UxIS_PoVoz8?si=3ibZms7Hk1oSj55k) showcasing the architecture.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lr3oiQDzibmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassicLeNet5(nn.Module):\n",
        "    \"\"\"\n",
        "    LeNet-5 CNN architecture for MNIST digit classification.\n",
        "\n",
        "    Original paper: \"Gradient-Based Learning Applied to Document Recognition\"\n",
        "    by LeCun et al. (1998)\n",
        "\n",
        "    Architecture (maintains original design with padding):\n",
        "    Input (28x28) -> Pad to (32x32) -> C1 (6@28x28) -> S2 (6@14x14) ->\n",
        "    C3 (16@10x10) -> S4 (16@5x5) -> C5 (120@1x1) -> F6 (84) -> Output (10)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ClassicLeNet5, self).__init__()\n",
        "\n",
        "        # Feature extraction layers\n",
        "        # C1: Convolutional layer - 6 feature maps, 5x5 kernels\n",
        "        # Add padding=2 to convert 28x28 input to 32x32, maintaining original design\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "        # S2: Subsampling layer (average pooling) - 2x2 with stride 2\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # C3: Convolutional layer - 16 feature maps, 5x5 kernels\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "\n",
        "        # S4: Subsampling layer (average pooling) - 2x2 with stride 2\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # C5: Convolutional layer - 120 feature maps, 5x5 kernels (original design)\n",
        "        # This reduces the 5x5 feature maps to 1x1\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1)\n",
        "\n",
        "        # Classification layers\n",
        "        # F6: Fully connected layer with 84 units\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "\n",
        "        # Output layer: 10 classes for digits 0-9\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 1, 28, 28)\n",
        "\n",
        "        Returns:\n",
        "            Output logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # C1: Convolution + activation (padding converts 28x28 to 32x32, then conv to 28x28)\n",
        "        # Input: (batch, 1, 28, 28) -> Pad to (32, 32) -> Conv to (batch, 6, 28, 28)\n",
        "        x = torch.tanh(self.conv1(x))\n",
        "\n",
        "        # S2: Average pooling\n",
        "        # Input: (batch, 6, 28, 28) -> Output: (batch, 6, 14, 14)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # C3: Convolution + activation\n",
        "        # Input: (batch, 6, 14, 14) -> Output: (batch, 16, 10, 10)\n",
        "        x = torch.tanh(self.conv2(x))\n",
        "\n",
        "        # S4: Average pooling\n",
        "        # Input: (batch, 16, 10, 10) -> Output: (batch, 16, 5, 5)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # C5: Convolution + activation (original 5x5 kernel design)\n",
        "        # Input: (batch, 16, 5, 5) -> Output: (batch, 120, 1, 1)\n",
        "        x = torch.tanh(self.conv3(x))\n",
        "\n",
        "        # Flatten for fully connected layers\n",
        "        # Input: (batch, 120, 1, 1) -> Output: (batch, 120)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # F6: Fully connected + activation\n",
        "        # Input: (batch, 120) -> Output: (batch, 84)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "\n",
        "        # Output layer (no activation - raw logits)\n",
        "        # Input: (batch, 84) -> Output: (batch, 10)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "i9bB2RaOyc3G"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an alternative and more modern implementation. Here the activation functions have been switched from tanh to ReLU and Max Pooling is used instead of Average Pooling. Feel free to choose either!\n",
        "\n",
        "Unlike the CLIP model, that has been pretrained, these networks are trained from scratch. We will use train portion of the MNIST dataset to do this."
      ],
      "metadata": {
        "id": "u31fRXOc1XA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative modern version with ReLU and MaxPooling\n",
        "class ModernLeNet5(nn.Module):\n",
        "    \"\"\"\n",
        "    Modernized version of LeNet-5 with ReLU activations and max pooling.\n",
        "    Often performs better on MNIST than the original version.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModernLeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(Fun.relu(self.conv1(x)))\n",
        "        x = self.pool(Fun.relu(self.conv2(x)))\n",
        "        x = Fun.relu(self.conv3(x))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = Fun.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Add dropout for regularization\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_OXvBhUE1Nqz"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtain the Training Dataset from FiftyOne's Zoo"
      ],
      "metadata": {
        "id": "2ga26CjkcMAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the training split to train our LeNet model\n",
        "train_val_dataset = foz.load_zoo_dataset(\"mnist\",\n",
        "                                         split='train',\n",
        "                                         dataset_name=\"mnist-train-val\")\n",
        "train_val_dataset.compute_metadata()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NezpK2qr18h3",
        "outputId": "fc8a3e34-8a0f-4220-a770-eb3acbc2de90"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 'train' already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Split 'train' already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'mnist-train-val'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'mnist-train-val'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting into Train and Validation\n",
        "\n",
        "The **validation set** serves as an unbiased evaluation mechanism during model development, acting as a proxy for real-world performance before touching the final test set. While the training set teaches the model to recognize patterns in handwritten digits, the validation set reveals whether the model has truly learned generalizable features or simply memorized the training data, a phenomenon known as overfitting.\n",
        "\n",
        "During training, we monitor both training and validation **loss** simultaneously. **Loss** is a numerical measure of how far off the model's predictions are from the correct answers - lower loss means better performance. We use categorical cross-entropy loss, which penalizes confident wrong predictions more heavily than uncertain ones. **Training loss** typically decreases steadily as the model learns, but **validation loss** tells the real story. If validation loss plateaus or begins increasing while training loss continues decreasing, the model is overfitting and memorizing training-specific details rather than learning robust digit recognition patterns. This signals when to stop training, adjust hyperparameters, or modify the architecture.\n",
        "\n",
        "The validation set also enables **hyperparameter tuning** without contaminating our final evaluation. We can experiment with different learning rates, batch sizes, regularization techniques, or architectural modifications, using validation loss to guide these decisions. Each configuration gets evaluated on the same held-out validation data, ensuring fair comparisons.\n",
        "\n",
        "**Important: the test set remains completely untouched** throughout the development process. Only after we've selected our final model configuration based on validation performance do we evaluate on the test set once, giving us an honest estimate of how the model will perform on truly unseen data. This three-way split (train/validation/test) is fundamental to responsible machine learning development and prevents the subtle \"data leakage\" that can make models appear better than they actually are."
      ],
      "metadata": {
        "id": "328v5V3E7nmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will take a couple of minutes (60k samples).\n",
        "# The images come with the 'train' tag and this must be deleted\n",
        "# at the sample level.\n",
        "for sample in tqdm(train_val_dataset, desc=\"Clearing tags\"):\n",
        "    sample.tags = []\n",
        "    sample.save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M9_3Gri26Wo",
        "outputId": "cc6c3d7e-85ec-42d0-8b60-5b5df0a737a4"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clearing tags: 100%|██████████| 60000/60000 [01:46<00:00, 565.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Correct Train/Validation Split for MNIST Training Dataset\n",
        "\n",
        "# Create random 85%/15% split using tags\n",
        "four.random_split(train_val_dataset,\n",
        "                  {\"train\": 0.85, \"validation\": 0.15},\n",
        "                  # The seed makes the split reproducible\n",
        "                  seed=51)\n",
        "\n",
        "# Verify the split by counting tags\n",
        "tag_counts = train_val_dataset.count_sample_tags()\n",
        "print(\"Tag counts after split:\")\n",
        "print(tag_counts)\n",
        "\n",
        "# Create views for training and validation sets\n",
        "train_view = train_val_dataset.match_tags(\"train\")\n",
        "val_view = train_val_dataset.match_tags(\"validation\")\n",
        "\n",
        "# Verify no overlap between train and validation\n",
        "train_ids = set(train_view.values(\"id\"))\n",
        "val_ids = set(val_view.values(\"id\"))\n",
        "overlap = train_ids.intersection(val_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfPp1Huh2258",
        "outputId": "00a28a93-9a42-4d50-9e76-20af8dd104e5"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag counts after split:\n",
            "{'train': 51000, 'validation': 9000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving the FiftyOne Data Splits to torch Datasets\n",
        "\n",
        "To train our PyTorch model, we need to convert our FiftyOne dataset views into PyTorch `Dataset` objects that can load and preprocess images during training. This bridge between FiftyOne's dataset management and PyTorch's training pipeline is important for maintaining both the metadata and annotations while enabling batch processing. PyTorch's `Dataset` class provides a standardized interface for data loading, handling tasks like image loading, preprocessing transforms, and label conversion. By creating a custom dataset class that works with FiftyOne's file paths and labels, we can leverage PyTorch's `DataLoader` for batching, shuffling, and parallel data loading while preserving all the dataset analysis capabilities that FiftyOne provides. This approach allows us to move between FiftyOne's visual analysis and PyTorch's training workflows without duplicating data or losing the rich metadata we've computed."
      ],
      "metadata": {
        "id": "RvCWt129XuCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom PyTorch Dataset class for MNIST training data\n",
        "class FiftyOneImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, fiftyone_view,\n",
        "                 image_transforms=None,\n",
        "                 label_map=None,\n",
        "                 gt_field=\"ground_truth\"):\n",
        "        self.fiftyone_view = fiftyone_view\n",
        "        self.image_paths = self.fiftyone_view.values(\"filepath\")\n",
        "        self.str_labels = self.fiftyone_view.values(f\"{gt_field}.label\")\n",
        "        self.image_transforms = image_transforms\n",
        "\n",
        "        if label_map is None:\n",
        "            self.label_map = {str(i): i for i in range(10)}  # \"0\"->0, \"1\"->1, etc.\n",
        "        else:\n",
        "            self.label_map = label_map\n",
        "\n",
        "        print(f\"FiftyOneImageDataset initialized with {len(self.image_paths)} samples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('L')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return torch.randn(1, 28, 28), torch.tensor(-1, dtype=torch.long)\n",
        "\n",
        "        if self.image_transforms:\n",
        "            image = self.image_transforms(image)\n",
        "\n",
        "        label_str = self.str_labels[idx]\n",
        "        label_idx = self.label_map.get(label_str, -1)\n",
        "        if label_idx == -1:\n",
        "            print(f\"Warning: Label '{label_str}' not in label_map for image {image_path}\")\n",
        "\n",
        "        return image, torch.tensor(label_idx, dtype=torch.long)"
      ],
      "metadata": {
        "id": "-D6khXPtAd2Q"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the Mean and Standard Deviation\n",
        "\n",
        "Before training neural networks, we compute the **mean and standard deviation** of our input data to apply **standard scaling** (also called normalization or standardization).\n",
        "\n",
        "\n",
        "**Standard Scaling Formula:**\n",
        "\n",
        "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Where:\n",
        "- $z$ = standardized value\n",
        "- $x$ = original pixel value\n",
        "- $\\mu$ = mean of all pixel values in the dataset\n",
        "- $\\sigma$ = standard deviation of all pixel values in the dataset\n",
        "\n",
        "In PyTorch, this is implemented with torch.`transforms.Normalize((mean_intensity), (std_intensity))`\n",
        "\n",
        "This preprocessing step transforms our pixel values to have zero mean and unit variance, which provides several critical benefits:\n",
        "\n",
        "**Why Standard Scaling Matters:**\n",
        "\n",
        "**Gradient Optimization**: Neural networks learn through gradient descent, which works best when input features are on similar scales. Without scaling, features with larger magnitudes (like raw pixel values 0-255) can dominate the gradient updates, leading to slower convergence and unstable training.\n",
        "\n",
        "**Weight Initialization Compatibility**: Modern weight initialization schemes (Xavier, He initialization) assume inputs are roughly centered around zero with unit variance. Standard scaling ensures our data matches these assumptions, preventing vanishing or exploding gradients during early training.\n",
        "\n",
        "**Activation Function Efficiency**: Many activation functions (tanh, sigmoid) work optimally when inputs are centered around zero. Scaled inputs help neurons operate in the most responsive regions of these functions rather than saturating in flat regions.\n",
        "\n",
        "**Learning Rate Stability**: With standardized inputs, we can use higher learning rates without instability, as the optimization landscape becomes more uniform across different dimensions.\n",
        "\n",
        "For MNIST images, we transform raw pixel values from the range [0, 255] to approximately [-1, 1] with mean ≈ 0, creating a more favorable training environment that typically results in faster convergence and better final performance.\n"
      ],
      "metadata": {
        "id": "p_F8qU_vFuS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_stats_fiftyone(fiftyone_view):\n",
        "    \"\"\"\n",
        "    Compute stats directly from FiftyOne using aggregations.\n",
        "    Requires images to be loaded as arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Computing image intensity statistics from FiftyOne view...\")\n",
        "\n",
        "    # Get all image filepaths\n",
        "    filepaths = fiftyone_view.values(\"filepath\")\n",
        "\n",
        "    # Load all pixel values\n",
        "    all_pixels = []\n",
        "\n",
        "    for filepath in tqdm(filepaths):\n",
        "\n",
        "        try:\n",
        "            # Load image as grayscale array\n",
        "            image = Image.open(filepath).convert('L')\n",
        "            # Scale values to the range [0, 1]\n",
        "            pixels = np.array(image, dtype=np.float32) / 255.0\n",
        "            all_pixels.append(pixels.flatten())\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Concatenate all pixel values\n",
        "    all_pixels = np.concatenate(all_pixels)\n",
        "\n",
        "    # Compute statistics\n",
        "    mean = np.mean(all_pixels)\n",
        "    std = np.std(all_pixels)\n",
        "\n",
        "    print(f\"Computed from {len(filepaths)} images\")\n",
        "    print(f\"Total pixels: {len(all_pixels):,}\")\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "mean_intensity, std_intensity = compute_stats_fiftyone(train_view)\n",
        "f\"Mean: {mean_intensity:.4f}, Std: {std_intensity:.4f}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "BwV9Hgh-E-PX",
        "outputId": "ce2b65f9-ba3f-4ebc-a331-c5c1bf06e613"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing image intensity statistics from FiftyOne view...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51000/51000 [00:07<00:00, 6684.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed from 51000 images\n",
            "Total pixels: 39,984,000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mean: 0.1318, Std: 0.3075'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the string labels to numerical values (we need this for the PyTorch dataset)\n",
        "label_map = {string_label: index for index, string_label in enumerate(dataset_classes)}\n",
        "label_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO92PNlTB_Gi",
        "outputId": "66e9ca22-c916-45eb-b191-b943601e466f"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0 - zero': 0,\n",
              " '1 - one': 1,\n",
              " '2 - two': 2,\n",
              " '3 - three': 3,\n",
              " '4 - four': 4,\n",
              " '5 - five': 5,\n",
              " '6 - six': 6,\n",
              " '7 - seven': 7,\n",
              " '8 - eight': 8,\n",
              " '9 - nine': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the PIL images into PyTorch tensors with scaling based on stats from the training set."
      ],
      "metadata": {
        "id": "5u_VZ-eoIpa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize((mean_intensity,), (std_intensity,))\n",
        "])"
      ],
      "metadata": {
        "id": "6Xr_lCpPDvBV"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_train_set = FiftyOneImageDataset(train_view,\n",
        "                                       label_map=label_map,\n",
        "                                       image_transforms=image_transforms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZpMBS8MBbdN",
        "outputId": "77acb8a5-9eea-44b3-f4f4-037b2ce0bc12"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FiftyOneImageDataset initialized with 51000 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_val_set = FiftyOneImageDataset(val_view,\n",
        "                                     label_map=label_map,\n",
        "                                     image_transforms=image_transforms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98QofkjSBqiE",
        "outputId": "c04e5cd2-a4b1-4070-b150-25e23b336d79"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FiftyOneImageDataset initialized with 9000 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create PyTorch DataLoaders\n",
        "\n",
        "DataLoaders wrap our custom datasets and handle the mechanics of training: batching samples together, shuffling data between epochs, and loading images in parallel using multiple CPU cores. The batch size determines how many images the model processes at once, affecting both memory usage and training dynamics. We shuffle the training data to prevent the model from learning spurious patterns based on sample order, but keep validation data unshuffled since evaluation order doesn't matter. Parallel loading with multiple workers speeds up training by preparing the next batch while the GPU processes the current one.\n",
        "\n"
      ],
      "metadata": {
        "id": "t4OX2J221s6u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb36104f",
        "outputId": "5317e62a-1181-4f00-a776-503f059f70b2"
      },
      "source": [
        "# Define batch size (you can adjust this based on your GPU memory)\n",
        "batch_size = 64\n",
        "num_workers = os.cpu_count()  # Number of CPU cores\n",
        "\n",
        "# Create PyTorch DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torch_train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,  # Shuffle training data\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True # Faster transfer between RAM and VRAM\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    torch_val_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, # No need to shuffle validation data\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Train and validation DataLoaders created successfully.\")\n",
        "print(f\"Train DataLoader has {len(train_loader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_loader)} batches.\")"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and validation DataLoaders created successfully.\n",
            "Train DataLoader has 797 batches.\n",
            "Validation DataLoader has 141 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate the Loss Function (Categorical Cross Entropy)\n",
        "\n",
        "**Categorical Cross Entropy** is the standard loss function for multi-class classification problems like MNIST digit recognition. It measures how far our model's predicted probability distribution is from the true distribution (one-hot encoded labels).\n",
        "\n",
        "**Mathematical Formula:**\n",
        "$$\\text{CCE} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n",
        "\n",
        "Where:\n",
        "- $C$ = number of classes (10 for MNIST digits 0-9)\n",
        "- $y_i$ = true label (1 for correct class, 0 for others)\n",
        "- $\\hat{y}_i$ = predicted probability for class $i$\n",
        "\n",
        "**Intuitions about Cross Entropy Loss**: The loss encourages the model to output high confidence (probability close to 1.0) for the correct class and low confidence for incorrect classes.\n",
        "\n",
        "For a perfectly correct prediction (probability = 1.0 for true class), the loss approaches 0 and gradients are small (meaning that there is little change on weights). For very wrong predictions (probability = 0.001 for the ground truth class), the loss approaches infinity, strongly penalizing confident mistakes and forcing the neural network to update its weights.\n",
        "\n",
        "PyTorch implements a numerically stable variant of CCE based on the LogSoftMax function. You can read more about it [here](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
      ],
      "metadata": {
        "id": "nkUDiGQT-U4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce_loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "00k7b9P--XdQ"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Training and Validation for our Custom Model\n",
        "\n",
        "We define two functions to handle the training and validation phases of each epoch. The `train_epoch()` function puts the model in training mode, processes batches through forward passes, computes loss, and updates weights via backpropagation. The `val_epoch()` function switches the model to evaluation mode and computes validation loss without updating weights, giving us an unbiased measure of performance on held-out data. These functions return the average loss across all batches, which we'll track to monitor training progress and detect overfitting."
      ],
      "metadata": {
        "id": "bK27g21hC68B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader):\n",
        "  batch_losses = []\n",
        "  model.train()\n",
        "  for images, labels in tqdm(train_loader, desc=\"Training: \"):\n",
        "      #import pdb; pdb.set_trace()\n",
        "\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      logits = model(images)\n",
        "      loss_value = ce_loss(logits, labels)\n",
        "      # Clear gradients from previous iteration (PyTorch accumulates by default)\n",
        "      optimizer.zero_grad()\n",
        "      # Computes the gradients with backpropagation\n",
        "      loss_value.backward()\n",
        "      # Updates the weights\n",
        "      optimizer.step()\n",
        "\n",
        "      batch_losses.append(loss_value.item())\n",
        "\n",
        "  train_loss = np.mean(batch_losses)\n",
        "  return train_loss\n"
      ],
      "metadata": {
        "id": "zvXszqWQ4dAn"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_epoch(model, val_loader):\n",
        "  batch_losses = []\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for images, labels in tqdm(val_loader, desc=\"Validation: \"):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      logits = model(images)\n",
        "      loss_value = ce_loss(logits, labels)\n",
        "      batch_losses.append(loss_value.item())\n",
        "  val_loss = np.mean(batch_losses)\n",
        "  return val_loss"
      ],
      "metadata": {
        "id": "mAf9bjcX51pf"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Optimizer\n",
        "\n",
        "**Configuring the Learning Algorithm**\n",
        "\n",
        "The optimizer determines how the neural network updates its weights based on computed gradients during training. This choice affects training speed, stability, and final model performance.\n",
        "\n",
        "**Adam (Adaptive Moment Estimation)** is a variant of gradient descent that maintains running averages of both gradients and their squared values, allowing us to adapt the learning rate based on the historical behavior of each weight.\n",
        "\n",
        "```python\n",
        "optimizer = Adam(model.parameters(),\n",
        "                 lr=0.003, betas=(0.9, 0.999),\n",
        "                 eps=1e-08, weight_decay=0)\n",
        "```\n",
        "\n",
        "The learning rate (lr=0.003) controls the step size for weight updates, while the beta settings control how much history to consider when computing the adaptive rates. This configuration provides stable training for most computer vision tasks.\n"
      ],
      "metadata": {
        "id": "GK52LQDkDRfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModernLeNet5().to(device)\n",
        "\n",
        "# Define the optimizer (variant of stochastic gradient descent)\n",
        "optimizer = Adam(model.parameters(),\n",
        "                 lr=0.003, betas=(0.9, 0.999),\n",
        "                 eps=1e-08, weight_decay=0)\n"
      ],
      "metadata": {
        "id": "y-SPncALBtCi"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Checkpointing the Model\n",
        "\n",
        "**Iterative Learning with Performance Monitoring**\n",
        "\n",
        "Training a neural network involves showing the model batches of data, computing prediction errors, and updating weights to minimize those errors. This process continues for multiple epochs, where each epoch represents one complete pass through the entire training dataset.\n",
        "\n",
        "**The Role of Validation-Based Checkpointing**\n",
        "\n",
        "During training, we monitor performance on both training and validation sets. Training loss decreases as the model learns, but validation loss reveals the true generalization capability. The validation set acts as a proxy for real-world performance since the model never sees these samples during weight updates.\n",
        "\n",
        "We save model checkpoints based on validation performance rather than training performance to prevent overfitting. A model might achieve low training loss by memorizing training examples, but this doesn't guarantee good performance on new data. By saving the model weights that achieve the best validation loss, we capture the point where the model has learned generalizable patterns without overfitting to training-specific details.\n",
        "\n",
        "```python\n",
        "if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    best_model = model\n",
        "    torch.save(best_model.state_dict(), model_save_path)\n",
        "    print('Found and saved better weights for the model')\n",
        "```\n",
        "\n",
        "This checkpointing strategy ensures we retain the model configuration that will perform best on unseen test data, even if training continues and validation performance later degrades due to overfitting."
      ],
      "metadata": {
        "id": "pqo9S2i7RjDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_model = None\n",
        "\n",
        "# Define the path to save the model within your hard-drive\n",
        "path = Path(os.getcwd()) # Feel free to change the path\n",
        "\n",
        "model_save_path = path / 'best_lenet.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader)\n",
        "    val_loss = val_epoch(model, val_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "        # Save the best model\n",
        "        torch.save(best_model.state_dict(), model_save_path)\n",
        "        print('Found and saved better weights for the model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT-TdBp8KGuI",
        "outputId": "074d086c-7340-4796-a584-74f033688bab"
      },
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:07<00:00, 110.01it/s]\n",
            "Validation: 100%|██████████| 141/141 [00:01<00:00, 87.08it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 0.2653 - Val Loss: 0.0855\n",
            "Found and saved better weights for the model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:07<00:00, 105.99it/s]\n",
            "Validation: 100%|██████████| 141/141 [00:01<00:00, 91.17it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Train Loss: 0.0980 - Val Loss: 0.0751\n",
            "Found and saved better weights for the model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:07<00:00, 99.98it/s] \n",
            "Validation: 100%|██████████| 141/141 [00:01<00:00, 97.83it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 - Train Loss: 0.0714 - Val Loss: 0.0688\n",
            "Found and saved better weights for the model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:07<00:00, 109.05it/s]\n",
            "Validation: 100%|██████████| 141/141 [00:01<00:00, 88.38it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 - Train Loss: 0.0602 - Val Loss: 0.0619\n",
            "Found and saved better weights for the model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:08<00:00, 97.97it/s] \n",
            "Validation: 100%|██████████| 141/141 [00:01<00:00, 92.74it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 - Train Loss: 0.0545 - Val Loss: 0.0577\n",
            "Found and saved better weights for the model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:07<00:00, 102.47it/s]\n",
            "Validation: 100%|██████████| 141/141 [00:02<00:00, 66.85it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 - Train Loss: 0.0539 - Val Loss: 0.0717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:07<00:00, 107.38it/s]\n",
            "Validation: 100%|██████████| 141/141 [00:01<00:00, 86.27it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 - Train Loss: 0.0445 - Val Loss: 0.0661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:07<00:00, 102.63it/s]\n",
            "Validation: 100%|██████████| 141/141 [00:01<00:00, 82.89it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 - Train Loss: 0.0410 - Val Loss: 0.0611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:07<00:00, 107.40it/s]\n",
            "Validation: 100%|██████████| 141/141 [00:01<00:00, 81.61it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 - Train Loss: 0.0428 - Val Loss: 0.0606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 797/797 [00:07<00:00, 100.44it/s]\n",
            "Validation: 100%|██████████| 141/141 [00:01<00:00, 84.61it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 - Train Loss: 0.0393 - Val Loss: 0.0711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Training vs Validation Loss\n",
        "\n",
        "Plotting training and validation loss over epochs provides insights into model learning dynamics. These curves reveal whether the model is learning, overfitting, or underfitting the data.\n",
        "In healthy training, both curves decrease together, with training loss lower than validation loss. When training loss continues decreasing while validation loss plateaus or increases, this indicates overfitting where the model memorizes training data rather than learning generalizable patterns. If both curves plateau at high values, the model may be underfitting and require more capacity (e.g.more layers or more weights) or more training epochs."
      ],
      "metadata": {
        "id": "HGC7oxwM9AGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ponD-bYqKvQy",
        "outputId": "4e733082-9045-4673-b3fd-e868fef38460"
      },
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reload the Best LeNet Model Weights\n",
        "\n",
        "After training completes, we need to load the saved model weights that achieved the best validation performance. This ensures we use the model configuration that generalizes best to unseen data, rather than the final training state which may have overfitted.\n",
        "The reloading process involves creating a new model instance with the same architecture, then loading the saved state dictionary containing the optimal weights. We must ensure the model is moved to the correct device (CPU or GPU) and set to evaluation mode for inference."
      ],
      "metadata": {
        "id": "NUY0t6oPcmJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path where the best model was saved\n",
        "path = Path(os.getcwd())\n",
        "model_save_path = path / 'best_lenet.pth'\n",
        "\n",
        "# Instantiate a new model with the same architecture\n",
        "# Make sure you use the same model class that was trained\n",
        "loaded_model = ModernLeNet5()\n",
        "\n",
        "# Load the saved state dictionary into the new model instance\n",
        "# Make sure the model is on the correct device (CPU or GPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "\n",
        "# Move the model to the device\n",
        "loaded_model = loaded_model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "print(f\"Model loaded successfully from {model_save_path}\")\n",
        "print(f\"Model is on device: {next(loaded_model.parameters()).device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzA4y5mpOImr",
        "outputId": "637f4614-7a07-4a92-d646-9ac456ce9201"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/best_lenet.pth\n",
            "Model is on device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply our Custom Model to the Test Set\n",
        "\n",
        "We have trained our LeNet-5 model and need to apply it to the test set to generate predictions and evaluate its performance. This step bridges the gap between PyTorch training and FiftyOne's analysis capabilities.\n",
        "\n",
        "**Why Store Predictions as FiftyOne Classifications?**\n",
        "\n",
        "Instead of storing raw predictions as strings or numbers, we use FiftyOne's [`Classification`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) objects, which provide several key advantages:\n",
        "\n",
        "**Structured Data Storage**: Classification objects encapsulate the predicted label, confidence score, and raw logits in a standardized format that FiftyOne understands.\n",
        "\n",
        "**Evaluation Integration**: FiftyOne's evaluation framework (`evaluate_classifications()`) can compare Classification objects against ground truth labels, generating metrics like confusion matrices, per-class accuracy, and performance reports.\n",
        "\n",
        "**Querying and Filtering**: With Classification objects, we can filter samples by confidence thresholds, find misclassifications, or identify uncertain predictions using FiftyOne's query language:\n",
        "\n",
        "```python\n",
        "# Find high-confidence predictions\n",
        "high_conf = dataset.match(F(\"predictions.confidence\") > 0.95)\n",
        "\n",
        "# Find misclassifications  \n",
        "errors = dataset.match(F(\"predictions.label\") != F(\"ground_truth.label\"))\n",
        "```\n",
        "\n",
        "**Visual Analysis**: The FiftyOne App can visualize Classification objects with confidence scores, making it simple to spot patterns in model behavior and identify errors.\n",
        "\n",
        "**Model Comparison**: Storing predictions in this standardized format enables comparison between different models (like our LeNet vs. CLIP's zero-shot classification) using the same evaluation framework.\n",
        "\n",
        "**Confidence-Based Analysis**: The embedded confidence scores allow for analysis like identifying samples where the model is uncertain, which correspond to edge cases or potential labeling errors in the dataset.\n",
        "\n",
        "This approach transforms raw model outputs into queryable metadata that integrates with FiftyOne's computer vision workflow, enabling insights into model performance and behavior."
      ],
      "metadata": {
        "id": "8VVt65BMRbZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply best_model to the test set, store logits and confidence\n",
        "\n",
        "# Create a PyTorch Dataset for the test set\n",
        "torch_test_set = FiftyOneImageDataset(test_dataset,\n",
        "                                      label_map=label_map, # Use the same label map as training\n",
        "                                      image_transforms=image_transforms) # Use the same transforms\n",
        "\n",
        "# Create a PyTorch DataLoader for the test set\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torch_test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, # No need to shuffle test data\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Test DataLoader created successfully.\")\n",
        "print(f\"Test DataLoader has {len(test_loader)} batches.\")\n",
        "\n",
        "# Set the loaded model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Lists to store predictions and logits\n",
        "predictions = []\n",
        "all_logits = []\n",
        "\n",
        "# Run inference on the test set\n",
        "print(\"Applying best LeNet model to the test set...\")\n",
        "with torch.inference_mode(): # Disable gradient calculation\n",
        "    for images, _ in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits = loaded_model(images)\n",
        "        all_logits.append(logits.cpu().numpy()) # Store logits\n",
        "\n",
        "        # Get predicted class indices\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        predictions.extend(predicted.cpu().numpy()) # Store predictions\n",
        "\n",
        "# Concatenate logits from all batches\n",
        "all_logits = np.concatenate(all_logits, axis=0)\n",
        "\n",
        "print(\"Inference on test set complete.\")\n",
        "print(f\"Shape of collected logits: {all_logits.shape}\")\n",
        "print(f\"Number of collected predictions: {len(predictions)}\")\n",
        "\n",
        "# Store the predictions and logits back into the FiftyOne dataset as Classification objects\n",
        "print(\"Storing predictions and logits as FiftyOne Classifications...\")\n",
        "\n",
        "for i, sample in enumerate(test_dataset):\n",
        "    # Get the predicted class index and corresponding class name\n",
        "    predicted_idx = predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx]\n",
        "\n",
        "    # Get logits for this sample\n",
        "    sample_logits = all_logits[i]\n",
        "\n",
        "    # Calculate confidence scores (softmax applied to logits)\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "\n",
        "    # Create FiftyOne Classification object with prediction\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()  # Store raw logits\n",
        "    )\n",
        "\n",
        "    # Store the Classification object in the sample\n",
        "    sample[\"lenet_classification\"] = classification\n",
        "\n",
        "    sample.save()\n",
        "\n",
        "\n",
        "# Verify the stored data structure\n",
        "# We should see Classification objects with label, confidence, and logits fields\n",
        "print(\"\\n=== Verification ===\")\n",
        "sample = test_dataset.first()\n",
        "print(f\"Sample prediction type: {type(sample.lenet_classification)}\")\n",
        "print(f\"Sample prediction: {sample.lenet_classification}\")\n",
        "print(f\"Prediction label: {sample.lenet_classification.label}\")\n",
        "print(f\"Prediction confidence: {sample.lenet_classification.confidence}\")\n",
        "print(f\"Prediction logits shape: {len(sample.lenet_classification.logits)}\")\n",
        "\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL: {session.url}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "rtL5ZfG6YYXe",
        "outputId": "5ada753f-c2fe-441e-e8fd-762dd3a0937b"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FiftyOneImageDataset initialized with 10000 samples.\n",
            "Test DataLoader created successfully.\n",
            "Test DataLoader has 157 batches.\n",
            "Applying best LeNet model to the test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:01<00:00, 97.13it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference on test set complete.\n",
            "Shape of collected logits: (10000, 10)\n",
            "Number of collected predictions: 10000\n",
            "Storing predictions and logits as FiftyOne Classifications...\n",
            "\n",
            "=== Verification ===\n",
            "Sample prediction type: <class 'fiftyone.core.labels.Classification'>\n",
            "Sample prediction: <Classification: {\n",
            "    'id': '6848d87da0733562788f217b',\n",
            "    'tags': [],\n",
            "    'label': '7 - seven',\n",
            "    'confidence': 0.9999990463256836,\n",
            "    'logits': array([-19.30611992,  -6.04000521,  -6.8308115 ,  -7.70924234,\n",
            "            -5.7675271 , -13.8985405 , -25.05943489,  11.61599922,\n",
            "           -10.76372623,  -2.30774975]),\n",
            "}>\n",
            "Prediction label: 7 - seven\n",
            "Prediction confidence: 0.9999990463256836\n",
            "Prediction logits shape: 10\n",
            "\n",
            "FiftyOne App URL: https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a view showing only LeNet's high-confidence predictions\n",
        "high_confidence_view = test_dataset.match(\n",
        "    F(\"lenet_prediction.confidence\") > 0.9\n",
        ")\n",
        "print(f\"\\nHigh confidence predictions (>0.9): {len(high_confidence_view)} samples\")\n",
        "\n",
        "# Create a view showing only LeNet's misclassifications\n",
        "misclassified_view = test_dataset.match(\n",
        "    F(\"lenet_prediction.label\") != F(\"ground_truth.label\")\n",
        ")\n",
        "print(f\"Misclassified samples: {len(misclassified_view)} samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBzGnUgIaYBj",
        "outputId": "6e006122-8859-4b77-99d8-312ea6c3b567"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "High confidence predictions (>0.9): 0 samples\n",
            "Misclassified samples: 10000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating LeNet's Classification Performance\n",
        "\n",
        "After applying our trained LeNet model to the test set, we need to evaluate its performance against the ground truth labels. This evaluation goes beyond simple accuracy to provide detailed insights into where and how the model succeeds or fails.\n",
        "FiftyOne's evaluation framework generates metrics including per-class precision, recall, and F1-scores, along with confusion matrices that reveal which digit pairs the model most often confuses. This analysis helps identify weaknesses and guides future improvements.\n",
        "\n"
      ],
      "metadata": {
        "id": "HAUyD2N1Z039"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_evaluation_results = test_dataset.evaluate_classifications(\n",
        "    \"lenet_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"lenet_eval\")\n",
        "\n",
        "session.refresh()\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "FbdrK2HVdL6t",
        "outputId": "719a8636-24e5-4a72-9f19-46d81ea6d2ec"
      },
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_evaluation_results.print_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdP_CpjJhPRZ",
        "outputId": "9ba8d445-1398-4032-eb8f-452b15bb2adb"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    0 - zero       0.99      0.99      0.99       980\n",
            "     1 - one       1.00      0.99      0.99      1135\n",
            "     2 - two       0.98      0.99      0.99      1032\n",
            "   3 - three       0.95      1.00      0.97      1010\n",
            "    4 - four       0.99      0.99      0.99       982\n",
            "    5 - five       0.99      0.97      0.98       892\n",
            "     6 - six       1.00      0.98      0.99       958\n",
            "   7 - seven       0.98      0.99      0.98      1028\n",
            "   8 - eight       0.99      0.97      0.98       974\n",
            "    9 - nine       0.97      0.98      0.98      1009\n",
            "\n",
            "    accuracy                           0.98     10000\n",
            "   macro avg       0.99      0.98      0.98     10000\n",
            "weighted avg       0.98      0.98      0.98     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_evaluation_results.plot_confusion_matrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "jahmUa2_SuM2",
        "outputId": "5e6bdff2-9c63-46b2-f8d9-50d1548f264a"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fiftyone/core/plots/plotly.py:1591: UserWarning:\n",
            "\n",
            "Interactive plots are currently only supported in Jupyter notebooks. Support outside of notebooks and in Google Colab and Databricks will be included in an upcoming release. In the meantime, you can still use this plot, but note that (i) selecting data will not trigger callbacks, and (ii) you must manually call `plot.show()` to launch a new plot that reflects the current state of an attached session.\n",
            "\n",
            "See https://docs.voxel51.com/user_guide/plots.html#working-in-notebooks for more information.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f9291aee-c5fa-4254-a01c-869bd1a0282a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f9291aee-c5fa-4254-a01c-869bd1a0282a\")) {                    Plotly.newPlot(                        \"f9291aee-c5fa-4254-a01c-869bd1a0282a\",                        [{\"mode\":\"markers\",\"opacity\":0.1,\"x\":[0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9],\"y\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"type\":\"scatter\",\"uid\":\"d0f2b9eb-7360-4cf5-9b63-ba7c68660370\"},{\"colorscale\":[[0.0,\"rgb(255,245,235)\"],[0.125,\"rgb(254,230,206)\"],[0.25,\"rgb(253,208,162)\"],[0.375,\"rgb(253,174,107)\"],[0.5,\"rgb(253,141,60)\"],[0.625,\"rgb(241,105,19)\"],[0.75,\"rgb(217,72,1)\"],[0.875,\"rgb(166,54,3)\"],[1.0,\"rgb(127,39,4)\"]],\"hoverinfo\":\"skip\",\"showscale\":false,\"z\":[[0,0,2,5,4,2,0,5,1,990],[1,0,2,14,0,2,0,3,945,7],[0,3,7,1,0,0,0,1014,0,3],[6,2,1,2,5,2,940,0,0,0],[1,0,0,22,0,865,1,1,0,2],[0,0,2,0,968,0,0,3,0,9],[0,0,1,1007,0,1,0,1,0,0],[1,0,1024,3,0,0,0,3,1,0],[0,1124,2,5,0,0,0,1,2,1],[970,0,3,0,0,0,0,2,1,4]],\"zmax\":1124,\"zmin\":0,\"type\":\"heatmap\",\"uid\":\"d6a856bd-cf0c-41ae-b119-cedc13166acf\"},{\"colorbar\":{\"len\":1,\"lenmode\":\"fraction\"},\"colorscale\":[[0.0,\"rgb(255,245,235)\"],[0.125,\"rgb(254,230,206)\"],[0.25,\"rgb(253,208,162)\"],[0.375,\"rgb(253,174,107)\"],[0.5,\"rgb(253,141,60)\"],[0.625,\"rgb(241,105,19)\"],[0.75,\"rgb(217,72,1)\"],[0.875,\"rgb(166,54,3)\"],[1.0,\"rgb(127,39,4)\"]],\"hovertemplate\":\"\\u003cb\\u003ecount: %{z}\\u003c\\u002fb\\u003e\\u003cbr\\u003eground_truth: %{y}\\u003cbr\\u003elenet_classification: %{x}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"opacity\":0.25,\"z\":[[0,0,2,5,4,2,0,5,1,990],[1,0,2,14,0,2,0,3,945,7],[0,3,7,1,0,0,0,1014,0,3],[6,2,1,2,5,2,940,0,0,0],[1,0,0,22,0,865,1,1,0,2],[0,0,2,0,968,0,0,3,0,9],[0,0,1,1007,0,1,0,1,0,0],[1,0,1024,3,0,0,0,3,1,0],[0,1124,2,5,0,0,0,1,2,1],[970,0,3,0,0,0,0,2,1,4]],\"zmax\":1124,\"zmin\":0,\"type\":\"heatmap\",\"uid\":\"03e61bb1-a512-46aa-9202-7999cae940ed\"}],                        {\"clickmode\":\"event\",\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(237,237,237)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(51,51,51)\"},\"error_y\":{\"color\":\"rgb(51,51,51)\"},\"marker\":{\"line\":{\"color\":\"rgb(237,237,237)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(51,51,51)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(51,51,51)\"},\"baxis\":{\"endlinecolor\":\"rgb(51,51,51)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(51,51,51)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"colorscale\":{\"sequential\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"sequentialminus\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]]},\"colorway\":[\"#F8766D\",\"#A3A500\",\"#00BF7D\",\"#00B0F6\",\"#E76BF3\"],\"font\":{\"color\":\"rgb(51,51,51)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"rgb(237,237,237)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"rgb(237,237,237)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"bgcolor\":\"rgb(237,237,237)\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"bgcolor\":\"rgb(237,237,237)\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"}},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\"}}},\"xaxis\":{\"constrain\":\"domain\",\"range\":[-0.5,9.5],\"tickmode\":\"array\",\"ticktext\":[\"0 - zero\",\"1 - one\",\"2 - two\",\"3 - three\",\"4 - four\",\"5 - five\",\"6 - six\",\"7 - seven\",\"8 - eight\",\"9 - nine\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9]},\"yaxis\":{\"constrain\":\"domain\",\"range\":[-0.5,9.5],\"scaleanchor\":\"x\",\"scaleratio\":1,\"tickmode\":\"array\",\"ticktext\":[\"9 - nine\",\"8 - eight\",\"7 - seven\",\"6 - six\",\"5 - five\",\"4 - four\",\"3 - three\",\"2 - two\",\"1 - one\",\"0 - zero\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9]},\"margin\":{\"r\":0,\"t\":30,\"l\":0,\"b\":0},\"title\":{}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f9291aee-c5fa-4254-a01c-869bd1a0282a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute quantiles of confidence\n",
        "test_dataset.quantiles(\"lenet_classification.confidence\", [0.01, 0.05, 0.25, 0.5, 0.75] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Y0qZVhAqOn",
        "outputId": "a4bb13a4-5764-42d4-fd62-cf9cd6795e49"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6422488689422607, 0.9848902821540833, 0.9999927282333374, 1.0, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_confidence_view = test_dataset.match(F(\"lenet_classification.confidence\") < 0.69)\n",
        "low_confidence_view"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JleYduhreBg6",
        "outputId": "b8731a95-d0db-4564-bb35-a3340987074f"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset:     mnist-test\n",
              "Media type:  image\n",
              "Num samples: 116\n",
              "Sample fields:\n",
              "    id:                             fiftyone.core.fields.ObjectIdField\n",
              "    filepath:                       fiftyone.core.fields.StringField\n",
              "    tags:                           fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:                       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
              "    created_at:                     fiftyone.core.fields.DateTimeField\n",
              "    last_modified_at:               fiftyone.core.fields.DateTimeField\n",
              "    ground_truth:                   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    clip_embeddings:                fiftyone.core.fields.VectorField\n",
              "    clip_zero_shot_classification:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    lenet_classification:           fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    hardness:                       fiftyone.core.fields.FloatField\n",
              "    mistakenness:                   fiftyone.core.fields.FloatField\n",
              "    retrained_lenet_classification: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    retrained_lenet_eval:           fiftyone.core.fields.BooleanField\n",
              "    clip_zero_shot_eval:            fiftyone.core.fields.BooleanField\n",
              "    lenet_eval:                     fiftyone.core.fields.BooleanField\n",
              "View stages:\n",
              "    1. Match(filter={'$expr': {'$lt': [...]}})"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session.view = low_confidence_view\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "lKnI5jJceXMK",
        "outputId": "8d797a97-2c45-46e3-b221-98b4c3cb7bb0"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Sample Hardness and Mistakenness through Logits\n",
        "\n",
        "**Quantifying Prediction Uncertainty and Label Quality**\n",
        "\n",
        "Model logits contain rich information beyond simple predictions. By analyzing the raw output scores before softmax conversion (the logits), we can compute metrics that reveal which samples the model finds challenging and which labels may be questionable.\n",
        "\n",
        "**Hardness** measures prediction uncertainty based on the model's confidence distribution. Samples with high hardness have flat probability distributions, indicating the model struggles to distinguish between classes. These often represent genuinely difficult cases or edge cases in the data.\n",
        "\n",
        "**Mistakenness** identifies samples where the model's confident predictions disagree with ground truth labels. High mistakenness scores suggest potential annotation errors rather than model failures, as the model may have learned correct patterns that conflict with incorrect labels.\n",
        "\n",
        "```python\n",
        "# Compute hardness based on prediction uncertainty\n",
        "fob.compute_hardness(test_dataset, label_field='lenet_classification')\n",
        "\n",
        "# Compute mistakenness to identify potential label errors  \n",
        "fob.compute_mistakenness(test_dataset,\n",
        "                        pred_field=\"lenet_classification\",\n",
        "                        label_field=\"ground_truth\")\n",
        "```\n",
        "\n",
        "These metrics transform raw model scores (logits) into actionable insights for the dataset"
      ],
      "metadata": {
        "id": "9eSfe8gg-yF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hardness is a measure computed based on model prediction output (through\n",
        "#logits) that summarizes a measure of the uncertainty the model had with the\n",
        "#sample. This makes hardness quantitative and can be used to detect things\n",
        "#like hard samples and annotation errors\n",
        "fob.compute_hardness(test_dataset,\n",
        "                     label_field='lenet_classification',\n",
        "                     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv9eUU8H7W8Q",
        "outputId": "5fa01cb8-e89c-4779-9182-f8bf359f1595"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing hardness...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.internal.core.hardness:Computing hardness...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████| 10000/10000 [20.9s elapsed, 0s remaining, 505.2 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████| 10000/10000 [20.9s elapsed, 0s remaining, 505.2 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hardness computation complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.internal.core.hardness:Hardness computation complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate sample mistakenness (how likely the sample is mislabeled)\n",
        "# Samples with high mistakenness often have conflicting model output and ground truth\n",
        "fob.compute_mistakenness(test_dataset,\n",
        "                        pred_field=\"lenet_classification\",\n",
        "                        label_field=\"ground_truth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnVwG6OHpQ7Z",
        "outputId": "6bc17903-d908-4fe1-fcce-a34b9ec6f6ff"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing mistakenness...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.internal.core.mistakenness:Computing mistakenness...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████| 10000/10000 [17.7s elapsed, 0s remaining, 587.4 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████| 10000/10000 [17.7s elapsed, 0s remaining, 587.4 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistakenness computation complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.internal.core.mistakenness:Mistakenness computation complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL: {session.url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9_ckgWTTo6cL",
        "outputId": "d251609a-173e-40db-821a-78337df6aa2f"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FiftyOne App URL: https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Embeddings from the LeNet-5 Model\n",
        "\n",
        "**Extracting Internal Feature Representations**\n",
        "\n",
        "Neural networks process information through multiple layers, with each layer transforming input data into increasingly abstract representations. While we typically focus on the final output layer for predictions, the intermediate layers contain valuable feature representations that reveal what the model has learned about the input data.\n",
        "\n",
        "LeNet-5's architecture builds features hierarchically: early convolutional layers detect edges and simple patterns, while deeper layers combine these into more complex digit-specific features. The fully connected layers before the final classification layer contain rich embeddings that capture the essential characteristics the model uses to distinguish between digit classes.\n",
        "\n",
        "By extracting these learned embeddings, we can analyze how our trained model represents each digit internally. Unlike pre-trained embeddings from models like CLIP, these features are optimized for our specific task of handwritten digit recognition. This makes them particularly valuable for understanding model behavior, identifying challenging samples, and comparing how different digits cluster in the learned feature space.\n",
        "\n",
        "The extraction process uses PyTorch hooks to capture intermediate layer outputs during inference, allowing us to access the 84-dimensional feature vectors from the penultimate layer that encode each sample's representation in the model's learned space.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KAQh1hXIrSS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract Embeddings from LeNet Model Using PyTorch Hooks\n",
        "\n",
        "def extract_lenet_embeddings(model, dataloader, device, layer_name='fc1'):\n",
        "    \"\"\"\n",
        "    Extract embeddings from a specified layer of the LeNet model using PyTorch hooks.\n",
        "\n",
        "    Args:\n",
        "        model: Trained LeNet model\n",
        "        dataloader: PyTorch DataLoader\n",
        "        device: Device to run inference on\n",
        "        layer_name: Name of the layer to extract embeddings from\n",
        "                   Options: 'conv3', 'fc1', or 'fc2'\n",
        "\n",
        "    Returns:\n",
        "        numpy array of embeddings\n",
        "    \"\"\"\n",
        "    # Dictionary to store the embeddings\n",
        "    embeddings_dict = {}\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        \"\"\"Hook function to capture layer outputs\"\"\"\n",
        "        # Flatten the output if it's from conv layers\n",
        "        if len(output.shape) > 2:\n",
        "            embeddings_dict['embeddings'] = output.view(output.size(0), -1).cpu().detach()\n",
        "        else:\n",
        "            embeddings_dict['embeddings'] = output.cpu().detach()\n",
        "\n",
        "    # Register the hook on the specified layer\n",
        "    layer_map = {\n",
        "        'conv3': model.conv3,  # Shape: (batch_size, 120, 1, 1) -> flattened to (batch_size, 120)\n",
        "        'fc1': model.fc1,     # Shape: (batch_size, 84) - most common choice\n",
        "        'fc2': model.fc2      # Shape: (batch_size, 10) - final logits\n",
        "    }\n",
        "\n",
        "    if layer_name not in layer_map:\n",
        "        raise ValueError(f\"Invalid layer_name. Choose from: {list(layer_map.keys())}\")\n",
        "\n",
        "    target_layer = layer_map[layer_name]\n",
        "    hook_handle = target_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    print(f\"Extracting embeddings from {layer_name} layer...\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, _ in tqdm(dataloader, desc=\"Processing batches\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass (hook will capture the embeddings)\n",
        "            _ = model(images)\n",
        "\n",
        "            # Store the captured embeddings\n",
        "            batch_embeddings = embeddings_dict['embeddings'].numpy()\n",
        "            all_embeddings.append(batch_embeddings)\n",
        "\n",
        "    # Remove the hook to clean up\n",
        "    hook_handle.remove()\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "\n",
        "    print(f\"Extracted embeddings shape: {final_embeddings.shape}\")\n",
        "    print(f\"Embedding dimension: {final_embeddings.shape[1]}\")\n",
        "\n",
        "    return final_embeddings"
      ],
      "metadata": {
        "id": "cxsL2g-Rstpq"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d73fcf8b",
        "outputId": "110f4334-ac9f-4b55-d548-417595f3fbae"
      },
      "source": [
        "# Create a new DataLoader for the train_set specifically for inference, ensuring shuffle is False\n",
        "# This uses the torch_train_set which is derived from train_view\n",
        "train_inference_loader = torch.utils.data.DataLoader(\n",
        "    torch_train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,    # CRITICAL: Must be False for ordered predictions\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Set the loaded model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Lists to store predictions and logits for the training set\n",
        "train_predictions = []\n",
        "train_all_logits = []\n",
        "\n",
        "# Run inference on the training set using the NON-SHUFFLED loader\n",
        "print(\"Applying LeNet model to the train_view (using non-shuffled loader)...\")\n",
        "with torch.inference_mode(): # Disable gradient calculation\n",
        "    # Use the new train_inference_loader\n",
        "    for images, _ in tqdm(train_inference_loader, desc=\"Processing train batches for inference\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits = loaded_model(images)\n",
        "        train_all_logits.append(logits.cpu().numpy()) # Store logits\n",
        "\n",
        "        # Get predicted class indices\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        train_predictions.extend(predicted.cpu().numpy()) # Store predictions\n",
        "\n",
        "# Concatenate logits from all batches\n",
        "train_all_logits = np.concatenate(train_all_logits, axis=0)\n",
        "\n",
        "print(\"Inference on train_view complete.\")\n",
        "print(f\"Shape of collected train logits: {train_all_logits.shape}\")\n",
        "print(f\"Number of collected train predictions: {len(train_predictions)}\")\n",
        "\n",
        "# Store the predictions and logits back into the FiftyOne dataset as Classification objects\n",
        "print(\"Storing predictions and logits as FiftyOne Classifications for train_view...\")\n",
        "for i, sample in enumerate(tqdm(train_view, desc=\"Storing train classifications\")):\n",
        "    predicted_idx = train_predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx] # Assuming dataset_classes is consistent\n",
        "    sample_logits = train_all_logits[i]\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()\n",
        "    )\n",
        "    sample[\"lenet_train_classification\"] = classification\n",
        "    sample.save()\n",
        "\n",
        "print(\"Predictions and logits stored successfully as FiftyOne Classifications for train_view.\")"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying LeNet model to the train_view (using non-shuffled loader)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train batches for inference: 100%|██████████| 797/797 [00:06<00:00, 116.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference on train_view complete.\n",
            "Shape of collected train logits: (51000, 10)\n",
            "Number of collected train predictions: 51000\n",
            "Storing predictions and logits as FiftyOne Classifications for train_view...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Storing train classifications: 100%|██████████| 51000/51000 [02:02<00:00, 414.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions and logits stored successfully as FiftyOne Classifications for train_view.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_train_evaluation_results = train_view.evaluate_classifications(\n",
        "    \"lenet_train_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"lenet_train_eval\"\n",
        ")\n",
        "session.refresh()\n",
        "lenet_train_evaluation_results.print_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1RwWM6aa19b",
        "outputId": "e74a8f01-d174-4639-d40d-e3b81afef778"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    0 - zero       1.00      0.99      0.99      5006\n",
            "     1 - one       1.00      0.99      0.99      5727\n",
            "     2 - two       0.99      0.99      0.99      5078\n",
            "   3 - three       0.97      1.00      0.98      5192\n",
            "    4 - four       1.00      0.99      0.99      4931\n",
            "    5 - five       0.99      0.98      0.99      4632\n",
            "     6 - six       1.00      0.99      0.99      5053\n",
            "   7 - seven       0.99      0.99      0.99      5369\n",
            "   8 - eight       0.99      0.98      0.99      4979\n",
            "    9 - nine       0.98      0.99      0.99      5033\n",
            "\n",
            "    accuracy                           0.99     51000\n",
            "   macro avg       0.99      0.99      0.99     51000\n",
            "weighted avg       0.99      0.99      0.99     51000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session = fo.launch_app(train_view, auto=False)\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "MimuXxRsTvP4",
        "outputId": "213e7e43-33fb-401c-e8f2-5ca87f73dfbb"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.session.session:Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a View of the Mislabeled Images in the Training Data\n",
        "\n",
        "**Identifying Potential Annotation Errors Through Model Predictions**\n",
        "\n",
        "When a trained model disagrees with ground truth labels, this often reveals annotation errors rather than model failures. A well-trained neural network that has learned robust patterns from thousands of examples may be more reliable than human annotators on ambiguous cases. This principle becomes valuable for quality control in large datasets where manual verification of every sample is impractical.\n",
        "\n",
        "**The Logic Behind Mislabel Detection**\n",
        "\n",
        "Models learn statistical patterns across the entire dataset. When a model with high accuracy predicts a different label than the ground truth, several scenarios are possible: the model made an error, the annotation is incorrect, or the sample is ambiguous. By examining these disagreements, we can identify samples that warrant human review.\n",
        "\n",
        "For MNIST, known annotation errors exist in the original dataset. Research has identified several hundred ambiguous or mislabeled samples where even human experts disagree on the correct digit. Our trained LeNet model, having learned from 50,000+ examples, may detect these problematic cases more than the original annotators.\n",
        "\n",
        "Creating a filtered view of these disagreements allows us to focus human attention on the most questionable samples. This approach scales annotation quality control from reviewing entire datasets to examining only the cases where model and annotator disagree.\n",
        "\n",
        "This approach to finding annotation errors represents a powerful application of model predictions for dataset improvement, transforming disagreements between model and labels into opportunities for enhanced data quality.\n",
        "\n",
        "We focus on the mislabeled images from the training set, as these are the ones that we should first **augment** to improve the performance of the model."
      ],
      "metadata": {
        "id": "qOivFC0taQk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we create a view that filters\n",
        "mislabeled_train_images_view = \\\n",
        "train_view.match(\n",
        "    F(\"lenet_train_classification.label\")!= F(\"ground_truth.label\"))\n",
        "\n",
        "mislabeled_train_images_view\n"
      ],
      "metadata": {
        "id": "auV2OrzEFzdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdbfd90a-47e5-4953-acd5-a7f32357d1c7"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset:     mnist-train-val\n",
              "Media type:  image\n",
              "Num samples: 510\n",
              "Sample fields:\n",
              "    id:                         fiftyone.core.fields.ObjectIdField\n",
              "    filepath:                   fiftyone.core.fields.StringField\n",
              "    tags:                       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:                   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
              "    created_at:                 fiftyone.core.fields.DateTimeField\n",
              "    last_modified_at:           fiftyone.core.fields.DateTimeField\n",
              "    ground_truth:               fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    lenet_train_classification: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    lenet_embeddings:           fiftyone.core.fields.VectorField\n",
              "    uniqueness:                 fiftyone.core.fields.FloatField\n",
              "    representativeness:         fiftyone.core.fields.FloatField\n",
              "    lenet_train_eval:           fiftyone.core.fields.BooleanField\n",
              "View stages:\n",
              "    1. MatchTags(tags=['train'], bool=True, all=False)\n",
              "    2. Match(filter={'$expr': {'$ne': [...]}})"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session = fo.launch_app(mislabeled_train_images_view, auto=False)\n",
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "TX7nYlhciQHW",
        "outputId": "84b27f31-46e7-4aba-b7d6-7c7b66f086df"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.session.session:Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Embeddings from the Trained LeNet\n",
        "\n",
        "**Capturing Learned Feature Representations**\n",
        "\n",
        "Neural networks learn hierarchical feature representations through their layers. While the final layer produces class predictions, intermediate layers contain rich embeddings that capture the visual patterns the model has learned to distinguish digits. These learned features often differ from pre-trained embeddings like CLIP, as they are optimized for the specific task of handwritten digit recognition.\n",
        "\n",
        "Extracting embeddings from our trained LeNet model allows us to analyze what visual concepts the network has learned. The fully connected layer before the final classification layer (fc1) produces 84-dimensional vectors that represent each digit in the feature space that the model uses for decision-making.\n",
        "\n"
      ],
      "metadata": {
        "id": "m9rqX4iDi6Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embeddings from the fc1 layer (84-dimensional representations)\n",
        "lenet_embeddings = extract_lenet_embeddings(\n",
        "    model=loaded_model,\n",
        "    dataloader=train_loader,\n",
        "    device=device,\n",
        "    layer_name='fc1'  # 84-dimensional embeddings from fully connected layer\n",
        ")\n",
        "\n",
        "print(f\"LeNet embeddings extracted: {lenet_embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkdkbeSFuwKI",
        "outputId": "a18408ea-513d-420c-aead-3fa0dfdbf8d6"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting embeddings from fc1 layer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches: 100%|██████████| 797/797 [00:06<00:00, 115.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted embeddings shape: (51000, 84)\n",
            "Embedding dimension: 84\n",
            "LeNet embeddings extracted: (51000, 84)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store LeNet Embeddings in FiftyOne Dataset\n",
        "\n",
        "**Integrating Model Features with Dataset Metadata**\n",
        "\n",
        "After extracting embeddings from our trained LeNet model, we need to store them in our FiftyOne dataset to enable analysis and visualization. This integration allows us to leverage FiftyOne's powerful querying and visualization capabilities on the learned feature representations.\n",
        "\n",
        "Storing embeddings as sample fields transforms abstract neural network features into queryable fields. We can then compute similarity indices, create visualizations, and analyze how the model's learned representations relate to prediction accuracy and sample characteristics."
      ],
      "metadata": {
        "id": "IErbAydN5hRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Storing LeNet embeddings in FiftyOne dataset...\")\n",
        "\n",
        "# Store embeddings in each sample\n",
        "for index, sample in enumerate(tqdm(train_view, desc=\"Storing embeddings\")):\n",
        "    sample[\"lenet_embeddings\"] = lenet_embeddings[index]\n",
        "    sample.save()\n",
        "\n",
        "print(\"LeNet embeddings stored successfully in samples from train_view.\")\n",
        "\n",
        "# Verify storage\n",
        "sample = train_view.first()\n",
        "print(f\"Sample LeNet embedding shape: {sample.lenet_embeddings.shape}\")\n",
        "print(f\"Embedding type: {type(sample.lenet_embeddings)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iXPUdi3u2v3",
        "outputId": "c2012df3-5705-45c5-fef0-f600d356f1e0"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Storing LeNet embeddings in FiftyOne dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Storing embeddings: 100%|██████████| 51000/51000 [01:37<00:00, 522.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeNet embeddings stored successfully in samples from train_view.\n",
            "Sample LeNet embedding shape: (84,)\n",
            "Embedding type: <class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Image Representativeness and Uniqueness through their Embeddings\n",
        "\n",
        "**Image representativeness** and **uniqueness** are metrics for understanding dataset quality and composition that work hand-in-hand with clustering analysis.\n",
        "\n",
        "**Representativeness** measures how well a sample captures the central characteristics of its cluster, highly representative images sit near cluster centers and exemplify the common visual patterns within each group.\n",
        "\n",
        "**Uniqueness** identifies samples that are distant from any cluster centers, often representing edge cases, rare scenarios, or potential annotation errors that clustering algorithms struggle to categorize.\n",
        "\n",
        "These scores are normalized to [0, 1] and become particularly insightful when viewed alongside clustering results. Representative samples serve as ideal cluster exemplars for understanding what each group represents, while highly unique samples often fall between clusters or form singleton groups, indicating unusual data points that may require special handling in our machine learning pipeline.\n"
      ],
      "metadata": {
        "id": "FISLQ-LuSlaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Compute Uniqueness and Representativeness for LeNet Embeddings\n",
        "\n",
        "print(\"Computing uniqueness scores based on LeNet embeddings...\")\n",
        "fob.compute_uniqueness(train_view, embeddings=lenet_embeddings)\n",
        "\n",
        "print(\"Computing representativeness scores based on LeNet embeddings...\")\n",
        "fob.compute_representativeness(train_view, embeddings=lenet_embeddings)\n",
        "\n",
        "print(\"Uniqueness and representativeness computation complete.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huEFWGbiu6kV",
        "outputId": "3fdcb0a2-8d8a-407f-d7df-cfa2b497b668"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing uniqueness scores based on LeNet embeddings...\n",
            "Computing uniqueness...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.internal.core.uniqueness:Computing uniqueness...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniqueness computation complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.internal.core.uniqueness:Uniqueness computation complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing representativeness scores based on LeNet embeddings...\n",
            "Computing representativeness...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.internal.core.representativeness:Computing representativeness...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing clusters for 51000 embeddings; this may take awhile...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.internal.core.representativeness:Computing clusters for 51000 embeddings; this may take awhile...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representativeness computation complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.internal.core.representativeness:Representativeness computation complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniqueness and representativeness computation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create 2D Visualizations of LeNet Embeddings\n",
        "Reducing High-Dimensional Features to Visual Form\n",
        "Our LeNet embeddings exist in 84-dimensional space, making them impossible to visualize directly. Dimensionality reduction techniques transform these high-dimensional vectors into 2D points that preserve important relationships between samples.\n",
        "\n",
        "Principal Component Analysis (PCA) finds the directions of maximum variance in the data and projects samples onto the two most important axes. This linear transformation preserves global structure and shows which samples vary most from the dataset center.\n",
        "\n",
        "\n",
        "UMAP (Uniform Manifold Approximation and Projection) uses non-linear techniques to preserve local neighborhoods while revealing cluster structure. UMAP often separates distinct groups more clearly than PCA, making it valuable for identifying how the model groups similar digits.\n"
      ],
      "metadata": {
        "id": "DUt9Tx2z6LiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating 2D visualizations of LeNet embeddings...\")\n",
        "\n",
        "# PCA visualization\n",
        "pca_viz_lenet = fob.compute_visualization(\n",
        "    train_view,\n",
        "    method=\"pca\",\n",
        "    embeddings=\"lenet_embeddings\",\n",
        "    num_dims=2,\n",
        "    brain_key=\"pca_lenet_embeddings\"\n",
        ")\n",
        "\n",
        "# UMAP visualization\n",
        "umap_viz_lenet = fob.compute_visualization(\n",
        "    train_view,\n",
        "    method=\"umap\",\n",
        "    embeddings=\"lenet_embeddings\",\n",
        "    num_dims=2,\n",
        "    brain_key=\"umap_lenet_embeddings\"\n",
        ")\n",
        "\n",
        "print(\"2D visualizations created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "e2e86d3c470e4ded988b11ed9b139e82",
            "4abff2f37a6f4214861f96df674c2ce5",
            "5561f4460ab2483295489c0512f185c3",
            "8b3287df74b04bcb9f83c02004ceff88",
            "ec8999eb8ab448fb83fc6b717f1f7160",
            "08b1ca31aef24559b76f67b149f89ed5",
            "4c151c74ac354bf7bef901dae0718b2f",
            "17ae3862843448da8b3e28fed43ccf35",
            "429186289f9f41d2a9dea3d284ac42f5",
            "0c33abcb029f4115a631cc6f9bf6776b",
            "754401b489744ce9a607d340b7acbd3f"
          ]
        },
        "id": "kIIRMjCi6C1S",
        "outputId": "82d8866c-6589-406d-fa67-26fd42ebac86"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating 2D visualizations of LeNet embeddings...\n",
            "Generating visualization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.visualization:Generating visualization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating visualization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.brain.visualization:Generating visualization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UMAP( verbose=True)\n",
            "Wed Jun 11 01:20:32 2025 Construct fuzzy simplicial set\n",
            "Wed Jun 11 01:20:32 2025 Finding Nearest Neighbors\n",
            "Wed Jun 11 01:20:32 2025 Building RP forest with 16 trees\n",
            "Wed Jun 11 01:20:32 2025 NN descent for 16 iterations\n",
            "\t 1  /  16\n",
            "\t 2  /  16\n",
            "\t 3  /  16\n",
            "\tStopping threshold met -- exiting after 3 iterations\n",
            "Wed Jun 11 01:20:34 2025 Finished Nearest Neighbor Search\n",
            "Wed Jun 11 01:20:34 2025 Construct embedding\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs completed:   0%|            0/200 [00:00]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2e86d3c470e4ded988b11ed9b139e82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tcompleted  0  /  200 epochs\n",
            "\tcompleted  20  /  200 epochs\n",
            "\tcompleted  40  /  200 epochs\n",
            "\tcompleted  60  /  200 epochs\n",
            "\tcompleted  80  /  200 epochs\n",
            "\tcompleted  100  /  200 epochs\n",
            "\tcompleted  120  /  200 epochs\n",
            "\tcompleted  140  /  200 epochs\n",
            "\tcompleted  160  /  200 epochs\n",
            "\tcompleted  180  /  200 epochs\n",
            "Wed Jun 11 01:21:01 2025 Finished embedding\n",
            "2D visualizations created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Uniqueness and Representativeness with Dataset Aggregations\n",
        "\n",
        "**Understanding Score Distributions Through Statistical Analysis**\n",
        "\n",
        "We can use FiftyOne's aggregation methods to explore the distribution of uniqueness and representativeness values across our dataset. This analysis helps us identify meaningful thresholds for categorizing our samples based on their statistical properties.\n",
        "\n",
        "**Quantile Analysis for Threshold Selection**\n",
        "\n",
        "Quantiles divide our data into meaningful segments, revealing the distribution shape and enabling principled threshold selection. The 25th, 50th (median), 75th, and 95th percentiles help us understand where most samples fall and identify the truly exceptional cases.\n",
        "\n",
        "For uniqueness scores, the 95th percentile reveals the most unusual samples that fall far from any cluster centers. These often represent edge cases, annotation errors, or rare variants that deserve special attention. The 75th percentile for representativeness identifies samples that exemplify their clusters well, making them ideal candidates for visualization or as training exemplars.\n",
        "\n",
        "```python\n",
        "# Get quantiles for uniqueness scores\n",
        "uniqueness_quantiles = train_view.aggregate(fo.Quantiles(\"uniqueness\",\n",
        "                                          [0.05, 0.25, 0.5, 0.75, 0.95]))\n",
        "\n",
        "# Get quantiles for representativeness scores  \n",
        "repr_quantiles = train_view.aggregate(fo.Quantiles(\"representativeness\",\n",
        "                                    [0.05, 0.25, 0.5, 0.75, 0.95]))\n",
        "```\n",
        "\n",
        "These quantile boundaries enable us to create filtered views targeting specific sample types, such as the top 5% most unique samples or the most representative examples from each cluster."
      ],
      "metadata": {
        "id": "988VFPHSIGII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get quantiles for uniqueness based on LeNet embeddings\n",
        "uniqueness_quantiles = train_view.aggregate(fo.Quantiles(\"uniqueness\",\n",
        "[0.05, 0.25, 0.5, 0.75, 0.95]))\n",
        "quantile_labels = [\"5th\", \"25th\", \"50th\", \"75th\", \"95th\"]\n",
        "uniqueness_pairs = [f\"{label}: {val:.2f}\" for label, val in zip(quantile_labels, uniqueness_quantiles)]\n",
        "print(f\"Uniqueness quantiles: {', '.join(uniqueness_pairs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz8rCYfFu_g6",
        "outputId": "6f4fd53f-c1cf-4795-a97b-de038954d27c"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniqueness quantiles: 5th: 0.07, 25th: 0.10, 50th: 0.14, 75th: 0.20, 95th: 0.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get quantiles for representativeness based on LeNet embeddings\n",
        "repr_quantiles = train_view.aggregate(fo.Quantiles(\"representativeness\",\n",
        "[0.05, 0.25, 0.5, 0.75, 0.95]))\n",
        "repr_pairs = [f\"{label}: {val:.2f}\" for label, val in zip(quantile_labels, repr_quantiles)]\n",
        "print(f\"Representativeness quantiles: {', '.join(repr_pairs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmdFZa82BtDE",
        "outputId": "06fbdde8-9cfe-43f8-e00c-635dc065967d"
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representativeness quantiles: 5th: 0.16, 25th: 0.24, 50th: 0.32, 75th: 0.41, 95th: 0.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use filtered Views of the dataset to get the most unique values and launch the app from them."
      ],
      "metadata": {
        "id": "-Hyl2_XYCnef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create Views for Most Unique and Most Representative Samples\n",
        "\n",
        "# Most unique samples (top 5% - these are often edge cases or outliers)\n",
        "most_unique = train_view.match(F(\"uniqueness\") > uniqueness_quantiles[4])  # > 95th percentile\n",
        "print(f\"Most unique samples: {len(most_unique)} samples\")\n",
        "\n",
        "# Most representative samples (top 5% - these exemplify their clusters well)\n",
        "most_representative = train_view.match(F(\"representativeness\") > repr_quantiles[4])  # > 95th percentile\n",
        "print(f\"Most representative samples: {len(most_representative)} samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld0QsUJyCyiE",
        "outputId": "a9ed0ff9-9abf-4f1a-966d-39fe9222584c"
      },
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most unique samples: 2550 samples\n",
            "Most representative samples: 2550 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Launch FiftyOne App to Visualize Results\n",
        "\n",
        "session = fo.launch_app(most_unique, auto=False)\n",
        "print(f\"FiftyOne App URL: {session.url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "d5VZ7AE3vCCG",
        "outputId": "18881ac5-acef-40a9-ebaa-8dbf7ddfdb52"
      },
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.session.session:Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FiftyOne App URL: https://5151-gpu-t4-hm-a2s0ieffqkw7-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmenting the Misclassified Training Samples and Retrain\n",
        "\n"
      ],
      "metadata": {
        "id": "waBtOaOoaaUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Effective Augmentations for MNIST\n",
        "\n",
        "\n",
        "### Geometric Transformations:\n",
        "\n",
        "* Small rotations (±10-15 degrees): Handwritten digits naturally vary in orientation\n",
        "* Small translations (±2-3 pixels): Accounts for centering variations in digit positioning\n",
        "* Slight scaling (0.9-1.1x): Handles size variations in handwriting\n",
        "* Moderate elastic deformations are  useful for MNIST because they simulate the natural variations in handwriting style - imagine stretching and compressing parts of a digit as different people might write them.\n",
        "### Why These Work\n",
        "The principle behind effective augmentation is creating realistic variations that preserve the digit's identity while exposing the model to plausible distortions. MNIST digits are centered and normalized, so augmentations should introduce controlled variability without making digits unrecognizable.\n",
        "### Augmentations to Avoid\n",
        "* Heavy distortions like large rotations (>20°), extreme scaling, or aggressive elastic deformation can make digits ambiguous - a rotated \"6\" might look like a \"9\", or a heavily stretched \"1\" might resemble a \"7\".\n",
        "\n",
        "* Color-based augmentations (brightness, contrast) have limited benefit since MNIST is grayscale and already normalized."
      ],
      "metadata": {
        "id": "H_uLxSnsm0rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retraining and Final Evaluation\n",
        "\n",
        "**Measuring the Impact of Augmented Data**\n",
        "\n",
        "After identifying misclassified training samples and creating augmented versions, we retrain our model to measure performance improvements. This process demonstrates how targeted data augmentation can address specific model weaknesses while maintaining overall performance.\n",
        "\n",
        "The retraining phase uses the original best model weights as a starting point, then fine-tunes on the combined dataset containing both original training data and augmented versions of problematic samples. We use a lower learning rate to preserve learned features while adapting to the new data.\n",
        "\n",
        "Final evaluation compares the retrained model against both the original LeNet performance and CLIP's zero-shot results. This comparison reveals the relative benefits of supervised learning with augmentation versus pre-trained model capabilities.\n",
        "\n",
        "```python\n",
        "# Load best model for retraining\n",
        "retrain_model = ModernLeNet5()\n",
        "retrain_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "\n",
        "# Retrain with combined dataset\n",
        "combined_train_dataset = ConcatDataset([torch_train_set, augmented_dataset])\n",
        "```\n",
        "\n",
        "The evaluation metrics quantify whether targeted augmentation successfully improved model robustness and generalization on the held-out test set."
      ],
      "metadata": {
        "id": "jujitJcyCvdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data Augmentation for Misclassified Training Samples\n",
        "\n",
        "print(f\"Number of misclassified training samples: {len(mislabeled_train_images_view)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWhQyNotngBu",
        "outputId": "1a324702-7a7e-4004-94fc-0bf5ce9f8b69"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of misclassified training samples: 510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Augmentations\n",
        "\n",
        "**Configuring Transformations for MNIST Digit Recognition**\n",
        "\n",
        "Data augmentation applies controlled transformations to training images, creating variations that help models generalize to new handwriting styles. For MNIST, effective augmentations simulate natural variations in digit writing without changing the digit's identity.\n",
        "\n",
        "**Seeding for Reproducibility**: Set random seeds before defining augmentations to ensure consistent results across training runs. This enables reproducible experiments and fair model comparisons.\n",
        "\n",
        "**MNIST-Specific Transformations**: Small rotations, translations, and elastic deformations work well for handwritten digits. These transformations mimic natural handwriting variations while preserving digit readability. Avoid extreme distortions that could make a \"6\" look like a \"9\" or render digits unrecognizable."
      ],
      "metadata": {
        "id": "VK3EL0cHf6tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random state object\n",
        "random_state = np.random.RandomState(51)\n",
        "\n",
        "mnist_augmentations = A.Compose([\n",
        "\n",
        "    # Use Affine transform for shifting, scaling, and rotating\n",
        "    A.Affine(\n",
        "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},  # ±10% translation\n",
        "        scale=(0.9, 1.1),     # ±10% scaling\n",
        "        rotate=(-5, 5),     # ±5° rotation\n",
        "        p=0.8\n",
        "    ),\n",
        "\n",
        "    # Elastic deformations to simulate handwriting style variations\n",
        "    A.ElasticTransform(\n",
        "        alpha=20,             # Strength of distortion\n",
        "        sigma=5,              # Smoothness of distortion\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        p=0.6\n",
        "    ),\n",
        "\n",
        "    # Slight Gaussian noise to increase robustness\n",
        "    A.GaussNoise(\n",
        "        p=0.3,\n",
        "        std_range=(0.01, 0.02)\n",
        "    )\n",
        "], random_state=random_state)\n"
      ],
      "metadata": {
        "id": "Cw0Edh8ZEG_l"
      },
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the torch Dataset for the Augmented Set\n",
        "\n",
        "**Building a Custom PyTorch Dataset for Augmented Training Data**\n",
        "\n",
        "Creating a PyTorch Dataset for augmented data requires handling multiple variations of problematic samples while maintaining compatibility with PyTorch's DataLoader system. Our `AugmentedMNISTDataset` class applies augmentations during training, ensuring each epoch sees different variations of misclassified samples.\n",
        "\n",
        "**Key Design Principles:**\n",
        "\n",
        "**Dynamic Augmentation**: Apply transformations during data loading rather than pre-generating samples. This saves disk space while providing variation potential.\n",
        "\n",
        "**Deterministic Access**: Use mathematical mapping for consistent sample ordering while allowing multiple augmented versions of each base sample. The formula `base_idx = idx // (augment_factor + 1)` determines which original sample to use.\n",
        "\n",
        "**Integration**: Inherit from PyTorch's `Dataset` interface for compatibility with DataLoader features like batching, shuffling, and multi-process loading.\n",
        "\n",
        "**Memory and Performance Considerations:**\n",
        "\n",
        "**Memory Usage**: Store file paths rather than loaded images to maintain small memory footprint regardless of augmentation factor.\n",
        "\n",
        "**Caching Strategy**: Consider implementing an LRU cache to store recent base images, reducing disk I/O during training.\n",
        "\n",
        "**Augmentation Randomness**: Each call to `__getitem__` may produce different results due to random augmentation parameters.\n",
        "\n",
        "**Error Resilience**: Robust error handling ensures training continues even if individual samples fail to load or augment.\n",
        "\n",
        "This design provides a foundation for targeted data augmentation while maintaining flexibility to experiment with different augmentation strategies."
      ],
      "metadata": {
        "id": "f11PddjugRS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AugmentedMNISTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch dataset that applies augmentations to misclassified MNIST samples.\n",
        "    Each sample can be augmented multiple times to create more training data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fiftyone_view, label_map, base_transforms,\n",
        "                 augmentations=None, augment_factor=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fiftyone_view: FiftyOne view of misclassified samples\n",
        "            label_map: Mapping from string labels to indices\n",
        "            base_transforms: Base PyTorch transforms (normalization, etc.)\n",
        "            augmentations: Albumentations transform pipeline\n",
        "            augment_factor: How many augmented versions to create per sample\n",
        "        \"\"\"\n",
        "        self.image_paths = fiftyone_view.values(\"filepath\")\n",
        "        self.str_labels = fiftyone_view.values(\"ground_truth.label\")\n",
        "        self.label_map = label_map\n",
        "        self.base_transforms = base_transforms\n",
        "        self.augmentations = augmentations\n",
        "        self.augment_factor = augment_factor\n",
        "\n",
        "        print(f\"AugmentedMNISTDataset: {len(self.image_paths)} base samples\")\n",
        "        print(f\"With augmentation factor {augment_factor}: {len(self)} total samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths) * (self.augment_factor + 1)  # +1 for original\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Determine which base sample and whether to augment\n",
        "        base_idx = idx // (self.augment_factor + 1)\n",
        "        aug_idx = idx % (self.augment_factor + 1)\n",
        "\n",
        "        # Load image\n",
        "        image_path = self.image_paths[base_idx]\n",
        "        image = Image.open(image_path).convert('L')\n",
        "\n",
        "        # Convert to numpy for albumentations\n",
        "        image_np = np.array(image, dtype=np.uint8)\n",
        "\n",
        "        # Apply augmentation if not the first version (original)\n",
        "        if aug_idx > 0 and self.augmentations is not None:\n",
        "            augmented = self.augmentations(image=image_np)\n",
        "            image_np = augmented['image']\n",
        "\n",
        "        # Convert back to PIL for PyTorch transforms\n",
        "        image = Image.fromarray(image_np, mode='L')\n",
        "\n",
        "        # Apply base transforms (normalization, tensor conversion)\n",
        "        if self.base_transforms:\n",
        "            image = self.base_transforms(image)\n",
        "\n",
        "        # Get label\n",
        "        label_str = self.str_labels[base_idx]\n",
        "        label_idx = self.label_map.get(label_str, -1)\n",
        "\n",
        "        return image, torch.tensor(label_idx, dtype=torch.long)\n",
        "\n"
      ],
      "metadata": {
        "id": "DTZEWNCHDejq"
      },
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_dataset = AugmentedMNISTDataset(\n",
        "    mislabeled_train_images_view,\n",
        "    label_map=label_map,\n",
        "    base_transforms=image_transforms,\n",
        "    augmentations=mnist_augmentations,\n",
        "    augment_factor=3  # Create 3 augmented versions per misclassified sample\n",
        ")\n",
        "\n",
        "print(f\"Original misclassified samples: {len(mislabeled_train_images_view)}\")\n",
        "print(f\"Total augmented dataset size: {len(augmented_dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGH3ZAyrDM1A",
        "outputId": "6bb3386c-d28f-4283-8d9f-8b6fc94824ae"
      },
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AugmentedMNISTDataset: 510 base samples\n",
            "With augmentation factor 3: 2040 total samples\n",
            "Original misclassified samples: 510\n",
            "Total augmented dataset size: 2040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Combined Training Dataset\n"
      ],
      "metadata": {
        "id": "h3EQSANOhriR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Combine original training data with augmented misclassified samples\n",
        "combined_train_dataset = ConcatDataset([augmented_dataset,torch_train_set])\n",
        "\n",
        "print(f\"Original training set size: {len(torch_train_set)}\")\n",
        "print(f\"Augmented misclassified samples: {len(augmented_dataset)}\")\n",
        "print(f\"Combined training set size: {len(combined_train_dataset)}\")\n",
        "\n",
        "# Create new DataLoader for combined dataset\n",
        "combined_train_loader = torch.utils.data.DataLoader(\n",
        "    combined_train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,  # Shuffle the combined dataset\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Combined DataLoader has {len(combined_train_loader)} batches.\")\n",
        "\n",
        "\n",
        "## Load Best Model for Retraining\n",
        "\n",
        "# Load the previously saved best model\n",
        "best_model_path = Path(os.getcwd()) / 'best_lenet.pth'\n",
        "retrain_model = ModernLeNet5()\n",
        "retrain_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "retrain_model = retrain_model.to(device)\n",
        "\n",
        "print(f\"Loaded best model from {best_model_path} for retraining\")\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "retrain_optimizer = Adam(retrain_model.parameters(),\n",
        "                        lr=0.001,  # Lower learning rate for fine-tuning\n",
        "                        betas=(0.9, 0.999),\n",
        "                        eps=1e-08,\n",
        "                        weight_decay=1e-4)  # Small weight decay for regularization\n",
        "\n",
        "print(\"Optimizer configured for retraining with lower learning rate\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpAymXsyhqC8",
        "outputId": "1a5fa4b3-0c83-49ec-ea38-6a7061ed11ca"
      },
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training set size: 51000\n",
            "Augmented misclassified samples: 2040\n",
            "Combined training set size: 53040\n",
            "Combined DataLoader has 829 batches.\n",
            "Loaded best model from /content/best_lenet.pth for retraining\n",
            "Optimizer configured for retraining with lower learning rate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Sample Augmentations in FiftyOne\n"
      ],
      "metadata": {
        "id": "CBokBRd7L9j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a small dataset with original and augmented versions for visualization\n",
        "def create_augmentation_samples(view, augmentations, num_samples=6):\n",
        "    \"\"\"Create FiftyOne samples showing original and augmented versions\"\"\"\n",
        "\n",
        "    # Create a new dataset for visualization\n",
        "    aug_viz_dataset = fo.Dataset(\"augmented_set\", persistent=False)\n",
        "\n",
        "    sample_paths = view.values(\"filepath\")[:num_samples]\n",
        "    sample_labels = view.values(\"ground_truth.label\")[:num_samples]\n",
        "\n",
        "    for i, (path, label) in enumerate(zip(sample_paths, sample_labels)):\n",
        "        # Load original image\n",
        "        original_image = Image.open(path).convert('L')\n",
        "        original_np = np.array(original_image)\n",
        "\n",
        "        # Create sample for original image\n",
        "        original_sample = fo.Sample(filepath=path)\n",
        "        original_sample.tags = [\"original\"]\n",
        "        original_sample[\"ground_truth\"] = fo.Classification(label=label)\n",
        "        original_sample[\"augmentation_type\"] = \"original\"\n",
        "        aug_viz_dataset.add_sample(original_sample)\n",
        "\n",
        "        # Create 3 augmented versions\n",
        "        for aug_idx in range(3):\n",
        "            # Apply augmentation\n",
        "            augmented = augmentations(image=original_np)['image']\n",
        "\n",
        "            # Save augmented image temporarily\n",
        "            aug_image = Image.fromarray(augmented, mode='L')\n",
        "            temp_path = f\"/tmp/aug_{i}_{aug_idx}.png\"\n",
        "            aug_image.save(temp_path)\n",
        "\n",
        "            # Create FiftyOne sample for augmented image\n",
        "            aug_sample = fo.Sample(filepath=temp_path)\n",
        "            aug_sample.tags = [\"augmented\"]\n",
        "            aug_sample[\"ground_truth\"] = fo.Classification(label=label)\n",
        "            aug_sample[\"augmentation_type\"] = f\"augmented_{aug_idx + 1}\"\n",
        "            aug_sample[\"original_sample_id\"] = str(i)\n",
        "\n",
        "            aug_viz_dataset.add_sample(aug_sample)\n",
        "\n",
        "    return aug_viz_dataset\n",
        "\n",
        "print(\"Creating augmentation visualization dataset...\")\n",
        "\n",
        "# Create the visualization dataset\n",
        "aug_viz_dataset = create_augmentation_samples(\n",
        "    mislabeled_train_images_view,\n",
        "    mnist_augmentations,\n",
        "    num_samples=6\n",
        ")\n",
        "\n",
        "print(f\"Created visualization dataset with {len(aug_viz_dataset)} samples\")\n",
        "print(f\"Original samples: {len(aug_viz_dataset.match_tags('original'))}\")\n",
        "print(f\"Augmented samples: {len(aug_viz_dataset.match_tags('augmented'))}\")\n",
        "\n",
        "# Launch FiftyOne App to visualize the augmentations\n",
        "aug_session = fo.launch_app(aug_viz_dataset, auto=False)\n",
        "print(f\"\\nAugmentation Visualization URL: {aug_session.url}\")\n",
        "\n",
        "# Create views for easy comparison\n",
        "original_view = aug_viz_dataset.match_tags(\"original\")\n",
        "augmented_view = aug_viz_dataset.match_tags(\"augmented\")\n",
        "\n",
        "print(f\"\\nTo compare:\")\n",
        "print(f\"- View original samples: aug_session.view = original_view\")\n",
        "print(f\"- View augmented samples: aug_session.view = augmented_view\")\n",
        "print(f\"- Group by original sample: Use 'original_sample_id' field to group related samples\")\n",
        "\n",
        "# Add some helpful aggregations\n",
        "print(f\"\\nSample distribution:\")\n",
        "print(f\"By augmentation type: {aug_viz_dataset.count_values('augmentation_type')}\")\n",
        "print(f\"By ground truth label: {aug_viz_dataset.count_values('ground_truth.label')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "508hMKjcFKfY",
        "outputId": "367c14a5-1391-4745-f478-45ff3be5f3ac"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating augmentation visualization dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Dataset name 'augmented_set' is not available",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-313-933998890>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Create the visualization dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m aug_viz_dataset = create_augmentation_samples(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mmislabeled_train_images_view\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmnist_augmentations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-313-933998890>\u001b[0m in \u001b[0;36mcreate_augmentation_samples\u001b[0;34m(view, augmentations, num_samples)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create a new dataset for visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0maug_viz_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"augmented_set\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msample_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filepath\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fiftyone/core/singletons.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, name, _create, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m         ):\n\u001b[1;32m     36\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m  \u001b[0;31m# `__init__` may have changed `name`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fiftyone/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, persistent, overwrite, _create, _virtual, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             doc, sample_doc_cls, frame_doc_cls = _create_dataset(\n\u001b[0m\u001b[1;32m    327\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersistent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fiftyone/core/dataset.py\u001b[0m in \u001b[0;36m_create_dataset\u001b[0;34m(obj, name, persistent, _patches, _frames, _clips, _src_collection)\u001b[0m\n\u001b[1;32m   8345\u001b[0m     \u001b[0m_src_collection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8346\u001b[0m ):\n\u001b[0;32m-> 8347\u001b[0;31m     \u001b[0mslug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_dataset_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8349\u001b[0m     \u001b[0m_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fiftyone/core/dataset.py\u001b[0m in \u001b[0;36m_validate_dataset_name\u001b[0;34m(name, skip)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mclashing_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclashing_name_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclashing_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset name '{name}' is not available\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Dataset name 'augmented_set' is not available"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/augmentation_mnist_vis.png?raw=true)"
      ],
      "metadata": {
        "id": "pjfiVoK0HTSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrain for n Epochs\n",
        "\n",
        "**Fine-tuning with Augmented Data**\n",
        "\n",
        "The retraining process combines our original training data with the augmented versions of misclassified samples, allowing the model to learn from its previous mistakes. We use a lower learning rate to preserve the knowledge already learned while adapting to the new augmented examples.\n",
        "\n",
        "During retraining, we monitor both training and validation loss to ensure the model improves without overfitting. The validation set continues to serve as our guide for saving the best model weights, ensuring we capture improvements in generalization rather than just memorization of the augmented data.\n",
        "\n",
        "```python\n",
        "for epoch in range(retrain_epochs):\n",
        "    train_loss = train_epoch(retrain_model, combined_train_loader)\n",
        "    val_loss = val_epoch(retrain_model, val_loader)\n",
        "    \n",
        "    if val_loss < best_retrain_val_loss:\n",
        "        torch.save(retrain_model.state_dict(), retrain_model_save_path)\n",
        "```\n",
        "\n",
        "This targeted approach allows us to address specific model weaknesses while maintaining overall performance on the broader dataset."
      ],
      "metadata": {
        "id": "c0sIham2JVEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting retraining with augmented data...\")\n",
        "\n",
        "retrain_epochs = 10\n",
        "retrain_losses = []\n",
        "retrain_val_losses = []\n",
        "\n",
        "# Track the best validation loss during retraining\n",
        "best_retrain_val_loss = float('inf')\n",
        "retrain_model_save_path = Path(os.getcwd()) / 'retrained_lenet.pth'\n",
        "\n",
        "for epoch in range(retrain_epochs):\n",
        "    print(f\"\\n--- Retrain Epoch {epoch+1}/{retrain_epochs} ---\")\n",
        "\n",
        "    # Training phase\n",
        "    retrain_model.train()\n",
        "    batch_losses = []\n",
        "\n",
        "    for images, labels in tqdm(combined_train_loader, desc=\"Retraining\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = retrain_model(images)\n",
        "        loss_value = ce_loss(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        retrain_optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        retrain_optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss_value.item())\n",
        "\n",
        "    train_loss = np.mean(batch_losses)\n",
        "    retrain_losses.append(train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    retrain_model.eval()\n",
        "    val_batch_losses = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = retrain_model(images)\n",
        "            loss_value = ce_loss(logits, labels)\n",
        "            val_batch_losses.append(loss_value.item())\n",
        "\n",
        "    val_loss = np.mean(val_batch_losses)\n",
        "    retrain_val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save best model during retraining\n",
        "    if val_loss < best_retrain_val_loss:\n",
        "        best_retrain_val_loss = val_loss\n",
        "        torch.save(retrain_model.state_dict(), retrain_model_save_path)\n",
        "        print(\"✓ Saved improved retrained model\")\n",
        "\n",
        "print(f\"\\nRetraining complete! Best model saved to {retrain_model_save_path}\")\n",
        "\n",
        "## Plot Training Progress\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot retraining losses\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(retrain_losses, label='Retrain - Training Loss', marker='o')\n",
        "plt.plot(retrain_val_losses, label='Retrain - Validation Loss', marker='s')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Retraining Progress ({retrain_epochs} Epochs)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot comparison with original training (if we have the data)\n",
        "plt.subplot(1, 2, 2)\n",
        "if 'train_losses' in locals() and 'val_losses' in locals():\n",
        "    epochs_orig = range(1, len(train_losses) + 1)\n",
        "    epochs_retrain = range(len(train_losses) + 1, len(train_losses) + 1 + len(retrain_losses))\n",
        "\n",
        "    plt.plot(epochs_orig, train_losses, label='Original Train', alpha=0.7)\n",
        "    plt.plot(epochs_orig, val_losses, label='Original Val', alpha=0.7)\n",
        "    plt.plot(epochs_retrain, retrain_losses, label='Retrain Train', marker='o', linewidth=2)\n",
        "    plt.plot(epochs_retrain, retrain_val_losses, label='Retrain Val', marker='s', linewidth=2)\n",
        "\n",
        "    plt.axvline(x=len(train_losses), color='red', linestyle='--', alpha=0.5, label='Retrain Start')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Complete Training History')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## Apply Retrained Model to Test Set\n",
        "\n",
        "print(\"\\nApplying retrained model to test set...\")\n",
        "\n",
        "# Load the best retrained model\n",
        "final_model = ModernLeNet5()\n",
        "final_model.load_state_dict(torch.load(retrain_model_save_path, map_location=device))\n",
        "final_model = final_model.to(device)\n",
        "final_model.eval()\n",
        "\n",
        "# Apply retrained model to test set\n",
        "retrained_predictions = []\n",
        "retrained_logits = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for images, _ in tqdm(test_loader, desc=\"Evaluating retrained model\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        logits = final_model(images)\n",
        "        retrained_logits.append(logits.cpu().numpy())\n",
        "\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        retrained_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Concatenate all results\n",
        "retrained_logits = np.concatenate(retrained_logits, axis=0)\n",
        "\n",
        "print(f\"Retrained model evaluation complete.\")\n",
        "print(f\"Predictions shape: {len(retrained_predictions)}\")\n",
        "print(f\"Logits shape: {retrained_logits.shape}\")\n",
        "\n",
        "## Store Retrained Model Predictions in FiftyOne\n",
        "\n",
        "print(\"Storing retrained model predictions in FiftyOne...\")\n",
        "\n",
        "for i, sample in enumerate(tqdm(test_dataset, desc=\"Storing retrained predictions\")):\n",
        "    predicted_idx = retrained_predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx]\n",
        "    sample_logits = retrained_logits[i]\n",
        "\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()\n",
        "    )\n",
        "\n",
        "    sample[\"retrained_lenet_classification\"] = classification\n",
        "    sample.save()\n",
        "\n",
        "print(\"Retrained model predictions stored successfully!\")\n",
        "\n",
        "## Evaluate Retrained Model Performance\n",
        "\n",
        "print(\"\\nEvaluating retrained model performance...\")\n",
        "\n",
        "# Evaluate retrained model\n",
        "retrained_evaluation_results = test_dataset.evaluate_classifications(\n",
        "    \"retrained_lenet_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"retrained_lenet_eval\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RETRAINED MODEL EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "retrained_evaluation_results.print_report()\n",
        "\n",
        "## Compare Original vs Retrained Performance\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get metrics for both models\n",
        "original_metrics = lenet_evaluation_results.metrics()\n",
        "retrained_metrics = retrained_evaluation_results.metrics()\n",
        "\n",
        "print(f\"{'Metric':<20} {'Original':<12} {'Retrained':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 56)\n",
        "\n",
        "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "for metric in metrics_to_compare:\n",
        "    if metric in original_metrics and metric in retrained_metrics:\n",
        "        orig_val = original_metrics[metric]\n",
        "        retrain_val = retrained_metrics[metric]\n",
        "        improvement = retrain_val - orig_val\n",
        "\n",
        "        print(f\"{metric:<20} {orig_val:<12.4f} {retrain_val:<12.4f} {improvement:+.4f}\")\n",
        "\n",
        "## Analysis of Misclassified Samples After Retraining\n",
        "\n",
        "# Find samples that were misclassified before but correct now\n",
        "originally_wrong = test_dataset.match(\n",
        "    F(\"lenet_classification.label\") != F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "now_correct = originally_wrong.match(\n",
        "    F(\"retrained_lenet_classification.label\") == F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "print(f\"\\nSamples fixed by retraining: {len(now_correct)}\")\n",
        "\n",
        "# Find samples that were correct before but wrong now\n",
        "originally_correct = test_dataset.match(\n",
        "    F(\"lenet_classification.label\") == F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "now_wrong = originally_correct.match(\n",
        "    F(\"retrained_lenet_classification.label\") != F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "print(f\"Samples broken by retraining: {len(now_wrong)}\")\n",
        "\n",
        "# Net improvement\n",
        "net_improvement = len(now_correct) - len(now_wrong)\n",
        "print(f\"Net improvement in correct predictions: {net_improvement}\")\n",
        "\n",
        "## Launch FiftyOne App with Results\n",
        "\n",
        "print(\"\\nLaunching FiftyOne App to explore results...\")\n",
        "\n",
        "# Create a view showing the comparison\n",
        "comparison_view = test_dataset.select_fields([\n",
        "    \"ground_truth\",\n",
        "    \"lenet_classification\",\n",
        "    \"retrained_lenet_classification\"\n",
        "])\n",
        "\n",
        "session = fo.launch_app(comparison_view, auto=False)\n",
        "print(f\"FiftyOne App URL: {session.url}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AUGMENTATION AND RETRAINING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"✓ Identified {len(mislabeled_train_images_view)} misclassified training samples\")\n",
        "print(f\"✓ Applied MNIST-appropriate augmentations (rotation, translation, elastic deform)\")\n",
        "print(f\"✓ Created {len(augmented_dataset)} augmented training samples\")\n",
        "print(f\"✓ Retrained model for {retrain_epochs} epochs with combined dataset\")\n",
        "print(f\"✓ Evaluated performance on test set\")\n",
        "print(f\"✓ Net improvement: {net_improvement} correctly classified samples\")\n"
      ],
      "metadata": {
        "id": "h2MiRH66n-7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Insights\n",
        "Data augmentation helps with misclassified samples: By specifically targeting the samples that our model struggled with during initial training, we can create additional training examples that help the model learn to handle these edge cases more effectively.\n",
        "\n",
        "Small rotations and elastic deformations are effective for handwritten digits: These transformations mimic natural variations in human handwriting without making digits ambiguous. The key is finding the right balance - enough variation to improve robustness, but not so much that a \"6\" looks like a \"9\".\n",
        "\n",
        "Fine-tuning with augmented data can improve model robustness: Rather than training from scratch, fine-tuning the already-trained model with augmented versions of problematic samples allows us to specifically address weaknesses while preserving the knowledge already learned.\n",
        "\n",
        "FiftyOne enables easy tracking of prediction changes across model versions: By storing predictions from both the original and retrained models in the same dataset, we can directly compare performance and identify which specific samples improved or degraded, enabling targeted analysis and further improvements.\n",
        "\n"
      ],
      "metadata": {
        "id": "XoMJ0qu3peE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeaways\n",
        "\n",
        "This tutorial demonstrates several fundamental principles:\n",
        "\n",
        "- **Zero-shot vs. Supervised Learning**: Modern pre-trained models can often achieve competitive performance without task-specific training, but custom models allow for domain-specific optimization\n",
        "- **Embeddings as Universal Representations**: High-dimensional vectors capture semantic similarity and enable powerful analysis and visualization techniques  \n",
        "- **Visual Debugging**: FiftyOne's interactive capabilities make it easy to understand model behavior and identify improvement opportunities\n",
        "- **Data Quality Matters**: Systematic analysis of your dataset often leads to more significant performance gains than model architecture changes\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "chH1kiI3N8W7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification Getting Started Series Summary\n",
        "\n",
        "This comprehensive series walks you through the core components of working with\n",
        "classification data in FiftyOne: from loading and visualizing datasets, to creating embeddings, evaluating models, and finding systematic errors.\n",
        "\n",
        "## Summary of Steps\n",
        "\n",
        "### Step 1: Understanding the MNIST Dataset\n",
        "\n",
        "Explore the MNIST dataset structure and load it into FiftyOne. Learn about the 60,000 training images and 10,000 test images of handwritten digits, and understand why this dataset remains fundamental for classification research.\n",
        "\n",
        "### Step 2: Creating and Visualizing Image Embeddings\n",
        "\n",
        "Learn how neural networks represent images as high-dimensional vectors. Generate embeddings with CLIP and visualize them using dimensionality reduction techniques like PCA and UMAP to understand image similarity and clustering.\n",
        "\n",
        "### Step 3: Zero-shot Classification with CLIP\n",
        "\n",
        "Discover how modern vision-language models can classify images without explicit training on your dataset. Use pre-computed embeddings to perform efficient zero-shot classification with meaningful text prompts.\n",
        "\n",
        "### Step 4: Evaluating Dataset Quality through Embeddings\n",
        "\n",
        "Assess your dataset's composition by analyzing embedding distributions. Learn to identify representative samples, outliers, and potential data quality issues using FiftyOne's analysis capabilities.\n",
        "\n",
        "### Step 5: Traditional Supervised Classification\n",
        "\n",
        "Build a custom Convolutional Neural Network (LeNet-5) from scratch in PyTorch. Understand the fundamentals of supervised learning including convolutional layers, training loops, and optimization.\n",
        "\n",
        "### Step 6: Bridging FiftyOne and PyTorch\n",
        "\n",
        "Master the integration between FiftyOne's dataset management and PyTorch's training capabilities. Convert datasets to DataLoaders while maintaining metadata and handling preprocessing efficiently.\n",
        "\n",
        "### Step 7: Model Comparison and Benchmarking\n",
        "\n",
        "Compare your custom CNN against CLIP's zero-shot classification using FiftyOne's evaluation framework. Learn comprehensive evaluation metrics and statistical significance testing.\n",
        "\n",
        "### Step 8: Analyzing Model Predictions\n",
        "\n",
        "Interpret model behavior by examining prediction confidence and identifying hard samples. Use logit analysis to find systematic misclassification patterns and debug model failures.\n",
        "\n",
        "### Step 9: Data Augmentation Strategies\n",
        "\n",
        "Improve model performance through principled data augmentation. Learn which geometric transformations and elastic deformations help MNIST classification while avoiding destructive augmentations.\n",
        "\n",
        "### Step 10: Advanced Error Analysis\n",
        "\n",
        "Create targeted views of model failures using FiftyOne's visualization capabilities. Identify false positives, false negatives, and the most problematic samples for systematic model improvement.\n",
        "\n",
        "---\n",
        "\n",
        "This series is part of the **Getting Started with FiftyOne** initiative. For\n",
        "more tutorials, head to [FiftyOne Documentation](https://docs.voxel51.com/).\n",
        "\n"
      ],
      "metadata": {
        "id": "MOoKTaHllqTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suggested Exercises\n",
        "\n",
        "1. **Sample Quality Analysis**: Notice that we computed the uniqueness, representativeness, and hardness of the samples on the training set. Can you create filtered views of other batches of retraining data based on them? Try selecting the most unique samples or those with highest hardness scores for additional augmentation.\n",
        "\n",
        "2. **Active Learning**: Use the uniqueness and hardness metrics to implement an active learning pipeline that selects the most informative samples for manual annotation or additional augmentation.\n",
        "\n",
        "3. **Dataset Exploration**: Apply these techniques to other classification datasets like [CIFAR-10](https://docs.voxel51.com/dataset_zoo/datasets.html#dataset-zoo-cifar10) or [Fashion-MNIST](https://docs.voxel51.com/dataset_zoo/datasets.html#dataset-zoo-fashion-mnist)\n",
        "\n",
        "4. **Architecture Comparison**: Implement and compare different CNN architectures (e.g. [Network in Network](https://arxiv.org/pdf/1312.4400), [ResNet](https://arxiv.org/pdf/1512.03385)) on MNIST\n",
        "\n",
        "5. **Transfer Learning**: Use pre-trained ImageNet models and fine-tune them for digit classification\n",
        "\n",
        "6. **Custom Augmentations**: Design and test novel augmentation strategies specific to handwritten digits, try augmenting the dataset [adding colors to the digits](https://paperswithcode.com/dataset/colored-mnist) and see how this change impacts the model's performance.\n",
        "\n",
        "7. **CLIP Model Variants and Prompting**: Experiment with different CLIP model variants available in FiftyOne's Model Zoo and compare their zero-shot performance on MNIST. Test how different text prompts affect accuracy across model sizes."
      ],
      "metadata": {
        "id": "YM6Hf32Vqyix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Resources and Further Reading\n",
        "\n",
        "- [FiftyOne Documentation](https://docs.voxel51.com/)\n",
        "- [FiftyOne's Filtering Cheatsheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n",
        "- [FiftyOne Model Zoo](https://docs.voxel51.com/user_guide/model_zoo/index.html)\n",
        "- [FiftyOne Dataset Zoo](https://docs.voxel51.com/user_guide/dataset_zoo/index.html)\n",
        "- [PyTorch Classification Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
        "- [CLIP Paper: Learning Transferable Visual Representations](https://arxiv.org/abs/2103.00020)"
      ],
      "metadata": {
        "id": "ZTgM1-9DatJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you've completed the Image Classification Getting Started series, here are\n",
        "some suggested next steps to deepen your journey with FiftyOne:\n",
        "\n",
        "-   **Explore Object Detection**  \n",
        "    Learn how to work with bounding boxes, evaluate detection models, and find annotation mistakes in object detection datasets.\n",
        "\n",
        "-   **Try Multi-label Classification**  \n",
        "    Extend these concepts to scenarios where images can belong to multiple classes simultaneously, common in real-world applications.\n",
        "\n",
        "-   **Experiment with FiftyOne Plugins**  \n",
        "    Enhance your workflow with powerful plugins for advanced augmentations, active learning tools, and integrations with annotation platforms.\n",
        "\n",
        "-   **Connect with the Community**  \n",
        "    Share your findings, ask questions, or browse community projects on the\n",
        "    [FiftyOne Discord](https://community.voxel51.com) or\n",
        "    [GitHub Discussions](https://github.com/voxel51/fiftyone/discussions).\n",
        "\n",
        "-   **Apply to Your Own Datasets**  \n",
        "    Adapt these workflows to your real-world classification projects. Whether\n",
        "    it's medical imaging, satellite analysis, or manufacturing quality control — FiftyOne supports diverse domains.\n",
        "\n",
        "-   **Dive into Advanced Topics**  \n",
        "    Explore segmentation, video analysis, and 3D data in the\n",
        "    [official documentation](https://docs.voxel51.com/).\n",
        "\n"
      ],
      "metadata": {
        "id": "c26WH1uGr-Wh"
      }
    }
  ]
}