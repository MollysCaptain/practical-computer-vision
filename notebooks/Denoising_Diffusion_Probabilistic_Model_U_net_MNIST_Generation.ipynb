{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone==1.5.2 > /dev/null"
      ],
      "metadata": {
        "id": "4kFm4d6mN1YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Denoising Diffusion Probabilistic Models (MNIST digit generation)"
      ],
      "metadata": {
        "id": "Feagam036AYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/images/generated_mnist_digits_FO.png)"
      ],
      "metadata": {
        "id": "7izynWdRGH1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* U-Net: For the noise prediction network in the Denoising Diffusion Probabilisit Model (DDPM)\n",
        "* Diffusion Process: Defines how noise is added (forward) and removed (reverse).\n",
        "* MNIST Classifier: A standard convolutional neural network for classifying digits. It doesn't share gradients or information with the U-net, we use it just to evaluate the quality of predictions.\n",
        "* Training Loops: One for the DDPM, one for the classifier.\n",
        "* Sampling & Evaluation: Generating digits from the DDPM and classifying them."
      ],
      "metadata": {
        "id": "r5GqxAMl7ndl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Benefits of U-Net's Downsampling-Upsampling Architecture in Diffusion Models\n",
        "\n",
        "## Multi-scale Feature Processing\n",
        "- U-Net captures hierarchical image features through progressive downsampling\n",
        "- Lower resolutions encode global context and structure\n",
        "- Higher resolutions preserve fine details and textures\n",
        "- This multi-scale approach matches diffusion's noise characteristics at different timesteps\n",
        "\n",
        "## Efficient Long-range Dependencies\n",
        "- Downsampling increases effective receptive field without computational explosion\n",
        "- Later timesteps (more noise) require global context to reconstruct structure\n",
        "- Earlier timesteps (less noise) need local precision for detail refinement\n",
        "- U-Net addresses both through its pyramidal architecture\n",
        "\n",
        "## Skip Connections Preserve Information\n",
        "- Skip connections between corresponding encoding/decoding layers maintain critical information\n",
        "- These connections combat vanishing gradients in deep networks\n",
        "- They create residual pathways that help preserve spatial details lost during downsampling\n",
        "- Particularly valuable in diffusion models where preserving underlying image structure is essential\n",
        "\n",
        "## Parameter Efficiency\n",
        "- Downsampling reduces spatial dimensions, allowing deeper networks with manageable parameters\n",
        "- Deeper networks capture more complex noise patterns across diffusion timesteps\n",
        "- Upsampling gradually reconstructs spatial resolution while maintaining semantic understanding\n",
        "- This efficiency enables handling the complex mapping from noisy to clean images\n",
        "\n",
        "## Resolution-dependent Processing\n",
        "- Different stages in diffusion process benefit from different resolution processing:\n",
        "  - Early denoising (high noise): Benefits from low-resolution processing focused on structural elements\n",
        "  - Late denoising (low noise): Requires high-resolution processing for detail refinement\n",
        "- U-Net's architecture naturally aligns with this progression\n",
        "\n",
        "## Consistent with Diffusion Physics\n",
        "- Noise diffusion follows a coarse-to-fine pattern in physical systems\n",
        "- U-Net's architecture mimics this natural process:\n",
        "  - Downsampling path: Identifies global structures in highly noisy images\n",
        "  - Upsampling path: Progressively refines details as noise levels decrease\n",
        "- This alignment improves model convergence and generation quality\n",
        "\n",
        "## Attention Integration\n",
        "- Modern diffusion U-Nets incorporate self-attention at lower resolutions\n",
        "- This combines convolutional inductive bias with transformer-like global reasoning\n",
        "- Particularly valuable for coherent structure generation in highly degraded images\n",
        "\n",
        "The U-Net architecture  balances global context and local detail processing, making it suited for the progressive denoising task of diffusion models across their full timestep spectrum."
      ],
      "metadata": {
        "id": "VtdeQlc7KPO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTXppAKy0c-b",
        "outputId": "d0c49111-58be-4088-cb6e-1e4e4c3275fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import fiftyone as fo\n",
        "\n",
        "# --- Configuration ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "IMG_SIZE = 28 # MNIST image size\n",
        "BATCH_SIZE_DDPM = 128\n",
        "BATCH_SIZE_CLASSIFIER = 64\n",
        "# Learning rates\n",
        "LR_DDPM = 1e-3\n",
        "LR_CLASSIFIER = 1e-3\n",
        "EPOCHS_DDPM = 50 # Increase for better results (e.g., 100-200)\n",
        "EPOCHS_CLASSIFIER = 10 # Usually converges faster\n",
        "T = 300 # Number of diffusion timesteps (can be 200-1000)\n",
        "SAVE_INTERVAL_DDPM = 10 # Save generated images every N epochs\n",
        "OUTPUT_DIR = \"ddpm_mnist_output\"\n",
        "CLASSIFIER_MODEL_PATH = os.path.join(OUTPUT_DIR, \"mnist_classifier.pth\")\n",
        "DDPM_MODEL_PATH = os.path.join(OUTPUT_DIR, \"ddpm_unet.pth\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why We Use Positional Embeddings for Timesteps\n",
        "\n",
        "Sinusoidal positional embeddings for timesteps `t` in Denoising Diffusion Probabilistic Models serve several critical functions:\n",
        "\n",
        "## Purpose\n",
        "- The U-Net predicts noise `Îµ` added to image `x_0` to produce `x_t`\n",
        "- Noise characteristics depend on timestep `t` - early timesteps have minimal noise while late timesteps contain mostly noise\n",
        "- The U-Net requires timestep information to predict the appropriate noise to remove\n",
        "\n",
        "## Problems with Raw Integer Timesteps\n",
        "- **Scale Issues:** Raw integers (0-1000) have disparate magnitudes that complicate neural network learning\n",
        "- **Interpretation Gaps:** Networks may treat each integer as distinct, missing the relationship between adjacent timesteps\n",
        "- **Limited Range:** Raw integers provide no framework for understanding timesteps outside training range\n",
        "- **Insufficient Information:** A single scalar value lacks expressiveness for timestep representation\n",
        "\n",
        "## Benefits of Sinusoidal Positional Embeddings\n",
        "- **Unique Vectors:** Each timestep receives a distinct vector representation\n",
        "- **Fixed Calculation:** Embeddings derive from sine/cosine functions rather than learning, providing consistency\n",
        "- **Continuity:** Smooth functions create proximity between adjacent timesteps in embedding space, enabling interpolation\n",
        "- **Structured Relationships:** For offset `k`, embedding of `t+k` can be expressed as linear function of embedding `t`\n",
        "- **Frequency Spectrum:** Multiple frequencies capture both coarse and fine-grained timestep relationships:\n",
        "  ```\n",
        "  PE(t, 2i)   = sin(t / 10000^(2i / d_model))\n",
        "  PE(t, 2i+1) = cos(t / 10000^(2i / d_model))\n",
        "  ```\n",
        "  - Low frequencies (large wavelengths) capture broad noise patterns\n",
        "  - High frequencies (small wavelengths) differentiate between adjacent timesteps\n",
        "- **Value Constraints:** Sine/cosine outputs bound between -1 and 1, stabilizing network training\n",
        "- **Dimensional Richness:** Vector representation (256-512 dimensions) provides depth beyond scalar values\n",
        "\n",
        "## Implementation in U-Net Architecture\n",
        "- Integer timestep `t` converts to sinusoidal positional embedding vector\n",
        "- This vector passes through a multi-layer perceptron for transformation\n",
        "- The resulting embedding integrates with U-Net feature maps via addition or concatenation, conditioning the network on the noise level"
      ],
      "metadata": {
        "id": "A9JcGDqTG43o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper: Sinusoidal Positional Embeddings for Timesteps ---\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "O5BuZaWx8DXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- U-Net Architecture ---\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
        "        if up:\n",
        "            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1) # Downsample\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # First Conv\n",
        "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        # Time embedding\n",
        "        time_emb = self.relu(self.time_mlp(t))\n",
        "        # Extend last 2 dimensions\n",
        "        time_emb = time_emb[(...,) + (None,) * 2]\n",
        "        # Add time channel\n",
        "        h = h + time_emb\n",
        "        # Second Conv\n",
        "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
        "        # Downsample or Upsample\n",
        "        return self.transform(h)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, img_channels=1, time_emb_dim=256):\n",
        "        super().__init__()\n",
        "        self.time_emb_dim = time_emb_dim\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Initial projection\n",
        "        self.initial_conv = nn.Conv2d(img_channels, 64, 3, padding=1)\n",
        "\n",
        "        # Downsampling\n",
        "        self.down1 = Block(64, 128, time_emb_dim) # 28x28 -> 14x14\n",
        "        self.down2 = Block(128, 256, time_emb_dim) # 14x14 -> 7x7\n",
        "\n",
        "        # Bottleneck (no down/upsample)\n",
        "        self.sa1 = Block(256, 256, time_emb_dim) # 7x7 -> 3x3 (due to kernel 4, stride 2, padding 1 in transform)\n",
        "                                                    # Let's adjust transform in Block for bottleneck\n",
        "                                                    # Or, use a simpler conv block for bottleneck\n",
        "        self.bot1 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.bot2 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.bot3 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "\n",
        "\n",
        "        # Upsampling\n",
        "        self.up1 = Block(256 + 256, 128, time_emb_dim, up=True) # 3x3 -> 7x7 (adjust for skip)\n",
        "        self.up2 = Block(128 + 128, 64, time_emb_dim, up=True) # 7x7 -> 14x14\n",
        "\n",
        "        # Output\n",
        "        self.out_conv = nn.Conv2d(64 + 64, img_channels, kernel_size=1) # Match original image size (28x28)\n",
        "                                                                     # This needs adjustment if upsampling reaches full size.\n",
        "                                                                     # Let's make sure upsampling goes to 28x28\n",
        "        # For U-Net, the upsampling should bring it back to the original size BEFORE the final output conv\n",
        "        # Let's redefine the upsampling path slightly to ensure correct sizes\n",
        "        # For MNIST 28x28:\n",
        "        # 28 -> 14 -> 7 (down)\n",
        "        # 7 (bottleneck)\n",
        "        # 7 -> 14 -> 28 (up)\n",
        "\n",
        "        # Re-defining U-Net for clarity on dimensions\n",
        "        # Input: (B, 1, 28, 28)\n",
        "        self.down1 = Block(64, 128, time_emb_dim)      # Output: (B, 128, 14, 14)\n",
        "        self.down2 = Block(128, 256, time_emb_dim)     # Output: (B, 256, 7, 7)\n",
        "\n",
        "        # Bottleneck (no spatial change here, just convs)\n",
        "        self.sa_in_ch = 256\n",
        "        self.sa_out_ch = 256\n",
        "        self.bot_conv1 = nn.Conv2d(self.sa_in_ch, self.sa_out_ch, 3, padding=1)\n",
        "        self.bot_relu = nn.ReLU()\n",
        "        self.bot_conv2 = nn.Conv2d(self.sa_out_ch, self.sa_out_ch, 3, padding=1)\n",
        "        # No transform in bottleneck, so `t` needs to be processed differently or not used in simple bottleneck\n",
        "        # Let's add time embedding to bottleneck too for consistency\n",
        "        self.bot_time_mlp = nn.Linear(time_emb_dim, self.sa_out_ch)\n",
        "\n",
        "        self.up0 = Block(256, 128, time_emb_dim, up=True)      # Input skip from down2: (B, 128, 14, 14)\n",
        "        self.up1 = Block(128, 64, time_emb_dim, up=True)       # Input skip from down1: (B, 64, 28, 28)\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.Conv2d(64, img_channels, kernel_size=1) # Should be (B, 1, 28, 28)\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        # Embedd time\n",
        "        t = self.time_mlp(timestep)\n",
        "\n",
        "        x = self.initial_conv(x) # (B, 64, 28, 28)\n",
        "\n",
        "        # Downsampling path\n",
        "        x1 = self.down1(x, t)    # (B, 128, 14, 14)\n",
        "        x2 = self.down2(x1, t)   # (B, 256, 7, 7)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bot_conv1(x2)\n",
        "        time_emb_bot = self.bot_relu(self.bot_time_mlp(t))\n",
        "        time_emb_bot = time_emb_bot[(...,) + (None,) * 2]\n",
        "        b = b + time_emb_bot\n",
        "        b = self.bot_conv2(self.bot_relu(b)) # (B, 256, 7, 7)\n",
        "\n",
        "        # Upsampling path\n",
        "        # To handle skip connections, the Block's 'up' conv1 needs to take 2*in_ch\n",
        "        # The input to up0 will be b and x2. Output of down2 is x2 (256 channels)\n",
        "        # The input to up1 will be output_of_up0 and x1. Output of down1 is x1 (128 channels)\n",
        "        # The input to self.output will be output_of_up1 and x. Initial_conv output (64 channels)\n",
        "\n",
        "        # For skip connection, the input to Block(up=True) is already prepared\n",
        "        # by concatenating. Let's adjust the Block up=True part.\n",
        "        # Block(in_ch, out_ch) means in_ch from previous layer, out_ch is the target for this block's output.\n",
        "        # So for up=True, conv1 should take (in_ch_skip + in_ch_previous_upsample)\n",
        "        # It's simpler if Block handles concatenation internally if up=True.\n",
        "        # Let's make Block take `in_ch_skip` as an additional argument for `up=True`\n",
        "        # Or, just define the concat explicitly here.\n",
        "\n",
        "        u0 = self.up0(torch.cat((b, x2), 1), t)      # Input: (B, 256+256, 7, 7) -> (B, 128, 14, 14)\n",
        "        u1 = self.up1(torch.cat((u0, x1), 1), t)     # Input: (B, 128+128, 14, 14) -> (B, 64, 28, 28)\n",
        "\n",
        "        # Final output\n",
        "        # Skip connection with initial conv output\n",
        "        # return self.output(torch.cat((u1, x), 1)) # This concat logic is handled by last Block implicitly\n",
        "        return self.output(u1) # The U-Net output structure usually has final conv on last upsample layer directly\n"
      ],
      "metadata": {
        "id": "KBqy3SQH8GCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Diffusion Process ---\n",
        "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
        "    return torch.linspace(start, end, timesteps)\n",
        "\n",
        "def get_index_from_list(vals, t, x_shape):\n",
        "    \"\"\" Returns a specific index t of a list of values vals for a batch of shape x_shape \"\"\"\n",
        "    batch_size = t.shape[0]\n",
        "    out = vals.gather(-1, t)\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "class Diffusion:\n",
        "    def __init__(self, timesteps=T, img_size=IMG_SIZE, device=DEVICE):\n",
        "        self.timesteps = timesteps\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "\n",
        "        self.betas = linear_beta_schedule(timesteps).to(device)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
        "\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        \"\"\" Forward process: q(x_t | x_0) \"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "\n",
        "        sqrt_alphas_cumprod_t = get_index_from_list(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "\n",
        "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, model, x, t, t_index):\n",
        "        \"\"\" Reverse process: p(x_{t-1} | x_t) \"\"\"\n",
        "        betas_t = get_index_from_list(self.betas, t, x.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
        "        sqrt_recip_alphas_t = get_index_from_list(self.sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "        # Equation 11 in DDPM paper\n",
        "        # Use our model (noise predictor) to predict the mean\n",
        "        model_mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t)\n",
        "\n",
        "        if t_index == 0:\n",
        "            return model_mean\n",
        "        else:\n",
        "            posterior_variance_t = get_index_from_list(self.posterior_variance, t, x.shape)\n",
        "            noise = torch.randn_like(x)\n",
        "            return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, image_size, batch_size=16, channels=1):\n",
        "        \"\"\" Sample batch_size images \"\"\"\n",
        "        shape = (batch_size, channels, image_size, image_size)\n",
        "        img = torch.randn(shape, device=self.device)\n",
        "        imgs = []\n",
        "\n",
        "        for i in tqdm(reversed(range(0, self.timesteps)), desc='Sampling loop time step', total=self.timesteps):\n",
        "            t = torch.full((batch_size,), i, device=self.device, dtype=torch.long)\n",
        "            img = self.p_sample(model, img, t, i)\n",
        "            # Optionally, clip to [-1, 1] at each step if desired, though often done at the end.\n",
        "            # img = torch.clamp(img, -1.0, 1.0)\n",
        "            if i % (self.timesteps // 10) == 0 or i < 10 : # Store some intermediate steps\n",
        "                 imgs.append(img.cpu())\n",
        "        imgs.append(img.cpu()) # final image\n",
        "        return imgs # Returns a list of tensors, last one is the final result\n",
        "\n",
        "    def p_losses(self, denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "\n",
        "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
        "        predicted_noise = denoise_model(x_noisy, t)\n",
        "\n",
        "        if loss_type == 'l1':\n",
        "            loss = F.l1_loss(noise, predicted_noise)\n",
        "        elif loss_type == 'l2':\n",
        "            loss = F.mse_loss(noise, predicted_noise)\n",
        "        elif loss_type == \"huber\":\n",
        "            loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "        return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "xM_bT2Br8lSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MNIST Classifier ---\n",
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 28x28 -> 14x14\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 14x14 -> 7x7\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10) # 10 classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        # DDPM output is [-1, 1]. Classifier usually trained on [0,1] or normalized differently.\n",
        "        # Let's assume classifier expects [0,1] then normalizes internally, or we normalize before passing.\n",
        "        # For simplicity, let's make classifier expect [-1, 1] directly for generated images.\n",
        "        # If training classifier on original MNIST, it will see [0,1] normalized to [-1,1] by transform.\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MFSfeE8ZDRAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Loading ---\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "transform_ddpm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),                # Scales to [0, 1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Scales to [-1, 1]\n",
        "])\n",
        "\n",
        "transform_classifier = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Standard MNIST normalization\n",
        "])\n",
        "\n",
        "# For DDPM training\n",
        "train_dataset_ddpm = datasets.MNIST('.', train=True, download=True, transform=transform_ddpm)\n",
        "train_loader_ddpm = DataLoader(train_dataset_ddpm, batch_size=BATCH_SIZE_DDPM, shuffle=True, drop_last=True)\n",
        "\n",
        "# For Classifier training and evaluation\n",
        "train_dataset_classifier = datasets.MNIST('.', train=True, download=True, transform=transform_classifier)\n",
        "test_dataset_classifier = datasets.MNIST('.', train=False, download=True, transform=transform_classifier)\n",
        "\n",
        "train_loader_classifier = DataLoader(train_dataset_classifier, batch_size=BATCH_SIZE_CLASSIFIER, shuffle=True)\n",
        "test_loader_classifier = DataLoader(test_dataset_classifier, batch_size=BATCH_SIZE_CLASSIFIER, shuffle=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JY-0w-CxDUqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Functions ---\n",
        "def train_ddpm(model, diffusion_process, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    print(\"Starting DDPM Training...\")\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for step, (images, _) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(DEVICE)\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            # Sample t uniformally for every example in the batch\n",
        "            t = torch.randint(0, diffusion_process.timesteps, (batch_size,), device=DEVICE).long()\n",
        "\n",
        "            loss = diffusion_process.p_losses(model, images, t, loss_type=\"huber\") # huber is often good\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, DDPM Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % SAVE_INTERVAL_DDPM == 0 or epoch == epochs - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Sample a few images\n",
        "                sampled_images_steps = diffusion_process.sample(model, IMG_SIZE, batch_size=16, channels=1)\n",
        "                final_sampled_images = sampled_images_steps[-1]\n",
        "                # Denormalize from [-1, 1] to [0, 1] for saving\n",
        "                final_sampled_images = (final_sampled_images + 1) / 2.0\n",
        "                grid = make_grid(final_sampled_images, nrow=4)\n",
        "                save_image(grid, os.path.join(OUTPUT_DIR, f\"ddpm_sample_epoch_{epoch+1}.png\"))\n",
        "                print(f\"Saved sample images at epoch {epoch+1}\")\n",
        "            model.train() # Set back to train mode\n",
        "\n",
        "    torch.save(model.state_dict(), DDPM_MODEL_PATH)\n",
        "    print(f\"DDPM model saved to {DDPM_MODEL_PATH}\")\n",
        "\n",
        "\n",
        "def train_classifier(model, train_loader, test_loader, optimizer, criterion, epochs):\n",
        "    model.train()\n",
        "    print(\"Starting Classifier Training...\")\n",
        "    if os.path.exists(CLASSIFIER_MODEL_PATH):\n",
        "        print(f\"Loading pre-trained classifier from {CLASSIFIER_MODEL_PATH}\")\n",
        "        model.load_state_dict(torch.load(CLASSIFIER_MODEL_PATH, map_location=DEVICE))\n",
        "        # Optionally, evaluate before further training\n",
        "        test_classifier(model, test_loader, criterion, \"Loaded Classifier\")\n",
        "        # return # Uncomment if you just want to load and not retrain\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Classifier Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Classifier Epoch {epoch+1}/{epochs}, Loss: {avg_epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "        test_classifier(model, test_loader, criterion, f\"Classifier Test Epoch {epoch+1}\")\n",
        "\n",
        "    torch.save(model.state_dict(), CLASSIFIER_MODEL_PATH)\n",
        "    print(f\"Classifier model saved to {CLASSIFIER_MODEL_PATH}\")\n",
        "\n",
        "\n",
        "def test_classifier(model, test_loader, criterion, name=\"Test\"):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"{name} Results: Avg Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    model.train() # Set back to train mode if called during training\n",
        "    return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "mYXre4RZ8Jo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Initialize and Train Classifier\n",
        "print(\"\\n--- MNIST Classifier ---\")\n",
        "mnist_classifier = MNISTClassifier().to(DEVICE)\n",
        "optimizer_classifier = optim.Adam(mnist_classifier.parameters(), lr=LR_CLASSIFIER)\n",
        "criterion_classifier = nn.CrossEntropyLoss()\n",
        "\n",
        "# Check if classifier is already trained, if so, load it. Otherwise, train.\n",
        "if os.path.exists(CLASSIFIER_MODEL_PATH) and EPOCHS_CLASSIFIER == 0 : # Only load if not training\n",
        "    print(f\"Loading pre-trained classifier from {CLASSIFIER_MODEL_PATH}\")\n",
        "    mnist_classifier.load_state_dict(torch.load(CLASSIFIER_MODEL_PATH, map_location=DEVICE))\n",
        "    test_classifier(mnist_classifier, test_loader_classifier, criterion_classifier, \"Loaded Pre-trained Classifier\")\n",
        "else:\n",
        "    if EPOCHS_CLASSIFIER > 0:\n",
        "          train_classifier(mnist_classifier, train_loader_classifier, test_loader_classifier,\n",
        "                          optimizer_classifier, criterion_classifier, EPOCHS_CLASSIFIER)\n",
        "    else: # if epochs is 0 and no file, it's an error or needs training\n",
        "        print(\"Classifier model not found and EPOCHS_CLASSIFIER is 0. Please train or provide a model.\")\n",
        "        # For this script, we'll proceed assuming we might train it or it might not be strictly needed for DDPM part only.\n",
        "        # However, for evaluating DDPM output, a trained classifier is essential.\n",
        "        # Let's try to train it if not found.\n",
        "        if not os.path.exists(CLASSIFIER_MODEL_PATH):\n",
        "            print(\"Training classifier as it was not found and EPOCHS_CLASSIFIER was 0 (forcing 10 epochs).\")\n",
        "            train_classifier(mnist_classifier, train_loader_classifier, test_loader_classifier,\n",
        "                              optimizer_classifier, criterion_classifier, 10) # Force some training\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZdrdKfSj1-ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Initialize and Train DDPM\n",
        "print(\"\\n--- DDPM U-Net ---\")\n",
        "unet_model = UNet(img_channels=1, time_emb_dim=256).to(DEVICE) # time_emb_dim can be tuned\n",
        "diffusion = Diffusion(timesteps=T, img_size=IMG_SIZE, device=DEVICE)\n",
        "optimizer_ddpm = optim.Adam(unet_model.parameters(), lr=LR_DDPM)\n",
        "\n",
        "if os.path.exists(DDPM_MODEL_PATH) and EPOCHS_DDPM == 0:\n",
        "    print(f\"Loading pre-trained DDPM U-Net from {DDPM_MODEL_PATH}\")\n",
        "    unet_model.load_state_dict(torch.load(DDPM_MODEL_PATH, map_location=DEVICE))\n",
        "else:\n",
        "    if EPOCHS_DDPM > 0:\n",
        "        train_ddpm(unet_model, diffusion, train_loader_ddpm, optimizer_ddpm, EPOCHS_DDPM)\n",
        "    else:\n",
        "        print(\"DDPM model not found and EPOCHS_DDPM is 0. Please train or provide a model.\")\n",
        "        # For this script, let's assume it needs to be trained if not found\n",
        "        if not os.path.exists(DDPM_MODEL_PATH):\n",
        "            print(\"Training DDPM as it was not found and EPOCHS_DDPM was 0 (forcing 50 epochs).\")\n",
        "            train_ddpm(unet_model, diffusion, train_loader_ddpm, optimizer_ddpm, 50)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "egb6S2BS5VB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Generate Digits with DDPM and Evaluate with Classifier\n",
        "print(\"\\n--- Generating and Evaluating DDPM Samples ---\")\n",
        "if not os.path.exists(DDPM_MODEL_PATH):\n",
        "    print(\"DDPM model not trained or loaded. Cannot generate samples.\")\n",
        "elif not os.path.exists(CLASSIFIER_MODEL_PATH):\n",
        "    print(\"Classifier model not trained or loaded. Cannot evaluate generated samples.\")\n",
        "else:\n",
        "    unet_model.eval()\n",
        "    mnist_classifier.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        num_generated_samples = 64 # How many samples to generate for evaluation\n",
        "        generated_image_steps = diffusion.sample(unet_model, IMG_SIZE, batch_size=num_generated_samples, channels=1)\n",
        "        generated_images_final = generated_image_steps[-1].to(DEVICE) # Final images are on CPU, move to DEVICE\n",
        "\n",
        "        # Generated images are in [-1, 1] range (same as DDPM training data)\n",
        "        # Classifier was trained on MNIST data normalized with mean 0.1307, std 0.3081\n",
        "        # We need to transform generated images to match classifier's expected input.\n",
        "        # Current DDPM output: [-1, 1]\n",
        "        # Step 1: Denormalize DDPM output to [0, 1]\n",
        "        generated_images_0_1 = (generated_images_final + 1) / 2.0\n",
        "\n",
        "        # Step 2: Normalize for classifier input (mean 0.1307, std 0.3081)\n",
        "        # Note: If your classifier was trained on [-1,1] normalized data directly, this step would differ or be skipped.\n",
        "        # Our classifier transform: transforms.Normalize((0.1307,), (0.3081,))\n",
        "        # This implies input to normalize was [0,1]\n",
        "\n",
        "        # Create a Normalize transform instance\n",
        "        classifier_normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "        generated_images_for_classifier = classifier_normalize(generated_images_0_1)\n",
        "\n",
        "\n",
        "        # Save a grid of generated images (denormalized to [0,1])\n",
        "        grid = make_grid(generated_images_0_1.cpu(), nrow=8)\n",
        "        save_image(grid, os.path.join(OUTPUT_DIR, \"ddpm_final_generated_grid.png\"))\n",
        "        print(f\"Saved final generated grid to {os.path.join(OUTPUT_DIR, 'ddpm_final_generated_grid.png')}\")\n",
        "\n",
        "        # Get classifier predictions\n",
        "        outputs = mnist_classifier(generated_images_for_classifier)\n",
        "        _, predicted_classes = torch.max(outputs.data, 1)\n",
        "\n",
        "        print(f\"\\nClassifier predictions for {num_generated_samples} DDPM-generated digits:\")\n",
        "        print(predicted_classes.cpu().numpy().reshape(-1, 8)) # Print in a grid-like format\n",
        "\n",
        "        # Optional: Plot some generated images with their predicted classes\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "        for i, ax in enumerate(axes.flat):\n",
        "            if i < num_generated_samples and i < 16: # Plot first 16\n",
        "                img_to_plot = generated_images_0_1[i].cpu().squeeze().numpy() # Use [0,1] for plotting\n",
        "                ax.imshow(img_to_plot, cmap='gray')\n",
        "                ax.set_title(f\"Pred: {predicted_classes[i].item()}\")\n",
        "                ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, \"ddpm_generated_with_predictions.png\"))\n",
        "        print(f\"Saved plot of generated images with predictions to {os.path.join(OUTPUT_DIR, 'ddpm_generated_with_predictions.png')}\")\n",
        "        # plt.show() # Uncomment to display plot interactively\n",
        "\n",
        "        # You could also calculate things like Inception Score or FID if you had a more\n",
        "        # sophisticated classifier and more diverse generated samples, but for MNIST,\n",
        "        # visual inspection and raw class predictions are a good start.\n",
        "        # For a simple \"accuracy\", we don't have true labels for generated images.\n",
        "        # We can look at the distribution of predicted classes.\n",
        "        class_counts = torch.bincount(predicted_classes.cpu(), minlength=10)\n",
        "        print(\"\\nDistribution of predicted classes for generated samples:\")\n",
        "        for digit, count in enumerate(class_counts):\n",
        "            print(f\"Digit {digit}: {count.item()} samples\")\n",
        "\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "hMxxp6-p5acn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Generate Digits with DDPM and Evaluate with Classifier\n",
        "print(\"\\n--- Generating and Evaluating DDPM Samples ---\")\n",
        "if not os.path.exists(DDPM_MODEL_PATH):\n",
        "    print(\"DDPM model not trained or loaded. Cannot generate samples.\")\n",
        "elif not os.path.exists(CLASSIFIER_MODEL_PATH):\n",
        "    print(\"Classifier model not trained or loaded. Cannot evaluate generated samples.\")\n",
        "else:\n",
        "    unet_model.eval()\n",
        "    mnist_classifier.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        num_generated_samples = 1024 # How many samples to generate for evaluation\n",
        "        generated_image_steps = diffusion.sample(unet_model, IMG_SIZE, batch_size=num_generated_samples, channels=1)\n",
        "        generated_images_final = generated_image_steps[-1].to(DEVICE) # Final images are on CPU, move to DEVICE\n",
        "\n",
        "        # Generated images are in [-1, 1] range (same as DDPM training data)\n",
        "        # Classifier was trained on MNIST data normalized with mean 0.1307, std 0.3081\n",
        "        # We need to transform generated images to match classifier's expected input.\n",
        "        # Current DDPM output: [-1, 1]\n",
        "        # Step 1: Denormalize DDPM output to [0, 1]\n",
        "        generated_images_0_1 = (generated_images_final + 1) / 2.0\n",
        "\n",
        "        # Step 2: Normalize for classifier input (mean 0.1307, std 0.3081)\n",
        "        classifier_normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "        generated_images_for_classifier = classifier_normalize(generated_images_0_1.clone()) # Use .clone() if modifying\n",
        "\n",
        "\n",
        "        # Save a grid of generated images (denormalized to [0,1])\n",
        "        grid = make_grid(generated_images_0_1.cpu(), nrow=8)\n",
        "        save_image(grid, os.path.join(OUTPUT_DIR, \"ddpm_final_generated_grid.png\"))\n",
        "        print(f\"Saved final generated grid to {os.path.join(OUTPUT_DIR, 'ddpm_final_generated_grid.png')}\")\n",
        "\n",
        "        # Get classifier predictions (logits)\n",
        "        classifier_outputs_logits = mnist_classifier(generated_images_for_classifier)\n",
        "\n",
        "        # Get probabilities (softmax over logits)\n",
        "        classifier_outputs_probs = F.softmax(classifier_outputs_logits, dim=1)\n",
        "\n",
        "        # Get predicted classes (integers) and their probabilities\n",
        "        predicted_probs, predicted_classes_int = torch.max(classifier_outputs_probs.data, 1)\n",
        "\n",
        "\n",
        "        print(f\"\\nClassifier predictions for {num_generated_samples} DDPM-generated digits:\")\n",
        "        print(predicted_classes_int.cpu().numpy().reshape(-1, 8)) # Print in a grid-like format\n",
        "\n",
        "        # Optional: Plot some generated images with their predicted classes\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "        for i, ax in enumerate(axes.flat):\n",
        "            if i < num_generated_samples and i < 16: # Plot first 16\n",
        "                img_to_plot = generated_images_0_1[i].cpu().squeeze().numpy() # Use [0,1] for plotting\n",
        "                ax.imshow(img_to_plot, cmap='gray')\n",
        "                ax.set_title(f\"Pred: {predicted_classes_int[i].item()}\")\n",
        "                ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, \"ddpm_generated_with_predictions.png\"))\n",
        "        print(f\"Saved plot of generated images with predictions to {os.path.join(OUTPUT_DIR, 'ddpm_generated_with_predictions.png')}\")\n",
        "        # plt.show()\n",
        "\n",
        "        class_counts = torch.bincount(predicted_classes_int.cpu(), minlength=10)\n",
        "        print(\"\\nDistribution of predicted classes for generated samples:\")\n",
        "        for digit, count in enumerate(class_counts):\n",
        "            print(f\"Digit {digit}: {count.item()} samples\")\n",
        "\n",
        "        # --- FiftyOne Integration ---\n",
        "        print(\"\\n--- Creating FiftyOne Dataset ---\")\n",
        "\n",
        "        # Define a directory to store images for FiftyOne\n",
        "        fiftyone_image_dir = os.path.join(OUTPUT_DIR, \"fiftyone_generated_images\")\n",
        "        if os.path.exists(fiftyone_image_dir):\n",
        "            shutil.rmtree(fiftyone_image_dir) # Clean up old images if re-running\n",
        "        os.makedirs(fiftyone_image_dir, exist_ok=True)\n",
        "\n",
        "        dataset_name = \"ddpm_mnist_generated_samples\"\n",
        "        try:\n",
        "            dataset = fo.load_dataset(dataset_name)\n",
        "            dataset.delete() # Delete if exists to start fresh, or use dataset.clear()\n",
        "            print(f\"Deleted existing dataset: {dataset_name}\")\n",
        "        except ValueError:\n",
        "            print(f\"Dataset {dataset_name} not found, creating new one.\")\n",
        "            pass # Dataset doesn't exist, which is fine\n",
        "\n",
        "        dataset = fo.Dataset(name=dataset_name, persistent=True) # persistent=True saves dataset meta to DB\n",
        "\n",
        "        samples = []\n",
        "        for i in range(num_generated_samples):\n",
        "            img_tensor_0_1 = generated_images_0_1[i] # Already in [0,1] format\n",
        "\n",
        "            # Save image to file\n",
        "            img_filename = f\"generated_img_{i:04d}.png\"\n",
        "            img_filepath = os.path.join(fiftyone_image_dir, img_filename)\n",
        "            save_image(img_tensor_0_1.cpu(), img_filepath) # save_image expects CHW\n",
        "\n",
        "            # Get classifier outputs for this specific sample\n",
        "            sample_logits = classifier_outputs_logits[i].cpu().numpy().tolist()\n",
        "            sample_probs = classifier_outputs_probs[i].cpu().numpy().tolist()\n",
        "            sample_predicted_class_int = predicted_classes_int[i].item()\n",
        "            sample_predicted_class_label = str(sample_predicted_class_int) # For FiftyOne classification\n",
        "            sample_predicted_confidence = predicted_probs[i].item()\n",
        "\n",
        "            # Create FiftyOne Sample\n",
        "            sample = fo.Sample(filepath=img_filepath)\n",
        "\n",
        "            # Add classification prediction\n",
        "            sample[\"prediction\"] = fo.Classification(\n",
        "                label=sample_predicted_class_label,\n",
        "                confidence=sample_predicted_confidence\n",
        "            )\n",
        "\n",
        "            # Add raw logits and probabilities as custom fields\n",
        "            sample[\"probabilities\"] = sample_probs # Store as fo.Vector\n",
        "            sample[\"predicted_class_int\"] = sample_predicted_class_int # Store integer class too\n",
        "\n",
        "            samples.append(sample)\n",
        "\n",
        "        dataset.add_samples(samples)\n",
        "        print(f\"Added {len(samples)} samples to FiftyOne dataset '{dataset_name}'.\")\n",
        "        print(f\"Images for FiftyOne are stored in: {fiftyone_image_dir}\")\n",
        "\n",
        "        # You can now explore this dataset in the FiftyOne App\n",
        "        # For example, in a Jupyter notebook:\n",
        "        # session = fo.launch_app(dataset)\n",
        "        # print(\"FiftyOne App launched. If in Colab, you might need to use session.show().\")\n",
        "        # If running in a script, you might want to print the command to launch it later:\n",
        "        print(f\"\\nTo view the dataset in FiftyOne App, run:\\nfiftyone app launch {dataset_name}\")\n",
        "\n",
        "        # Example of how to access the data later:\n",
        "        # loaded_dataset = fo.load_dataset(dataset_name)\n",
        "        # for sample in loaded_dataset.limit(5):\n",
        "        #     print(f\"Image: {sample.filepath}\")\n",
        "        #     print(f\"  Predicted Label: {sample.prediction.label}\")\n",
        "        #     print(f\"  Confidence: {sample.prediction.confidence}\")\n",
        "        #     print(f\"  Probabilities (first 3): {sample.probabilities.values[:3]}\")\n",
        "\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UF_Tx8wfL0oe",
        "outputId": "acee368e-89d5-44aa-b1e2-2b86639015fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating and Evaluating DDPM Samples ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling loop time step: 100%|ââââââââââ| 300/300 [01:27<00:00,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved final generated grid to ddpm_mnist_output/ddpm_final_generated_grid.png\n",
            "\n",
            "Classifier predictions for 1024 DDPM-generated digits:\n",
            "[[2 3 3 ... 3 6 8]\n",
            " [5 3 9 ... 3 5 9]\n",
            " [0 0 2 ... 2 9 8]\n",
            " ...\n",
            " [5 8 3 ... 2 9 6]\n",
            " [6 1 4 ... 0 5 6]\n",
            " [2 8 7 ... 6 3 3]]\n",
            "Saved plot of generated images with predictions to ddpm_mnist_output/ddpm_generated_with_predictions.png\n",
            "\n",
            "Distribution of predicted classes for generated samples:\n",
            "Digit 0: 134 samples\n",
            "Digit 1: 38 samples\n",
            "Digit 2: 153 samples\n",
            "Digit 3: 125 samples\n",
            "Digit 4: 115 samples\n",
            "Digit 5: 104 samples\n",
            "Digit 6: 71 samples\n",
            "Digit 7: 86 samples\n",
            "Digit 8: 104 samples\n",
            "Digit 9: 94 samples\n",
            "\n",
            "--- Creating FiftyOne Dataset ---\n",
            "Deleted existing dataset: ddpm_mnist_generated_samples\n",
            " 100% |âââââââââââââââ| 1024/1024 [427.9ms elapsed, 0s remaining, 2.4K samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |âââââââââââââââ| 1024/1024 [427.9ms elapsed, 0s remaining, 2.4K samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 1024 samples to FiftyOne dataset 'ddpm_mnist_generated_samples'.\n",
            "Images for FiftyOne are stored in: ddpm_mnist_output/fiftyone_generated_images\n",
            "\n",
            "To view the dataset in FiftyOne App, run:\n",
            "fiftyone app launch ddpm_mnist_generated_samples\n",
            "\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 16 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8YAAAPdCAYAAABIgHGZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAgupJREFUeJzt3XnYnOPZP/Ar+06IJNYktqSlVYQQ1C5IQi2pViihrdSuthKtNbbSNl5bvERoBa0XrXoVSZvatbGXFhEJgkbyIiRElmd+f/SQNj+u637MZDLzPNfncxz+8Hxz3nPNPXPNPWfmyZwtSqVSKQAAAECmWtZ6AQAAAFBLGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxrjZqxPnz5hxIgRtV4GsBzZ19D82NfQ/NjXTY/GuEpuuOGG0KJFi6X/tW/fPvTt2zccc8wxYdasWbVeXqEXX3wxnHrqqWHTTTcNXbp0CWussUYYMmRIeOKJJ2q9NKiZpr6v33rrrXDwwQeHfv36hS5duoSuXbuGAQMGhBtvvDGUSqVaLw9qwr6G5qep7+v/34QJE0KLFi1C586da72UZq11rRfQ3J177rlh3XXXDQsWLAgPP/xwuPrqq8M999wTnn/++dCxY8daLy/quuuuC+PGjQv7779/OOqoo8LcuXPDNddcE7beeutw7733hl133bXWS4Saaar7es6cOWHmzJlh2LBhoVevXmHRokVh4sSJYcSIEeGll14KF1xwQa2XCDVjX0Pz01T39X+aN29eOPXUU0OnTp1qvZTmr0RVjB8/vhRCKE2ZMmWZn5944omlEELp5ptvjtbOmzdvuayhd+/epUMPPbSs2ieeeKL04YcfLvOzOXPmlLp3717adtttl8PqoOlp6vs6ZujQoaVOnTqVFi9evFyPC02BfQ3NT3Pa1z/60Y9K/fr1Kx100EGlTp06Vb4wovwq9Qq28847hxBCmD59egghhBEjRoTOnTuHadOmhcGDB4cuXbqEgw46KIQQQkNDQxgzZkzYeOONQ/v27UPPnj3DyJEjw3vvvbfMMUulUhg9enRYe+21Q8eOHcNOO+0UXnjhhc+9/WnTpoVp06YVrrN///6f+XWNbt26ha9//evhH//4xxe+39CcNZV9HdOnT5/w0UcfhYULF5Z9DGhu7Gtofpravp46dWr4xS9+EX7+85+H1q39om+1OcMr2KeboVu3bkt/tnjx4rD77ruH7bbbLlx66aVLf7Vj5MiR4YYbbgiHHXZYOO6448L06dPDFVdcEZ5++unwyCOPhDZt2oQQQjjzzDPD6NGjw+DBg8PgwYPDU089FQYNGvS5F8NddtklhBDCjBkzylr/P//5z7DaaquVVQvNVVPb1x9//HGYP39+mDdvXnjggQfC+PHjw8CBA0OHDh0qOQ3QrNjX0Pw0tX19wgknhJ122ikMHjw4/OY3v6nkrtMYtf3Auvn69Fc4Jk2aVJo9e3bpjTfeKN16662lbt26lTp06FCaOXNmqVQqlQ499NBSCKF02mmnLVP/0EMPlUIIpQkTJizz83vvvXeZn7/zzjultm3bloYMGVJqaGhY+udGjRpVCiF85lc4evfuXerdu3dZ9+nBBx8stWjRovSTn/ykrHpo6prLvr7wwgtLIYSl/+2yyy6l119//QucCWg+7GtofprDvr777rtLrVu3Lr3wwgtL1+pXqavLJ8ZV9v9/SVXv3r3DhAkTwlprrbXMz4888shl/v+2224LK6+8cthtt93CnDlzlv78019xnjx5chg+fHiYNGlSWLhwYTj22GNDixYtlv65E0444XO/cKPcT4rfeeedMHz48LDuuuuGU089taxjQHPR1Pf1gQceGLbYYoswe/bscPfdd4dZs2aFjz/++AsdA5ob+xqan6a6rxcuXBh++MMfhh/84Adho402alQNldMYV9mVV14Z+vbtG1q3bh169uwZ+vXrF1q2XPafdrdu3Tqsvfbay/xs6tSpYe7cuaFHjx6fe9x33nknhBDCa6+9FkIIYcMNN1wm7969e1hllVWWy32YP39+GDp0aPjwww/Dww8/7KviyV5T39e9e/cOvXv3DiH86830EUccEXbdddfw0ksv+bVLsmVfQ/PTVPf1L37xizBnzpxwzjnnlH0MvjiNcZUNGDAgbLHFFsk/065du89s0oaGhtCjR48wYcKEz63p3r37cltjysKFC8N+++0XnnvuuXDfffeFr3zlKyvkdqGeNfV9/f8bNmxYuPbaa8ODDz4Ydt9995qsAWrNvobmpynu67lz54bRo0eHo446KnzwwQfhgw8+CCH8a2xTqVQKM2bMCB07dow27ZRPY1yn1l9//TBp0qSw7bbbJv+m99O/HZ46dWpYb731lv589uzZn/nWvC+qoaEhHHLIIeGPf/xj+M1vfhN22GGHio4HuauHff15Pv11y7lz5y73Y0NzZ19D81PLff3ee++FefPmhZ/+9Kfhpz/96WfyddddN3zjG98Iv/3tb8s6PnHGNdWpAw44ICxZsiScd955n8kWL14c3n///RDCv/7tRJs2bcLll18eSqXS0j8zZsyYzz3uF/ma+GOPPTb8+te/DldddVXYb7/9vvB9AJZV6309e/bsz/35uHHjQosWLcLmm29efCeAZdjX0PzUcl/36NEj3HnnnZ/5b6eddgrt27cPd955Zzj99NPLvm/E+cS4Tu2www5h5MiR4cILLwzPPPNMGDRoUGjTpk2YOnVquO2228Jll10Whg0bFrp37x5OPvnkcOGFF4ahQ4eGwYMHh6effjr84Q9/+NyxSo39mvgxY8aEq666KgwcODB07Ngx3HTTTcvk++67b+jUqdNyu7+Qg1rv6/PPPz888sgjYY899gi9evUK7777brj99tvDlClTwrHHHhs22GCDatxtaNbsa2h+armvO3bsGPbZZ5/P/Py3v/1t+Otf//q5GcuHxriOjR07NvTv3z9cc801YdSoUaF169ahT58+4eCDDw7bbrvt0j83evTo0L59+zB27NgwefLksNVWW4X7778/DBkypOzbfuaZZ0IIITz22GPhscce+0w+ffp0jTGUoZb7esiQIWHatGnh+uuvD7Nnzw7t27cPm2yySRg/fnw49NBDl8fdgyzZ19D81HJfUxstSv/5uT8AAABkxr8xBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAstboOcYtW8Z7aBOfoFg97pPUvm6Kis5xixYtVtBKWNEqeewrqW1oaEgvrAY8z6Ey9Xi9btWqVTSr5noree2slWqt2bmovUqub0uWLCn8M83rXTEAAAB8QRpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACy1uhxTU3xK71hRTMmpbac/3xV8th73kB+7Pt/KToPqff/1aytlmrdbiXnopJjFx23Vrdbrdpq84kxAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWWv0HGMoRyWzyJri7OymuGYAyE1Tu15Xa73VPA+VzLIt97hFx65Wbb0+n2r1GFSrttp8YgwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGTNuCaq+rX3e+yxRzS76qqrkrX//d//Hc2uvfbaZO2cOXOSebXU81fQA7BiGOvC8laP4y9rNe6nkmNXsubmtg+KHoNU3rJl+rPVJUuWRLOi81jL99I+MQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrLUqNHMplPmt9q2QWcZHUrLIJEyYka7/1rW+VfbupNY8cOTJZe91115V9u9VSj/PviubQAWkNDQ21XsJnuF6vGOXOSv3Vr36VPO4rr7wSzX73u98la//5z39Gs1VXXTVZ++qrr0azomvFxx9/HM3q8dpXpB7X3KpVq6oct15nEddK6nzUatZ3Jbebqi16/KZNmxbNevfunazt1q1bNHvvvfeStdV6XqVmK3/Ku2IAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrrWu9ABqvkq8vT41aGDBgQLJ2r732imbDhg1L1pb7tfdFevXqVXYtrGipffD9738/mp199tnJ466xxhrRbPHixcnao446Kpr96U9/StZOnz49mUMOUvt6vfXWi2YHHXRQ2bd51llnJfPUOJKicT+VXJNvueWWaDZ8+PCyj8u/VfJ+qh7HT9WrehxBVa330gcffHAyX3vttaNZ0XuMopFM9conxgAAAGRNYwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGTNHOM6U+48stSssRBCOPDAA6PZeeedl6xt3bo6T5NK5urNmjVrOa4EQvjGN76RzNdff/1odumllyZrFy5cGM2KZoumNDQ0RLPU7PIQQhg7dmw0K5o/mJp9eP/99ydrIQczZsyIZldeeWWyNjX3980330zW9u3bN5oVvSakFF2vt9xyy7KPDaRVMqt40003jWaXXXZZ2bebml1eVFvPc7V9YgwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGTNuKYVrOgr11NfYZ6qnTBhQvK4AwcOjGbVHOFQiWnTpkWziRMnVu12c9JUv04/ZuONN07mp5xySjTbbbfdkrXdu3ePZkuWLEnWpkYypUYuFb1eVDLCIfX4Pvzww8na1N6EXJT7+nnssccmj3vyySdHs6KxSL///e+jWdu2bZO1lbwWderUKZlDc1et9/eV+trXvhbNOnfunKxdvHhxNPvJT36SrK3kPWQltZWeS58YAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDVzjOtMav7WueeeG8223Xbbso9bpJLZa6naefPmJWuPOOKIaPbyyy8na2mcpjirOGWrrbZK5t/5zneiWTXPRerYn3zySTRr3759NZYTQghhypQp0WzfffdN1lZz5iLkLrW/UvPUQwhhpZVWqsrtLliwIFm7zTbblH27NE61Xnfr9fW8R48e0eyiiy5K1r799tvR7Iwzzih7TSlF7yFqdZ433njjaFa0punTp0ez119/vew1FSl3Rnxj8iI+MQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALJmXNMKVvQ14i1bxv+uIjWKplrjmEIIYfHixdGsdev0Uyi1rqKxEi+++GIyr0f1OvYgF3/4wx+Seeq5XLQPPvzww2j27LPPJmu33377aNahQ4dkbbmKnounnXZa2bVA+dq2bZvMU+NkRo0atbyXs9TChQuj2Y033pisnTFjxnJeDctTrV7TU9fVoveAEyZMiGY777xzsnbkyJHphVVBrc5xqm8IIYTdd9+97GO/9dZb0axW4z6rfZ59YgwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWzDFewYrmb6Xmgm2yySZl3+6cOXOi2ejRo5O1Dz74YDT71a9+lazdeOONo1nRDLT27dsn83pUq7lu/Mvbb7+dzNu1a1f2sYtmLqb89a9/jWbrr79+2cdNvZ4UPRcPPPDAaJba80W32xzZ13mq5Hrdp0+faPaXv/wledzVVlut7DWdcsop0exHP/pR2bVFc4ypb6nnajVfz1PH/q//+q9k7Y477hjNPvroo2Ttn//852ReDUXXiWqd54aGhmS+4YYbRrOiNa288splralIJeei2tdjnxgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZM66pzqS+wjw1amHAgAHJ444bNy6aPfXUU8na1JiGr3zlK8na1Neqv/7668na1157LZnXo9zG2OQkNR5i+PDhydrUSKZKRi5VMoJjiy22iGZFI6ReffXVZN7c2Nd5Ktp/q666ajS7/fbbo1lqHFMI6fErLVumP8+46KKLotndd9+drP3lL3+ZzGm6avUa9s1vfjOaHXzwwcna1P4bMmRIsvaVV15JL6xMTXF036xZs6LZOuusk6ydNGnS8l5OCKGy81jJGL3G8IkxAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWTPHuM6k5hemZgw+++yzyeM+99xzZa9pzz33jGaVzFkdO3Zs2WuqV01xxh3/UjQb74QTTohmF198cdm3W8ks4kqeb5tuumk0e/zxx5O15513XjRLzUwPIT0PGurJ5ZdfnsyPPvroqtxuq1atqlK76667JmtT85XnzJmTrHXty1OHDh2S+WmnnRbNip4zjz32WDR78MEHk7W1uK7Walb06quvnszXXHPNso/98ssvl12bUs1zVemxfWIMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkrVmPa6rkK7tTtdUcS5C63VQ2c+bM5HFbtoz/HcgWW2yRrH3vvfeiWWq8Q5Enn3yy7FpY3rbeeutkfuaZZ0azJUuWJGtT+y8lNb4thBAmT54czT755JNkbWoM26qrrpqsHTNmTDT729/+lqz985//nMyhXnTv3j2ZlzsS5plnnkkeN3Vt/Otf/5qsTY1S69mzZ7L2Jz/5STQ7/vjjk7Xk6aqrrkrmm2yySTR75513krUXXHBBNKvV+/BaSd3fNdZYI1mbuj9F7zFmzZqVXliZih6/Sh6DSp8bPjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACyVvNxTbX6WvT27dsn89Rolh/96EfJ2tRXhX/wwQfJ2pVXXjmazZs3L5p16tQpedwFCxZEs7lz5yZri0Y8pPzmN7+JZn/605/KPi4sb5XsobZt2yZrU+OaUq+BQ4cOTR73/vvvj2atWrVK1k6aNCmaff3rX0/Wpl7jjjnmmGTtAw88UNZxYUX7zne+k8zHjRsXzVLP84ULFyaPW+7YxhBC2HfffaNZakQbxGy55ZbR7OCDD07Wpp6vRdfN++67L72wMm+3KUpdG4tGqaXOxSuvvJKs/d///d/0wsq83Wqq9HZ9YgwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWaj7HuBIdOnRI5scee2w022+//ZK1qbltDQ0N6YUldO7cOZmnjp2as1o0/7Ndu3bRrEePHsnalKLbve6666JZJecRlreOHTsm85VWWimaFc0MTnn++eejWSWzvpcsWZLMb7vttmi2/fbbJ2tT+36fffZJ1vbt2zeavfTSS8laWJEWLVqUzCdOnFjWcSuZs1lUu/LKK5d9bPJU9D7uxRdfjGaLFy9O1rZuHW8zUtfUENKzwIvm666xxhrRLHV/KrHVVlsl89Sadtttt2Tt3Llzo1nReUwZO3ZsMk+93hQ9b5oqnxgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZq/txTS1bxnv30aNHJ2t/+MMfRrOiUUGpr6C/9dZbk7XTp0+PZqNGjUrWljvGoZLxD9X8yvX33nsvmtXrmsnTQw89lMz/+7//O5odeeSRydrU8/WAAw6IZkXjYirZQ3369Cm7tpIRDqmRFcY1NV+pa3kIIey0005l1/71r3+NZqkxJ/UqtYeK9m1q1GSR1LlqjtfcSl4/m5Oi8/Dhhx9Gs1NOOSVZm8p79uyZrB04cGA022abbZK1tVDUV6TOc1FtalxrJW688cZk3hz3fRGfGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJC1up9jfOCBB0azY489Nlmbmr/1ySefJGtHjBgRzW677bZkbcqXv/zlZL7//vtHs0pmh6bmQC5ZsiRZm1I0/+7hhx+OZs8991yy9oILLohmd911V3ph8AW9++67yfy4446LZuPGjUvWfve7341mL774YnphVdKuXbtoVs3Zha1atSr7ds0drW9bb711NDvjjDOStYMGDYpmbdu2TdZefPHF0ey0005L1lai3OdjUV2bNm2i2XXXXVd2bdHM0kcffTSZNzc5zmhd3q644opkfu2110azfv36JWt33XXXaLb22msna1dfffVotnDhwmRt6n1ravZy0XuI+fPnR7PUekMI4bzzzkvmKakZydV8/99U+cQYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICsaYwBAADIWs3HNRV9XX6XLl2iWWrsRwjpr0bffffdk7WPPfZYNGvfvn2ydtSoUdFs3333TdZW6+vPU1/XXnSbqdrUGKgQQrjmmmui2c9+9rNk7cyZM5M5rEip16pnnnkmWTthwoSybrOS14Ozzjormf/gBz+IZpWMTSqqTY2HaK7jH3Kx7rrrRrPBgweXfdyi59QRRxwRzRYtWpSsTY1BKapNrSv1PuHss89OHveAAw6IZqlzHEL6el00Gu7ee+9N5vBFLViwIJoVjexM5U1xtF9qzRtttFGy9pxzzolmRb1Q6jx++OGHydqUen0MKh3D5hNjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAslbzOcZFc64WL15c9rFTs4hHjhyZrD3ttNOi2dChQ8teU5HU/U3NJf3rX/+aPO4xxxwTzfbee+9kbevW8afJwoULk7VXX311NDOnmHpSzZl7f/nLX6py3NSaN99882RtavZhahZqpap5bGorNbO0aLZk6vlYVLvyyitHs1GjRiVrf/rTn0az7t27J2s7d+4czc4444xo9p3vfCd53NQe+eCDD5K1F1xwQTS75JJLkrWVzv+kfqUe20qufbWaZVvN63W1zlWq9qWXXkrWvvHGG9FsvfXWS9YWzWNnWT4xBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAslbzcU1FX/WeGnlQpEOHDtHs4IMPLvu4lZg2bVoyHzZsWDR79tlny77d1Fe9f+Mb3yj7uL/+9a+T+csvv1z2saGpqGRkRSXjH9q0aRPNNtpoo7KPW7SmVP7hhx8ma2fNmlXWmmi81ONTzZE8jz/+eDR77bXXkrXrrrtu2bebur9F48EmT54czTbbbLOyb7eSx+BnP/tZNBszZkyy9q233krmsDwVXSua2wiwSu5PJa9TH3/8cdm3e8MNN5RdW6trSS35xBgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICs1XyOcZFrrrkmmm233XbJ2vbt20ezO++8M1mbmt1VVJuaC1w0l3nq1KllranIzJkzq3Lcgw46KJlfeuml0ey5554r+3bJV+r52rFjx2TtmWeeGc323HPPZG3fvn2j2TPPPJOsPfroo6PZU089Fc1222235HGHDBkSzfr06ZOsrURqfuHtt9+erH3ppZfKOm4Ilb1W5aRW8yXffvvtaHbIIYcka3/6059Gs2222absNbVsmf77/80337zsY6fOc2pe9/e///3kce+5555oVjTvFD5PJfNove5WX6tWrZL5mmuuGc2KHp9qvWZU8ryo5+ecT4wBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICstSg1cq6Dr2v/t2p+RXm1pEbRpEZDhJBec9H9eeSRR6LZ9ttvn6xtbmr12KcUjTKpR6kxbNOnT0/WrrbaamXfbmrf33rrrcna8ePHR7Nddtklmh1//PHJ47Zt2zaZV8uf/vSnaFY0iua1116LZk3xOlOP43Oa4nlMjRcbOHBgsjY16ik15iSEEBYtWhTNiq6Nhx9+eDSbO3duNKvHawHLqsfHqGikT0rq/lTz9aIez2M9KnoMUq8nnTp1StY+8MAD0Sz1/qNIJY9trUY9LVmypPD4Te9dMQAAACxHGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrrWu9gKaoHueyFc0E+9rXvhbNiu7PCy+8EM2efPLJZO1FF12UzOGLSs2h6969e9nHLZpHm9pjW2+9dbK2W7du0WzXXXdNL6xK/v73v0ezQw45JFn77LPPln27TXHGLtU3Y8aMsrIQiueIp9Tj9RzqST3ukaLrSD2uuRIvvvhiNOvfv3+ydt11141mtZpFXKSWj59PjAEAAMiaxhgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKy1KDXyO7GN2KhvRY9Pp06dolnHjh2Tte+8805Za2JZ9Tg+oGXL5vV3Y0Ujl8aPHx/N3nvvvWRtaiRCv379krWLFy+OZquvvnqytlwTJkxI5scdd1w0+/DDD5f3cpqtoudcLbheQ2Xq8XrdqlWrsmtT96eS0UfVHJuUOnY1xwzV4rGv5DW7qDb1vEm9NwmheueiVqOeGnO9bl7vigEAAOAL0hgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZM8cYVpB6nIvY3OYYF83k/uSTT6LZkiVLlvdyGqVdu3bRrGjGYK3WXC1Fe6Qer0PmGEPzU4/X69Q82lrN9W2OteWq5nOmkjXXapZ0rc5z6nYb856peb0rBgAAgC9IYwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNaMa4IVpB7HPzS3cU2wohnXBM1PU7te12p8UTVH9hjXtHzU6npQqzFRKY25XntXDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNZa13oB0JyYHwrwxVVzlip8nqZ2va5kvdW6r9U8h/V4f+vtNutZU3zOheATYwAAADKnMQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGstSuYcAAAAkDGfGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY1xM9anT58wYsSIWi8DWI7sa2h+7GtofuzrpkdjXCU33HBDaNGixdL/2rdvH/r27RuOOeaYMGvWrFovr1EaGhrCT3/607DuuuuG9u3bh0022STccssttV4W1ExT39dvvfVWOPjgg0O/fv1Cly5dQteuXcOAAQPCjTfeGEqlUq2XBzXR1Pf1p6ZNmxaGDx8eevToETp06BA23HDDcMYZZ9R6WVATzWFfv/322+GII44I6667bujQoUNYf/31w4knnhj+7//+r9ZLa7Za13oBzd25554b1l133bBgwYLw8MMPh6uvvjrcc8894fnnnw8dO3as9fKSzjjjjHDRRReF73//+2HLLbcMv/vd78Lw4cNDixYtwre//e1aLw9qpqnu6zlz5oSZM2eGYcOGhV69eoVFixaFiRMnhhEjRoSXXnopXHDBBbVeItRMU93XIYTwzDPPhB133DGstdZa4aSTTgrdunULr7/+enjjjTdqvTSoqaa6r+fNmxcGDhwY5s+fH4466qiwzjrrhGeffTZcccUVYfLkyeHJJ58MLVv6fHO5K1EV48ePL4UQSlOmTFnm5yeeeGIphFC6+eabo7Xz5s1bLmvo3bt36dBDDy2rdubMmaU2bdqUjj766KU/a2hoKH39618vrb322qXFixcvlzVCU9LU93XM0KFDS506dbKvyVJT39dLliwpfeUrXylttdVWpY8++mi5rAeauqa+rydMmFAKIZTuvvvuZX5+5plnlkIIpaeeemo5rJD/n79qWMF23nnnEEII06dPDyGEMGLEiNC5c+cwbdq0MHjw4NClS5dw0EEHhRD+9avMY8aMCRtvvHFo37596NmzZxg5cmR47733ljlmqVQKo0ePDmuvvXbo2LFj2GmnncILL7zwubc/bdq0MG3atMJ1/u53vwuLFi0KRx111NKftWjRIhx55JFh5syZ4bHHHivr/kNz1FT2dUyfPn3CRx99FBYuXFj2MaC5aSr7+v777w/PP/98OOuss0KHDh3CRx99FJYsWVLJXYdmq6ns6w8++CCEEELPnj2X+fkaa6wRQgihQ4cOX+Be01h+lXoF+3QzdOvWbenPFi9eHHbfffew3XbbhUsvvXTpr3aMHDky3HDDDeGwww4Lxx13XJg+fXq44oorwtNPPx0eeeSR0KZNmxBCCGeeeWYYPXp0GDx4cBg8eHB46qmnwqBBgz73Te4uu+wSQghhxowZyXU+/fTToVOnTuHLX/7yMj8fMGDA0ny77bYr7yRAM9NU9vWnPv744zB//vwwb9688MADD4Tx48eHgQMHutDCf2gq+3rSpEkhhBDatWsXtthii/Dkk0+Gtm3bhn333TdcddVVYdVVV634XEBz0VT29fbbbx9atmwZjj/++PCzn/0srL322uG5554L559/fthnn33Cl770peVxOvj/1fYD6+br01/hmDRpUmn27NmlN954o3TrrbeWunXrVurQoUNp5syZpVKpVDr00ENLIYTSaaedtkz9Qw89VAohlCZMmLDMz++9995lfv7OO++U2rZtWxoyZEipoaFh6Z8bNWpUKYTwmV/h6N27d6l3796F6x8yZEhpvfXW+8zP58+f/7nrhRw09X39qQsvvLAUQlj63y677FJ6/fXXv8CZgOajqe/rvffeuxRCKHXr1q100EEHlf7nf/6n9JOf/KTUunXr0jbbbLPMbUEumvq+LpVKpeuuu67UtWvXZa7Xhx56aGnRokVf8GzQWD4xrrJdd911mf/v3bt3mDBhQlhrrbWW+fmRRx65zP/fdtttYeWVVw677bZbmDNnztKf9+/fP3Tu3DlMnjw5DB8+PEyaNCksXLgwHHvssaFFixZL/9wJJ5zwuV+k80U+UWrXrt1nft6+ffulOeSqqe7rTx144IFhiy22CLNnzw533313mDVrlj1N9prqvp43b14IIYQtt9wy3HTTTSGEEPbff//QsWPHcPrpp4c//vGPn7lvkIumuq9DCGGttdYKAwYMCIMHDw69e/cODz30UPiv//qvsNpqq4VLL7200ceh8TTGVXbllVeGvn37htatW4eePXuGfv36feZb5Fq3bh3WXnvtZX42derUMHfu3NCjR4/PPe4777wTQgjhtddeCyGEsOGGGy6Td+/ePayyyiplr7tDhw7hk08++czPFyxYsDSHXDXVff2p3r17h969e4cQ/tUkH3HEEWHXXXcNL730kr1Ntprqvv50zx544IHL/Hz48OHh9NNPD48++qjGmGw11X39yCOPhKFDh4bHH388bLHFFiGEEPbZZ5+w0korhXPOOSccfvjhYaONNir7+Hw+jXGVDRgwYOkTOqZdu3af2aQNDQ2hR48eYcKECZ9b07179+W2xs+zxhprhMmTJ4dSqbTM34C9/fbbIYQQ1lxzzarePtSzprqvY4YNGxauvfba8OCDD4bdd9+9JmuAWmuq+/rT6/H//yU9n76h//+/KAhy0lT39TXXXBN69uz5mbXvvffe4eyzzw6PPvqoxrgKNMZ1av311w+TJk0K2267bfITnE8/9Zk6dWpYb731lv589uzZFV0MN91003DdddeFf/zjH8tsvL/85S9Lc+CLqfW+jvn016jnzp273I8NzV2t93X//v3DtddeG958881lfv7WW2+FEGr3F27QlNV6X8+aNetzv11+0aJFIYR/fWEYy59xTXXqgAMOCEuWLAnnnXfeZ7LFixeH999/P4Twr3870aZNm3D55ZeHUqm09M+MGTPmc4/b2K+J/8Y3vhHatGkTrrrqqqU/K5VKYezYsWGttdYK22yzzRe7Q0DN9/Xs2bM/9+fjxo0LLVq0CJtvvnnxnQCWUet9/Y1vfCO0a9cujB8/PjQ0NCz9+XXXXRdCCGG33Xb7AvcGCKH2+7pv375h1qxZ4c9//vMyP7/llltCCCFsttlmjbsjfCE+Ma5TO+ywQxg5cmS48MILwzPPPBMGDRoU2rRpE6ZOnRpuu+22cNlll4Vhw4aF7t27h5NPPjlceOGFYejQoWHw4MHh6aefDn/4wx/Caqut9pnjNvZr4tdee+1wwgknhEsuuSQsWrQobLnlluG3v/1teOihh8KECRNCq1atqnG3oVmr9b4+//zzwyOPPBL22GOP0KtXr/Duu++G22+/PUyZMiUce+yxYYMNNqjG3YZmrdb7evXVVw9nnHFGOPPMM8Mee+wR9tlnn/Dss8+Ga6+9Nhx44IFhyy23rMbdhmat1vv6mGOOCePHjw977bVXOPbYY0Pv3r3DAw88EG655Zaw2267ha222qoadzt7GuM6Nnbs2NC/f/9wzTXXhFGjRoXWrVuHPn36hIMPPjhsu+22S//c6NGjQ/v27cPYsWPD5MmTw1ZbbRXuv//+MGTIkIpu/6KLLgqrrLJKuOaaa8INN9wQNtxww3DTTTeF4cOHV3rXIFu13NdDhgwJ06ZNC9dff32YPXt2aN++fdhkk03C+PHjw6GHHro87h5kqdbX6x//+MdhlVVWCZdffnk44YQTlmmWgfLUcl/369cvPPnkk+HHP/5xuOmmm8I///nPsOaaa4aTTz45nHPOOcvj7vE5WpT+83N/AAAAyIx/YwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWWjf2D7Zq1SqaGYW8/NTjuWzRokU0K1pvrWorkbrdSjQ0NFTluJVo2TL+d2P1+FyEWki9JtjXrEhF1yeP7/JRj+fRvobKNGaf+MQYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICsNfpbqSv5xrtUbSXfsFitbw+upXLvUyXf8FzpsWuhud2fWnEuoJh9wvJUyfsez8V8eeyh+nxiDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNYaPcc4pVozjqtZW6RWM5KrNbe5knNVj+eiXm+3Oc7WBpoW807r2+DBg5P5+eefH81+8IMfJGsff/zxstYEgE+MAQAAyJzGGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImsYYAACArC2XcU21UjQap1qjj6qp3HXVapRTLY9dC/X6vAGgfvzwhz+MZmeeeWaytmvXrtGsXbt2ydozzjgjmv36179O1r7yyivJHJqK1Hu1pvi+tFrvPas5nrQpnucQfGIMAABA5jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkTWMMAABA1ho9x7gpzqOqZO5Xq1atotlJJ52UrD3uuOOi2RprrJGsTZ3nuXPnRrPU7MIQQvjoo4+i2YwZM5K1Dz74YDQrel40t1lyABBC+vr2ve99L5otWrQoedw999wzmk2ZMiVZm3qfsOGGGyZrp02bFs1cr6knHTp0SOZbbbVVNHvxxReTtf/85z/LWlMlKulXimovuOCCaHbKKacka19++eVo9pWvfCVZu2TJkmRer3xiDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZK1FqZHfwZ/6OvCirwqv1tf8V2scUwghnHXWWdGsaFxT27Zty1pTkUpGH6XGNb3//vvJ2pNPPjma3Xbbbcna3EY8pB6jhoaGFbiSxqlkDwH1+RpnX68YqfcRDz30UDQrGpF40EEHlbukpEqeqy1bpj9HqcfrWyXs69pL3d+bbropWTt8+PBoNmbMmGTtD3/4w2ReDUWPbZs2baLZOeeck6xN9SxF+/a5556LZgMGDEjW1qPG7GufGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJC11rVeQJFqzSo+/fTTk7VnnHFGNFu8eHGy9oMPPohmK620UrK2XEXnqVOnTmVlIYQwfvz4aLZgwYJk7V133ZXM61FuswJrITUT85BDDknWfve7341mw4YNS9bOmjUrvbAmZr311otmEydOTNauscYa0Wz11VdP1qZe46ApSb3eF828XHvttaPZ+++/H82uvfba5HHrcYZuc5tTTP3bfPPNo9ngwYOTtann62abbVb2mqqlaM+nrsk/+tGPyj72pZdemqw9++yzk3lz5BNjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAga40e11TJSINyj1up/fffP5qlxjEVueaaa5L5r371q2hW9BXzffr0iWYdOnSIZjvuuGPyuN27d49mRY9f+/bto9mQIUOStb///e/Lvt1KVOv5WqQex2zUo9TevP7665O1M2fOjGZf//rXk7W33357NKvH52Nq74UQwoQJE6JZ6rWkyHHHHZfMR48eXfaxoZ6k9l+bNm2StTfccEM0++ijj6LZ5MmTC9cVU/SeqVrvx1zbWN5SYxtDCOHb3/52NOvSpUuyduHChdGsWtevSvZmUe2VV15Z1nFDCOGhhx6KZj/96U+Ttanz2Fz5xBgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICsNXqOcSXzt2plp512imZt27ZN1s6YMSOaFc39Ss1ZnTJlSrI2JfUYrL/++snab37zm9HshBNOSNb26NEjmh1yyCHJ2p///OfR7OWXX07WVqJWMxfrdS/Um3feeSeaLVmyJFm71lprRbNf//rXydrvfe976YUldOzYMZotWLAgWfvlL385mnXu3DmabbzxxsnjDhgwIJpV8lwsuj+Qg4022iiZb7HFFtHs6KOPXt7LCSFU99pmVjEr0iabbJLMTzzxxGhW9FydN29eNJs0aVJ6YQnVeo939tlnJ/M999wzms2aNStZe9ppp0Wz999/P1mbI58YAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWWv0uKZKvqK8VqOeUuOaikycODGapcYxhVC9+5uqffXVV5O1F198cTS79957k7W33HJLNOvXr1+y9qGHHopmqTFQlUqdKyMpau/BBx+MZkUjwC677LJotuqqqyZrr7322vTCytSyZXX+jrFodFVKQ0NDMk/tkSuuuKLs24V6UnTNTV0P+vTpk6x95ZVXotlzzz2XrKX2jFesvpVWWimanXTSScna1N4sGjN02GGHJfNyVfL+ftNNN41mZ5xxRrI2dezzzz8/Wfv4448nc5blE2MAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACy1ug5xpWo1Qzkzp07l137t7/9Lb2wMlUyQ7da5/HZZ59N1n7zm9+MZjfccEOytn///tHsH//4R7J21113jWZvvvlmspb6lno+puZmh5B+3vz0pz9N1u68887RrJK5v0W1c+bMiWZPPfVUNJs8eXLyuLNnz45m48aNS9amZi8vXrw4WZs6F0UznSuZzQxfVNE1t1OnTtFsr732Stb++c9/jmbPPPNMspbaq+T9GI3TpUuXaPatb30rWZu6zrz99tvJ2gceeCC9sDKl1tS1a9dk7bnnnhvNWrVqlay95557olnR+3C+GJ8YAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWVsh45pqJfV17d/+9reTtauuumo0W2mllZK1c+fOjWZFI5fKHR9QVJe63aLaF154IZoNHTo0WTtq1KhodswxxyRrf/nLX0az1CinEIxhaM5SY1AGDRqUrE3tg7Zt2yZr27dvH8223nrrZO3999+fzMu19957R7OiPZAam/Taa68la9dcc81otueeeyZr77333mQOy1PRNXfdddeNZsOGDUvW7r777mWtCXKRGqs0ceLEZG1qf/Xr1y9Ze9ZZZ0WzW2+9NVmbGmHas2fPaFY0NmnHHXeMZrfddluy9vjjj49m8+fPT9byxfjEGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKy1KDVy4GvLluX30NWaKVs0n/CUU06JZhdeeGHZx07N9Q0hhG222Saaffjhh8nactdUzTnGRec5JXXshoaGZO3ixYuj2RZbbJGsfe6559ILS6jWuSq6v7VQyWNL9RXNVv773/8ezdZbb71kbSWvy6naDTbYIFk7Y8aMsm+3HtXjzHT7+t+KzkVqtuj48eOTtX379o1mCxcuTNZS3+zr6uvQoUMyv+qqq6LZ/vvvn6zt1KlTWWsKofz3akV9UipPzXsOIYTevXtHs0WLFqUXxlKN2dc+MQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALLWurF/MPUV17X6Cvmir91+4IEHotnDDz+crN16662j2X333ZesnT9/fjJPKfdc1utjkFrXHXfckazdb7/9otn3vve9ZO3xxx8fzepxDAP5atOmTTT785//nKzt06dPNKvkeT5v3rxkfvPNN1flduHzVHJ9S405CSGEvffeO5pdfPHFydpqjUmpZGQgNBUff/xxMj/88MOj2Zlnnpms3WyzzaJZ0fvHwYMHR7NKXouWLFkSzbp3756svf7666PZrbfemqydNGlSNPvSl76UrE2NPm2ur0U+MQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALLWotTI79tu2TLeQ9frV3anvla9U6dOydp//OMf0ey73/1usjb11ejVOleVjE2qZE2VfHX9zjvvnMwnTpwYzaZMmZKsTY3bqtW5qsd9UqsxX/xbauTSyy+/nKxt3brRE/c+4+67745mI0eOTNa+/fbbZd9uc2NfV1/R/Um9P5k2bVqy9pFHHolmBx10UHphVVKtx6/oudqxY8doVjRapx73QSXq8f40t31diaJzkXr8fvjDHyZrL7300rLWlBrHFEIIp5xySjRr165dsjY1Ymq11VZL1r700kvRLDXWKoT0OKcZM2Yka+tRY/a1T4wBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICsaYwBAADIWvlDMP9DJfPEqnm7KfPnz0/mCxYsiGa33HJLsvYb3/hGNEvNTKxErR6DSsyePTuZp2bCbbDBBsna5jZvmKYtNW94yy23LKsuhPRzteg1oU2bNtHMnGJWtEqu5w0NDdFslVVWSdaedNJJZd9utaT2df/+/ZO1p512WjQbNmxYsnbWrFnRbM0110zWum6yIhU93/r06RPNUvOEK7nd4cOHJ2vvuOOOaJZ6DQshhHvuuSeanXHGGcna1L4vet3t3LlzMm+OfGIMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkbbmMa6qVSsYDFH1F+d/+9rdotu+++yZr+/XrF80efvjhitZVrtRxi85jtdb0/vvvJ/PUqJpVV101WVvJ/S33uBCz2267RbNbb7217ONWMq5pp512imZPPvlksnafffaJZm+88UayFpa31JiUhQsXJmuL8nrz6quvJvPU+Kmjjz46WZsaJ/nlL385WZu6Xg8dOjRZ+/e//z2a/e53v0vWpl4Da/XehuWjkvdxp59+ejTr0aNHsvajjz6KZptttlk0mzZtWvK4lbz3fP7556PZwQcfnKxNjZ899NBDk7Vf/epXy1pTU+YTYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALLWpOcYV9OUKVOi2bbbbpus7dq1azSrx7l5tVrT5ptvXnbtxx9/nMyrNcfYXEQ+T2rPhxDCUUcdFc1atoz//eQrr7ySPO6dd94ZzdZbb71k7ZAhQ6LZ1772tWTtGmusEc3MMaYclczk3mOPPaLZKqusUvbtVqKSa1Cqdu7cucnaVN65c+dkbWp++R133JGs7du3bzIv16hRo5L5RRddVPaxq/XY0ziVvF/addddk/n3vve9aNbQ0JCsvfrqq6NZ0TW5WlLnquj+rLnmmmXfbps2bcqurZVK34f7xBgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiacU0R77zzTjTr3r17sna77baLZj//+c/LXlO1VHMEUerY5557btnHHTduXDIv+vp6WJ623nrrZD548OBotmDBgmh23HHHJY973333RbO99947WbvffvtFs6LXhL/+9a/JHJanVVddNZmnxjX95S9/SdZ+8sknZa2pmlL7b/vtt0/WbrPNNtHs+OOPT9b27NkzvbCE1JqLXk+uuOKKaHb77beXfbtFjFesraLHLrXvb7755rJvd+zYscn8tNNOK/vY1ZI6V1/60peStTvuuGNZxw0hhOeeey6ZV0u1+o7G8IkxAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWVsuc4yrOUeu0nlU5Zo9e3bZta1bx09r0f0td/5uJTO/qjkvrG/fvtHsq1/9arL2n//8ZzT72c9+ll5YQr0+52i6Zs6cmczHjx8fzcaMGRPNXnzxxeRxO3ToEM1+9KMfJWtT/va3v5VdC8vbyiuvnMxXWmmlaNa2bdtk7brrrhvN3nzzzWTtEUccEc0efPDBaNayZfozif333z+abbLJJsnaXXfdNZoVvb/4+OOPo9mECROStan3THfddVeyNjUXvdz3RI3hWl/fDjjggGhWNNv8nXfeiWaXXHJJsraaz7lytWrVKppdfPHFZdcWvcd444030gurklruTZ8YAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWWv0uKbm9rX2Rffn9ddfL7t26NCh0ezOO+9M1p5++unR7KWXXopmtfp6+aKxEz/84Q+jWdF5vOKKK6LZjBkzkrWVSI1zKlpzc9snNM7zzz+fzL/3ve9Fs0qeb6n9NWDAgGRt6jXjqKOOStbCinTQQQcl89SIxP79+ydrU6PJKhllWC1Frwnz5s2LZqNGjUrW3nHHHdHsrbfeSi8swXWRcqSum0Xat28fzWo1gij1elL0WnPqqadGsz333LPsNaXeZ4cQwv/93/+VfeymyifGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZK3Rc4wrmbWZUqv5dkUzw5599tlo1rdv32TtvffeG8323nvvZO1ee+0VzV588cVodtdddyWP+/Of/zyazZkzJ1nbo0ePaHbKKacka4844ohotmDBgmRtteZLFj3nKjl2Pc69ZPlIPbbrr79+sja1xxYvXhzNTjzxxORxFy1aFM0WLlyYrH3vvfeiWadOnZK1sCI9+uijyfyyyy6LZn369EnWpmYgb7jhhsnaddddN5o98MAD0eyJJ55IHvfPf/5zNJs4cWKytprXRliRPvjgg2hW9Fzt0qVLNPvZz36WrP39738fzWbPnh3NUu/RQwjhS1/6UjT70Y9+lKz99re/Hc1atkx/xjlt2rRo9thjjyVr61HRa1ylr2M+MQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALLWotTI77Uu+jrwlHocAVDNsTqrr756NDvrrLOStd/97nejWatWrcpeU0NDQzQrOhfVeuzPOOOMZO1FF11U1nFDqN7IikqOm3oMasV4qcbbdNNNo1nR+JWpU6dGsxtuuCGapfZAkY8//jiZ77zzztHs8ccfL/t2c5Pb9S0nRde+1GNfj88LGq8eH7/c9nX//v2jWdGYodS5quQ9byXvD6s12nbSpEnJ2uHDh0ezd999t+w1NUWNeQx8YgwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWlssc43qc91ap1Dyyas7Q/fKXvxzNDjjggGh2+umnJ4/bpk2baFbJ7LUlS5Yka0866aRodsUVVyRrK5m9XC3VmkNXK7nNRazEFltsEc0efvjhZG3r1q2jWSWPQWr/TZw4MVm79957l3VclmVfQ/NjX9deJe/DL7300mh2/PHHl72mSnqhVP72228na2+66aZo9pOf/CRZ63r+b+YYAwAAQAGNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWVsu45qKNMWvvU+tuR5HBeX2Nf6VqOa4rZTU+Kla8bxpvLXWWiuanXHGGcnaH/zgB9Es9Xx87rnnksdNjaSYMGFCspbloyle34A0+zpf9Xie6/H52BQZ1wQAAAAFNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWGj3HODXXq5KZwNVUySyySmYGV3J/63F+WnNjjvG/eb4tH/369Uvm2223XTR79dVXo9kDDzyQPG49PqdyU4/zJe1rqIx9Dc2POcYAAABQQGMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWVsi4pkpUa2xSrdZcK6n7W6vxRbmpx9E6HluoTFN7va/X8YqwoqX2gus1ND/GNQEAAEABjTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJC11o39g7Wan1bJ7TbFNddCU1svwIrU1F4jK5lbX+5xKz12c1Otx6BWKnnsK9k/ReequZ1noDKVXq99YgwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGStRcn32QMAAJAxnxgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNcTPWp0+fMGLEiFovA1iO7GtofuxraH7s66ZHY1wlN9xwQ2jRosXS/9q3bx/69u0bjjnmmDBr1qxaL6/QjBkzlln/f/5366231np5UBNNfV+/9dZb4eCDDw79+vULXbp0CV27dg0DBgwIN954YyiVSrVeHtREU9/XIYTwyiuvhGHDhoVVVlkldOzYMWy33XZh8uTJtV4W1ExT39eu17XRutYLaO7OPffcsO6664YFCxaEhx9+OFx99dXhnnvuCc8//3zo2LFjrZdX6MADDwyDBw9e5mcDBw6s0WqgPjTVfT1nzpwwc+bMMGzYsNCrV6+waNGiMHHixDBixIjw0ksvhQsuuKDWS4Saaar7+o033ggDBw4MrVq1Cqecckro1KlTGD9+fBg0aFD44x//GLbffvtaLxFqpqnua9frGilRFePHjy+FEEpTpkxZ5ucnnnhiKYRQuvnmm6O18+bNWy5r6N27d+nQQw8tq3b69OmlEELpkksuWS5rgeagqe/rmKFDh5Y6depUWrx48XI9LjQFTX1fH3XUUaXWrVuXXnzxxaU/mz9/fmmdddYpbb755stlfdDUNPV9HeN6XV1+lXoF23nnnUMIIUyfPj2EEMKIESNC586dw7Rp08LgwYNDly5dwkEHHRRCCKGhoSGMGTMmbLzxxqF9+/ahZ8+eYeTIkeG9995b5pilUimMHj06rL322qFjx45hp512Ci+88MLn3v60adPCtGnTvtCa58+fHxYuXPhF7ypkoynu6//Up0+f8NFHH9nn8B+ayr5+6KGHwmabbRb69eu39GcdO3YMe++9d3jqqafC1KlTy7r/0Bw1lX0d43pdXX6VegX7dDN069Zt6c8WL14cdt9997DddtuFSy+9dOmvdowcOTLccMMN4bDDDgvHHXdcmD59erjiiivC008/HR555JHQpk2bEEIIZ555Zhg9enQYPHhwGDx4cHjqqafCoEGDPnfT7LLLLiGEf/0b4sY455xzwimnnBJatGgR+vfvH84///wwaNCgSk4BNDtNbV9//PHHYf78+WHevHnhgQceCOPHjw8DBw4MHTp0qOQ0QLPSVPb1J598ElZZZZXP/PzTtT355JNhww03/OInAJqhprKvP+V6vYLV9gPr5uvTX+GYNGlSafbs2aU33nijdOutt5a6detW6tChQ2nmzJmlUqlUOvTQQ0shhNJpp522TP1DDz1UCiGUJkyYsMzP77333mV+/s4775Tatm1bGjJkSKmhoWHpnxs1alQphPCZX+Ho3bt3qXfv3oXrf+2110qDBg0qXX311aW77rqrNGbMmFKvXr1KLVu2LN19991lnBFo+pr6vv7UhRdeWAohLP1vl112Kb3++utf4ExA89HU9/Vee+1V6tq1a+mDDz5Y5ucDBw4shRBKl156aWNPBTQbTX1ff8r1esXyiXGV7brrrsv8f+/evcOECRPCWmuttczPjzzyyGX+/7bbbgsrr7xy2G233cKcOXOW/rx///6hc+fOYfLkyWH48OFh0qRJYeHCheHYY48NLVq0WPrnTjjhhM/9h/mN/RuqXr16hfvuu2+Zn33nO98JG220UTjppJPCkCFDGnUcaI6a6r7+1IEHHhi22GKLMHv27HD33XeHWbNmhY8//vgLHQOam6a6r4888sjw+9//PnzrW98K559/fujUqVO46qqrwhNPPBFCCPY2WWuq+/pTrtcrlsa4yq688srQt2/f0Lp169CzZ8/Qr1+/0LLlsv+0u3Xr1mHttdde5mdTp04Nc+fODT169Pjc477zzjshhBBee+21EEL4zK9Jde/e/XN/taoSq666ajjssMPCRRddFGbOnPmZNUMumvq+7t27d+jdu3cI4V8X3SOOOCLsuuuu4aWXXvLrWWSrqe7rPffcM1x++eXhtNNOC5tvvnkIIYQNNtggnH/++eHUU08NnTt3LvvY0NQ11X39KdfrFUtjXGUDBgwIW2yxRfLPtGvX7jObtKGhIfTo0SNMmDDhc2u6d+++3Nb4RayzzjohhBDeffddjTHZam77etiwYeHaa68NDz74YNh9991rsgaotaa8r4855phw2GGHheeeey60bds2bLrppmHcuHEhhBD69u1b9duHetWU9/Xncb2uLo1xnVp//fXDpEmTwrbbbpv8G6FP/xZp6tSpYb311lv689mzZ3/mW/OWh1dffTWEULsXBGjK6nVff/prWXPnzl3ux4bmrl72dadOncLAgQOX/v+kSZNChw4dwrbbblvxsSE39bKv/3+u19VlXFOdOuCAA8KSJUvCeeed95ls8eLF4f333w8h/OvfTrRp0yZcfvnloVQqLf0zY8aM+dzjNvZr4mfPnv2Zn7355pvh+uuvD5tssklYY401GndHgKXqcV+HEMK4ceNCixYtlv4aJtB4td7Xn+fRRx8Nd9xxR/jud78bVl555bKOATmr9b52va4NnxjXqR122CGMHDkyXHjhheGZZ54JgwYNCm3atAlTp04Nt912W7jsssvCsGHDQvfu3cPJJ58cLrzwwjB06NAwePDg8PTTT4c//OEPYbXVVvvMcRv7NfGnnnpqmDZtWthll13CmmuuGWbMmBGuueaaMH/+/HDZZZdV4y5Ds1frfX3++eeHRx55JOyxxx6hV69e4d133w233357mDJlSjj22GPDBhtsUI27Dc1arff1a6+9Fg444ICw9957h9VXXz288MILYezYsWGTTTb53C//AYrVel+7XteGxriOjR07NvTv3z9cc801YdSoUaF169ahT58+4eCDD17mV6NGjx4d2rdvH8aOHRsmT54cttpqq3D//fdX9M3RgwYNCmPHjg1XXnlleO+990LXrl3D9ttvH3784x/7WyqoQC339ZAhQ8K0adPC9ddfH2bPnh3at28fNtlkkzB+/Phw6KGHLo+7B1mq5b5eaaWVwhprrBGuuOKK8O6774a11lorHHfcceGMM84IXbp0WR53D7Lkep2fFqX//NwfAAAAMuPfGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJC11o39gy1bxntoo5ChWD3uk9S+Bv6lRYsW0WzJkiUrcCWN43oNlanHfWJfQ2Uas0+8KwYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALLW6HFNAJCjpjYKpamtF2ohNYatHtnXUH0+MQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrjZ5jbH4aFGtqcxGhHKnnuWsF0BR4rQL+fz4xBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAstbocU2sGOWO+ykaO5A6bsuW6b8fWbJkSVlrAqorta833HDDZO2f/vSnaLbmmmsma08++eRo9otf/CJZa0QKALVSyVjNSq5fxhw2DT4xBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGstSo0cnlXJ3C/+reg8ph6OAQMGRLO//OUvZa+pyE033RTNDjnkkGSt2Wz/Vo/nomiGNfWtdev4KPq77rorWbvbbruVfbtz5syJZr17907WLly4sOzbrUcNDQ21XsJnuF43X126dEnmW2+9dTQbO3Zssna99dYra00hhHDOOedEs8svvzxZ+95770WzWu2verxe29f/tvbaayfzO+64I5r1798/WfvGG29Es9R77V/+8pfJ4/7v//5vMk8xA3n5aMy58q4YAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImnFNEan727Vr12Rt586do9mwYcOStSNGjIhmPXr0iGarr7568riVfJ17qvbaa69N1v7qV7+KZo888kjZa6pXqedNPY51Ma6p+Ro9enQyP+2008o+9uLFi6NZ0bimWbNmlX279age93Vu1+umZpVVVknm++yzTzQbNWpUsnb99dcvZ0khhOqNhFmyZEky32mnnaLZww8/XPbtVqIeR+DY1/9WdC5S16HzzjsvWbvffvtFs/bt25e9pnnz5kWz//7v/07WXnfdddHspZdeStbW43O5VoxrAgAAgAIaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGtNeo5xJWsqutsbbbRRNJs4cWKydo011ihrTdWUmiNYNMu2kvP84YcfRrM999wzWfvoo4+Wfbv1qB5nyZlj3Hx16tQpmb/88svRrGfPnsna1OzegQMHJmuffPLJZN7UmGPM59luu+2i2fXXX5+s3WCDDcq+3dS1/q233krWXn755dHs5ptvTtZus802ZdeOGTMmmv3oRz9K1lbrulqP12v7esU46KCDolmfPn2i2Yknnpg8bpcuXaJZmzZtkrWLFy+OZvvvv3+y9ve//300q8fneTWZYwwAAAAFNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGStda0XUE2pr+WeOnVqsrZbt27RrGvXrmXfbq2kxvKkxjuEEELr1uU/TVJfT7/77rsna5vbuCZYkVZbbbVk/sknn0SzSl7D1llnnWTe3MY1wec56qijotmGG26YrP3jH/8Yzf7nf/4nWXvvvfdGs9deey1ZW4k777wzmhWNBdxss82iWSWvRUXjjerxvRq1N2HChLLqLrzwwmSe6h1SY6BCCOHWW2+NZpdcckmydsqUKdHs7bffTtY2RZWONfOJMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFlr1nOMU7p3757MU/N3K1HNmXzlKpoxWC1rrrlmTW6X6it6rqbyoj3St2/faFY0czs1s++9995L1tZi5mUlczjXXnvtZG3RvOGU1Lp+85vfJGuPPPLIaDZu3Liy1wSfp5qzbFPH/vDDD6PZ8OHDk8dNzQResGBB8cIiKnkPUXSe2rVrF83mzp2brN1iiy2iWatWrZK1S5YsiWbmFLMiNTQ0JPPUe4yi9x/Tp0+PZrvttluy9tvf/nY0+8UvfpGsbYoq3fc+MQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALLWrMc1pcYQVXOcTLnHrUTRmlK3u2jRomRt27Ztyzpukeeff77sWupb0fPxe9/7XjQbNWpUsrZXr15l325qnMLUqVOTtZMmTYpmbdq0Sdauvvrq0eykk06KZkXjp1L5oEGDkrWp18eisROp81z0mrDHHntEM+Oamq9qXftCSO+/outbJVL7YOTIkVW73Vro0KFDMj/88MOjWdeuXZO1Y8aMiWapcUzQXHzta19L5jvuuGM0K3pt/fWvf13OkpqsSq81PjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgay1KjRzKW80ZhLWwcOHCZF40P7RcRae73PmglcwxLqpNzTutxNprr53M33rrrarcbq1UMv+6Wqr12FZirbXWSuadO3eOZn379k3WpmaLFs39Tanksa1knnAls9pTay663ZSi2z3xxBOj2RVXXFH27dZKJeeqWmp1va7kdlO1V155ZbJ2n332iWZF15nUnNxK7k89vt4X3Z/U68nZZ5+drD399NOj2YwZM5K1W2yxRTR7//33k7XV0hQfP6qvkvfSPXr0iGYXX3xxsnbEiBHR7Pnnn0/WpvbXJ598kqxtbhqzr+vvXTEAAACsQBpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACyVp2ZRHUi9bXqkydPTtbuuuuuVbndSmqrNT6gkrEuRRYsWBDNZs2aVfZxab7efPPNZJ56vr700kvJ2j/84Q/RbMstt0zW7rnnntFs5513TtZutdVWyTymaG/WapzMmDFjotmFF16YrK3V+JWUar4GNifVHBczevToaHbEEUcka1OPT2ocU5FKxiBW631A0fipzTbbrOzanj17RrMzzjgjWZty6aWXJvN6fE2gvtVqdFW3bt2iWWr/hBDCz372s2i2++67J2tTI5kOPPDAZG1uI5kq5RNjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAstai1MgBjbWaGZZSyZratm2bzB977LFolpoTGEJ69mEl8zBnz54dzVZaaaVkbbt27cq+3UpmK++///7R7Le//W25S6pbqXPV0NCwAlfSOC1b+rux5aFozurll18ezSp5DCrZm3//+9+j2T777JOsnTFjRtm329zU476u1fU6dbu9e/dO1r7yyivRrGiPTJ06NZrtuOOOydqidaWsscYa0WyHHXaIZkWzlb/zne9Es9Qc1Uql9m7Rc+qDDz6IZltvvXWytmj+fC3U4+tYPb4Pr5Wic5HK77zzzmTtWmutFc2K3v+nfPjhh9Hsl7/8ZbL2Jz/5STSbO3du2WvKTWP2tXfFAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkTWMMAABA1lrXegGVKPra7dTXtX/yySfJ2jZt2pS1pkql7lP37t3LqqvkNkNIn8eir8yvx5EH1ZTb/c1Jq1atotnhhx+erE2Nm0lllYwCKtqbG2+8cTQ7//zzk7UjRoyIZgsXLkzWUn2VjPGqROrYixcvTtZ+9NFH0axz587J2g033DCavfnmm8na1JqLxkRV61y+//770eyOO+5I1v7f//1fNHvttdeStfvtt180++pXv5qsve6666JZPY5jommr5H1r165dk7WbbrppWbc7Z86c5HH79u0bzebNm5esrcexgM2VT4wBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICstSg1ct5A0eiP5uaDDz6IZl26dEnWVnN0UrkqGd9RSe3//M//RLOJEycma8eNG1f27dajelxz0TgSGueaa65J5qlxTldddVU0+9a3vpU8buq1qG3btsnalCVLliTzt956K5r95Cc/SdbeeuutZd9uParHMRpN8XqdGpFy+eWXJ2u32Wab5byafyl6zX799dejWer6dvfddyeP++ijj0az9957L1nboUOHaDZo0KBk7fjx46NZ0evJJptsEs1eeeWVZG2tpPaJfZ2v9dZbL5qdfvrp0axobOPMmTOjWdHeNPJs+WjM+3DvigEAAMiaxhgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaOcYRt99+ezTba6+9krWtWrWKZkXnsR7nGFfL7Nmzk/mWW24Zzd54443lvZxGqeTxy2mOca2e5/Wq3P1XdB6/+tWvRrO+ffsmaw888MBoVvQal1pX0Zo32mijaPbyyy8na+uReafVV8n96dSpUzJfbbXVolnR6+Orr74azWr1GPzoRz+KZuedd16yNnV///SnPyVrU68ZCxYsSNbWo3q8RjW3fd0UtWnTJpqNHTs2WXvYYYdFsz/84Q/J2iOOOCKavfnmm8la/s0cYwAAACigMQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGvGNUXsscce0eyee+5J1tbia/4reXyqud5KxkQ99dRT0WzbbbdN1i5cuDC9sBqox/EP1RrXVKRDhw7R7NZbb03W/v3vf49mp59+etlrMmLq3/bZZ59kftttt0WzovP09a9/PZo9/vjjydp6vA4Z18QXVfT4pPLBgwcna++8885oVvR6n9q7N998c7L2kEMOSeZNTT2+3tvXTdv06dOjWe/evZO1559/fjT7yU9+UvaacmNcEwAAABTQGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFlrXesF1KvHHnssml1++eXJ2n/84x/R7KGHHkrWpmaVPf3009Fs3rx5yeOuttpq0WyXXXZJ1vbo0SOa9erVK1mbUjSTr3///tGsaI7x5MmTy1oTK8Ypp5wSzVIzxEMI4ZZbbin7diuZA5maAVrJzMt6nJeZmoUaQghvv/12NFtzzTWTtYMGDYpmf/nLX9ILg2agTZs2yXznnXeOZnfccUeytlWrVtGs6LVm2rRp0azoNQFyV/T+4vDDD49mf/zjH5O166yzTllr4ovziTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA145oi5s6dG82OP/74qt3uPvvsU7Vjl+uaa66JZt///vdX4Er+rX379jW5XZaPs846K5o1NDQka/fee+9otv/++ydr+/XrF826d++erE2NPEuNcgohhFmzZkWzq666qqwshBDefffdZF6urbfeOpl36tSp7GNvuummZddCU5Ea3ZJ6DQshve9T45hCSI9kevTRR5O1qfc2Tz31VLIWclc0Dm327NnRbPHixcnazp07R7OiMVH1OBKynvnEGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKxlO8e4aO5XJVIzwyqZN1atNRfNRfzkk0/KPna17k/btm3LrqX6ih7b9957L5p17do1WfvNb36znCVVVdGcwB49ekSz1EznddZZJ3ncZ599Npq1bp1+eW/Xrl00O/zww5O1K6+8cjQreuwrqTWPkRWpaD75l7/85Wh2/vnnR7MhQ4aUvaaXX345mZ966qnR7N57703WLlq0qKw1ASGsvvrqyfy6666LZkXX6xTXxeXLJ8YAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWmvS4pkrG/RR9rfo+++wTzaZPn56sfffdd6PZm2++maxduHBhNEuNV9lyyy2Tx+3evXs0+8Y3vpGs3W233aJZNb8mfv78+dHspZdeqtrtUrmi58W4ceOiWdGooFVWWSWaLV68OFmbGr9S9HpSyetNufvksMMOK/u4Reut5FykbrehoSFZ+9FHH5V1XJq21PUrhPRosqJr7pIlS8paUwghrLrqqtHsqquuStZuv/320Sw1oq1of40fPz6anX322cnamTNnJnP+rZpjO6muao4+TV0b11prrWj2/e9/P3nc1Pv0ovcuN9xwQzJn+fGJMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFlrUWrk4Mh6nPdWtKaVV145mqXmqIYQwn777VfWmkKobBZnapZZq1atolk1Z6ymjl3JfS1a86xZs6JZauZlCMUz4WqhHme0pp5T9bjeIh07dkzmp512WjQ7+uijk7Wp15NKVDJvsVqPUdHtpmYVv/jii8nab3/729HshRdeqGhdtVA0t7kW6vE89e3bN5k/8sgj0exvf/tbsnallVaKZkXzk1dbbbVolppFHEIIixYtimZTpkyJZj//+c+Tx/3tb38bzZri63JTVI/nuVr7+ktf+lIyP/3006PZoYcemqyt5nvTlNS+P/PMM5O1++67bzTr169fNCu6r6n3pUUzkG+88cZkTuM05jnlE2MAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrrWu9gEoUfe126uvPBw0aVNGxU1LjO1q2TP9dROvW8YcktaZKRi5Voui4S5YsiWbz5s1L1l566aXRrB7HMTVF9TiSohIfffRRMk+NaRg7dmyy9qtf/Wo0S418CSH9XN9qq62i2XHHHZc8btu2baNZ0RihoteilN/97nfRbPTo0cnav//979GsHscMsXxMnTo1mV988cXRLDU+JYT0+L727dsna996661o9tBDDyVrUyOmUntkxowZyePCitS1a9dkPnz48Gg2ffr0ZO3Xvva1aJZ6vxtC+vrWvXv3ZO36668fzTp37pysLVfRqMLDDz88mv3lL39Z3suhTD4xBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGstSo0cZNoU50tutNFG0axDhw7J2l122SWavf3228na7bffPprtt99+ydq//e1v0Sw1l2311VdPHreSmaWVPPY33XRTNBszZkyy9umnn45m9Tp/N3WuiubK1kIlzwuqr2jvVbIPKtnX9br/aqEe93VTvF6n9OjRI5n36tUrmi1atChZ+8knn0SzV155JVm7ZMmSaFaPe6SaryfNTT2ei1rt69R76f333z9Ze9JJJ0Wzdu3aJWsXLlwYzaZNm5asffnll6NZp06dkrXXX399NEu93j/zzDPJ47722mvJnOprzL72rhgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMhasx7XVI/qcVxCrR7behyHUCnjmiAv9bivXa+hMvX4/sS+hsoY1wQAAAAFNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWWtd6Abmpx9l49bimpsq5BID6Zy5w46TOk/c8NDc+MQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALJmXBOFfFU/ANRe0Ygh1+TGc64ax3kiJz4xBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGuNnmNcNDuvuUnNbavmHMGmdp4rOReV3Neic1yr2ctN7fEDitnX/1LN1+xKbrda15ki5d5uNa9ftXqMKlGtx6ia56oWavW+huWjVq8JtdJU34f7xBgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMhai1K9fs83AAAArAA+MQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgaxrjZqxPnz5hxIgRtV4GsBzZ19D82NfQ/NjXTY/GuEpuuOGG0KJFi6X/tW/fPvTt2zccc8wxYdasWbVeXqG33norHHzwwaFfv36hS5cuoWvXrmHAgAHhxhtvDKVSqdbLg5qwr6H5aer7esaMGcus/z//u/XWW2u9PKiJpr6vQwjhlVdeCcOGDQurrLJK6NixY9huu+3C5MmTa72sZq11rRfQ3J177rlh3XXXDQsWLAgPP/xwuPrqq8M999wTnn/++dCxY8daLy9qzpw5YebMmWHYsGGhV69eYdGiRWHixIlhxIgR4aWXXgoXXHBBrZcINWNfQ/PTVPf1pw488MAwePDgZX42cODAGq0G6kNT3ddvvPFGGDhwYGjVqlU45ZRTQqdOncL48ePDoEGDwh//+Mew/fbb13qJzVOJqhg/fnwphFCaMmXKMj8/8cQTSyGE0s033xytnTdv3nJZQ+/evUuHHnrocjnWp4YOHVrq1KlTafHixcv1uNAU2NfQ/DT1fT19+vRSCKF0ySWXLJe1QHPQ1Pf1UUcdVWrdunXpxRdfXPqz+fPnl9ZZZ53S5ptvvlzWx2f5VeoVbOeddw4hhDB9+vQQQggjRowInTt3DtOmTQuDBw8OXbp0CQcddFAIIYSGhoYwZsyYsPHGG4f27duHnj17hpEjR4b33ntvmWOWSqUwevTosPbaa4eOHTuGnXbaKbzwwgufe/vTpk0L06ZNK3v9ffr0CR999FFYuHBh2ceA5sa+huanKe7r+fPn28eQ0FT29UMPPRQ222yz0K9fv6U/69ixY9h7773DU089FaZOnVrW/SfNr1KvYJ9uhm7dui392eLFi8Puu+8etttuu3DppZcu/dWOkSNHhhtuuCEcdthh4bjjjgvTp08PV1xxRXj66afDI488Etq0aRNCCOHMM88Mo0ePDoMHDw6DBw8OTz31VBg0aNDnXhx32WWXEMK//k1SY3z88cdh/vz5Yd68eeGBBx4I48ePDwMHDgwdOnSo5DRAs2JfQ/PT1Pb1OeecE0455ZTQokWL0L9//3D++eeHQYMGVXIKoNlpKvv6k08+Caussspnfv7p2p588smw4YYbfvETQFptP7Buvj79FY5JkyaVZs+eXXrjjTdKt956a6lbt26lDh06lGbOnFkqlUqlQw89tBRCKJ122mnL1D/00EOlEEJpwoQJy/z83nvvXebn77zzTqlt27alIUOGlBoaGpb+uVGjRpVCCJ/5FY7evXuXevfu3ej7ceGFF5ZCCEv/22WXXUqvv/76FzgT0HzY19D8NPV9/dprr5UGDRpUuvrqq0t33XVXacyYMaVevXqVWrZsWbr77rvLOCPQ9DX1fb3XXnuVunbtWvrggw+W+fnAgQNLIYTSpZde2thTwRfgE+Mq23XXXZf5/969e4cJEyaEtdZaa5mfH3nkkcv8/2233RZWXnnlsNtuu4U5c+Ys/Xn//v1D586dw+TJk8Pw4cPDpEmTwsKFC8Oxxx4bWrRosfTPnXDCCZ/7RTqN/ZvnTx144IFhiy22CLNnzw533313mDVrVvj444+/0DGgubGvoflpqvu6V69e4b777lvmZ9/5znfCRhttFE466aQwZMiQRh0HmqOmuq+PPPLI8Pvf/z5861vfCueff37o1KlTuOqqq8ITTzwRQgiu2VWiMa6yK6+8MvTt2ze0bt069OzZM/Tr1y+0bLnsP+1u3bp1WHvttZf52dSpU8PcuXNDjx49Pve477zzTgghhNdeey2EED7z6xTdu3f/3F/B+KJ69+4devfuHUL415vpI444Iuy6667hpZde8muXZMu+huanqe/r/7TqqquGww47LFx00UVh5syZn1kz5KKp7us999wzXH755eG0004Lm2++eQghhA022CCcf/754dRTTw2dO3cu+9jEaYyrbMCAAWGLLbZI/pl27dp9ZpM2NDSEHj16hAkTJnxuTffu3ZfbGr+IYcOGhWuvvTY8+OCDYffdd6/JGqDW7Gtofprbvl5nnXVCCCG8++67GmOy1ZT39THHHBMOO+yw8Nxzz4W2bduGTTfdNIwbNy6EEELfvn2rfvs50hjXqfXXXz9MmjQpbLvttslPcD791Gfq1KlhvfXWW/rz2bNnf+Zb85aHT391Y+7cucv92NDc2dfQ/NTrvn711VdDCLVrzKEpq5d93alTp2XmkU+aNCl06NAhbLvtthUfm88yrqlOHXDAAWHJkiXhvPPO+0y2ePHi8P7774cQ/vVvJ9q0aRMuv/zyUCqVlv6ZMWPGfO5xG/s18bNnz/7cn48bNy60aNFi6a91AI1nX0PzU4/7+s033wzXX3992GSTTcIaa6zRuDsCLFXrff15Hn300XDHHXeE7373u2HllVcu6xik+cS4Tu2www5h5MiR4cILLwzPPPNMGDRoUGjTpk2YOnVquO2228Jll10Whg0bFrp37x5OPvnkcOGFF4ahQ4eGwYMHh6effjr84Q9/CKutttpnjtvYr4k///zzwyOPPBL22GOP0KtXr/Duu++G22+/PUyZMiUce+yxYYMNNqjG3YZmzb6G5qfW+/rUU08N06ZNC7vssktYc801w4wZM8I111wT5s+fHy677LJq3GVo9mq9r1977bVwwAEHhL333jusvvrq4YUXXghjx44Nm2yyyed+qRfLh8a4jo0dOzb0798/XHPNNWHUqFGhdevWoU+fPuHggw9e5lcoRo8eHdq3bx/Gjh0bJk+eHLbaaqtw//33V/RNlEOGDAnTpk0L119/fZg9e3Zo37592GSTTcL48ePDoYceujzuHmTJvobmp5b7etCgQWHs2LHhyiuvDO+9917o2rVr2H777cOPf/xjvwUCFajlvl5ppZXCGmusEa644orw7rvvhrXWWiscd9xx4YwzzghdunRZHnePz9Gi9J+f+wMAAEBm/BtjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAga42eY9yyZbyHNvEJitXjPmnRokWtlwBNWj3ua9frFSP1+lmr81zJmqpVW6Qen5P1uCbXa6hMY/a1T4wBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICsNXpcUz1+dT0AsKymOCqoKWpu96la96eaY6IAliefGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJC1Rs8xJl9mDAI0D6nX8xBC6NSpUzRbsGBBsnbbbbeNZm+//Xay9uWXX07mNE61rslFz5tKbtf7CKBe+MQYAACArGmMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImnFNzUQloxSqOYYBctCqVatk3tDQEM3sL1akoufbb37zm2i25557Jms//PDDaNatW7f0wqi53EYzFr33AfLjE2MAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACyZo5xE9KyZfzvMfr06ZOsnTRpUtm1qfmFS5YsSdYOGTIkmk2cODFZC8tbag917do1Wfu///u/0Sw1pziEEAYOHBjNUrM033vvveRxjz766Gh2yy23JGvh8zz66KPRLLV/QghhlVVWiWatW6ffbixatCi9MBqlWrN5ix771PuEohnI1Vpzc5y9TJ5ymzFeiUpfT3xiDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZK1FqZHf812tr9Nvjir5WvVu3bpFs9NOOy2aHXPMMcnjFo3KSEmtuWiEw+uvvx7Nttpqq2Tt7Nmz0wtrYurxK/Wb274uuj+77LJLNLvjjjuStZ06dSprTZUouj+pMVHXXnttsvaEE06IZp988kmyln/LaV8XHXe99daLZq+++mqyth7PY5F6HDOUWlPRegcNGhTNxo4dm6x96623olnRKLzTTz89mt11113J2mo9b+rx+djcrtdNUT0+BvX4XK1XjTlXPjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAga+YYV8GBBx4YzUaMGJGsHThwYDTr2LFjNCt6GOfPnx/Nzj///GRtu3btotnZZ5+drE3ZZpttkvlf//rXso9dj+px1lxz29c9e/ZM5pMmTYpmX/7yl5O1qdm+V111VbL2nXfeiWZDhgyJZh999FHyuHvssUcyT/nTn/4UzXbbbbdkbT0+l2ulHs9FU9zXLVvG/55+o402StbOmDEjms2bNy9ZW8m5Sl3PU+8DvvnNbyaPO3fu3GhWyXrXWWedZD5t2rRolppTHEIIo0aNimb3339/snbOnDnRrGh/pc5HJXvTvm6+KjmPO+64YzTbYYcdkrWpOeFbb711svaf//xnNJs4cWKydvHixdHs8ssvT9Y+88wzybypMccYAAAACmiMAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImnFNEan7+9///d/J2sMPP7zs2009HAsXLoxmv/71r5PHvfDCC6PZK6+8kqz9xje+Ec1uv/32ZG1qTNR6662XrJ09e3Yyb2qMf1g+UmsuGh925plnRrPXX389Wbv//vtHsyeffDJZm3rsK3kMUqNoOnTokKxtaGiIZp07d07WLliwIL2wjNjXy0ePHj2iWdF15tVXX41mjzzySLI2NY5kypQpydrUPkntzUpGEFXirLPOSuap18c11lgjWZsaSVfJHik6F5W8tqZq7evqq+Zju+2220azwYMHJ2u//e1vR7NevXqVvaaUWj1Xp0+fnsz32WefaPbCCy+Ufbu1YlwTAAAAFNAYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWWtd6wXUq/79+0ezgw46qOzj/ulPf0rmv/vd76LZb37zm2iWmiFYZIMNNkjm5557bjQrmgl2zTXXRLM5c+akFwafo02bNtFsp512Stamnq9F87yfe+65so5bpJJZjdtvv300u+GGG5K1G2+8cTTr3r17svaNN95I5vBFpWbeP/roo8na7bbbLpp9/etfT9YOHTo0mhXt6w8//DCZV0OrVq2S+cEHHxzNvvOd7yRr995772hW9B6jVnN/U6+R9TiLuLlp2TL9+dpGG20Uzb72ta8la0855ZRo1qFDh2TteuutF80qmTecek4VzfVNvb8vWlNDQ0M0mzlzZrJ2k002iWZHHXVUsja15tS85xBCeOKJJ5J5vfKJMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDXjmiJ23XXXaFb0NfGXXHJJNDv99NOTtUuWLEkvrExdunSJZv/zP/+TrP3yl78czf7+978na1MjY4xSoBypPTJt2rRk7ZZbbhnN1lxzzWTtwoUL0wurgqI98vTTT0ez999/P1mbGg/xrW99K1l76aWXJnP4olLjms4777xkbWpkzM9//vNk7ZVXXhnNUqOPQgjhzTffTOblateuXTQ766yzkrWp8Sv/9V//lay9++670wtLqGRsUrVqi1RSm5PUebr11luTtTvvvHM0W2WVVcpeU9HzIjV+ceLEicna1Nil1J4v2j+VjGYs97ghhHDkkUeWfbvrrrtuNDvzzDOTtanxb9VU6b72iTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZa1Fq5DDZ3Oa9pe5v//79k7VPPPHE8l5OCCG9ppVXXjlZe/PNN0ez3XffPVn7z3/+M5odffTRydrf/va3yTwn9Ti3Obd9nZp3WvT41OPjl3LYYYcl89RM0w8++CBZ26dPn2i2aNGiZG1zU4/Pi9z2dSXuu+++aLbpppsma1PzlVPzXTt06JA87g9+8INolppJGkIIV199dTQrmjuamhHfHKX2SUNDwwpcSePUal+nriXXXXddsjZ1zf3oo4+StePGjYtmxx9/fLK2Hl+XU6r52E6ePDma7bDDDsna1HmcMGFCsvY73/lOemE10JjnhU+MAQAAyJrGGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImsYYAACArLWu9QLqVeorvas1jimE9Fe2t2nTJpqdeuqpyeMOGjQomi1evDhZ+9Of/jSa/e53v0vWQj2pxxEcG2ywQTTr2bNnsnbgwIHRbPjw4cnaTp06RbOOHTsma9u3bx/NchvXRNP2k5/8JJrddNNNydrUOJKjjjoqmqWu5SGE0KNHj2i21VZbJWtffvnlaFaPr3+11NRG+tTKrrvuGs2Kxgy9+OKL0WzfffdN1qaey83tsavk/hSNXNpuu+3KPnbKjBkzqnLcWvOJMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFkzx7jOHHLIIdFszz33jGb7779/8ripWXNPPfVUsnbcuHHJHHJXNMsxNWfwV7/6VTRba621ksdNzT4sWlNKUW1qzvGHH35Y9u3CivbXv/41mvXt2zdZu8suu0Sz2267LZqtvPLKyeMuWbIkml1xxRXJ2ksuuSSa3Xfffcla+Dzf/OY3y66dPHlyNEvNOGZZqWvyJ598UnZt0fzkVL548eJkbVPlE2MAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrxjWtYHvvvXcyv+6666JZy5bxv8co+sr1lAEDBiTzxx57LJrdcMMNydoxY8ZEs9RICqg3qRFFDz/8cLJ2s802i2YNDQ1lryml6LipUQvt2rVL1qZG0Wy//fbphUEz8cc//jGanXHGGdFsq622Sh73oIMOimY77bRTsjZ17C233DJZa3wOnyd1rWjTpk2y9uOPP17ey2myKhmb9LWvfS2aXX311cnaVO9Q9D5h7ty50axo1GtT5RNjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAgay1KjZzzk/qacZa15557RrNf/vKXydpVVlklmv3lL3+JZs8//3zyuE888UQ0O+2005K1vXv3jmZFT59LL700mp1++unJ2uamkpFa1WJfN163bt2i2eOPP56sXX/99Zf3ckII6ZFnZ555ZrI2tTdTY2hCCOErX/lKNDv77LOTtZdffnk0q8c9UqQe11yP+7poTfV4Hqul6Fy0atUqmh155JHJ2v/6r/+KZqn3ECGEsPPOO0ezjz76KFnb3NTj87FW+/q3v/1tNCsaQfrkk09Gs6233jpZW48jPSt5DFJjky6++OJk7ciRI6NZapRkkaLn+dFHHx3NrrnmmrJvt1Yas699YgwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWzDEuQ2rGYAghPProo9GsdevWydo//elP0Wz06NHR7IMPPkgeN/Uw9+zZM1n7j3/8I5qtvPLKydqU1LzFEEL44Q9/WPax65G5iE1b6lxNmjQpWbvTTjst7+WEEEIYN25cNEvNPQwhhIaGhrJvt0+fPtFsxowZZR+3KbKvl49qrbkeH59KpGahhhDCb37zm2i27777JmvPO++8aHbOOecka5vbea7H+1Orfb3XXntFs9/97ndlH/cXv/hFMj/77LOj2Ycfflj27VZyHlPPiy5duiRrBwwYEM3uueeeZG2qdyh6rqbu7/3335+sPemkk6LZ3//+97Jvt1b7yxxjAAAAKKAxBgAAIGsaYwAAALKmMQYAACBrGmMAAACypjEGAAAga8Y1rWBF57EeRwSkxiZdcsklydpKnjdbbbVVNHviiSfKPm6t1ONja18vH/3790/me++9dzRbffXVo9mIESOSx02NrDjssMOStf/7v/8bzSoZ5ZQb+/rfUqMMi85TJeexHh+Dail6bL/61a9Gs4ceeihZO3HixGj2rW99K1m7ZMmSZN7U1ONzKvXY1+q95XHHHZfMi0YypaTe5z3++OPJ2rfeeiuavfPOO8na1AjTNdZYI5p9/etfTx73a1/7WjSr5uOXGqGYGiEVQgj/93//V/bt1iPjmgAAAKCAxhgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKxpjAEAAMhatnOMqzkzLHXsepyNV6RNmzbR7IQTTkjWnnvuudGsbdu2ydrUjOTTTjstWVuP6vGxb277uilKPQbDhw9P1t54443RbO7cucna1HzJCy64IFlrzvG/5bSvUzM8Qwhh0003jWb33XdfsjZ1HuvxHNerli3jn3fccccdydq99tormqXmrYcQwuzZs9MLq4FK9kE9vsalHtta7ZHUmkIIoVevXtEs9f4whBAGDRoUzbp3756srdZrYLVep1577bVk3qdPn7KPnZoHvdVWW5V93KbIHGMAAAAooDEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrrWu9gCK1+Mr1ej52LSxatCia3XLLLcnagw8+OJp99atfTdauvPLK6YVBM5B6vZgwYUKy9lvf+lY023PPPZO15513XjR74YUXkrV33nlnMm9ujDX7l1mzZiXze++9N5oVXRed48YpOk+pMUOffPJJ2cf+8MMP0wurQ83tvVg9KhprNWPGjGh2yCGHJGtT49+++93vJmu7dOkSzebMmZOs3WSTTaJZ6jXwpZdeSh73oYceimY77rhjsvass86KZkXP87vuuiuZsyyfGAMAAJA1jTEAAABZ0xgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJC1up9jnJrPlZrDGUIIH330UTS7+OKLk7VFs9nKVTSDsH379tFsyZIl0WzhwoVlr6kS/fv3T+Zf+cpXolnR7LVXXnmlrDVBLg488MBo9s9//jNZ27lz52h22GGHJWt/+9vfRrPmODu0Od6nchSdh0rOk3O8fKTeY7Rt2zZZO2XKlGi2YMGCstfE8pHbHnn22Wej2XHHHVf2cWs1Uz11u+PHjy+7tsjDDz9cdm2OfGIMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkre7HNW2//fbRrOjr2ldaaaVolhqLFEIIZ599djIvV2pESggh3HjjjdHs3HPPjWbPPPNMuUsqtMEGG0Szq666KlmbGnv14IMPJmt/9rOfpRcGK9CAAQOiWWqUWgghvP7669EstUfefffd5HE//vjjaPbiiy8ma1Oj1jbbbLNk7dChQ6PZ73//+2QtTVetxsUUjU9JrauS0Sv1eH+L3kMMGzYsmqX2bQgh3HvvvWWtKYSmOUqoWmN5WD5q9Zyqxe3OmDEjma+77rrRrGi8bGrsFZ/lE2MAAACypjEGAAAgaxpjAAAAsqYxBgAAIGsaYwAAALKmMQYAACBrGmMAAACyVvdzjB9++OFoduqppyZrUzN2R40alaxdb731otkdd9yRrF1rrbWi2fDhw5O1AwcOjGYPPPBANCuabfjhhx9Gsx122CFZe9BBB0Wz1VdfPVmbmsN6yy23JGuhngwePDia/fjHPy77uKlZxK+88kqytmfPntGsaG+mpF7DQiiemwjLU9G82aOPPjqaHX/88cnaLbfcMpq9//77ydpqSc1R7devX7L2xBNPjGatW6ff8t12223phTUzTXH2Ms1T0XMxlX/wwQfLezlZ84kxAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQtbof15QaC5Ia5RRCCK+99lo06927d7L2wAMPjGZFI5dSX6teNHYidX9//vOfl3WbIYTQsmX870Aq+Zr46dOnJ2uHDh0azV588cVkLdSTl156KZoVjS9q1apVNOvUqVM022STTYoXVuaaUq9FRWNq5syZU86SoCxFz+V27dpFsw022CBZmxqROHny5GTtJ598ksxjisYmbbXVVtEs9T4ghBA23njjaJYa2xhC+jXOaCOoTOqa26VLl7Jri8a1Gq/4xfjEGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImsYYAACArGmMAQAAyJrGGAAAgKzV/RzjlBdeeCGZH3nkkdHs+9//frL2rbfeimbbbrttsvarX/1qNHv77beTtU8++WQ0W2eddaJZv379ksdt06ZNNPvFL36RrJ0yZUo0e+qpp5K1qVnS0JT8+te/jmarr756snbUqFHRbNVVV41mRXPPJ02aFM2K9t7WW28dzQ477LBkbep1Cla0119/vezae+65J5r94x//SNbeeuut0WyllVaKZptuumnyuDvuuGM0K3pN+POf/xzNDjzwwGTtrFmzkjlQvtQs8LXXXjtZm5pF3KpVq2Rtnz59otlzzz2XrG2Kil4ji/jEGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImsYYAACArGmMAQAAyFqLUur7w//zD1b49deQu0ZutRXKvobK2Ne1l7q/J554YrL2gAMOiGYff/xxsnaHHXaIZqnnxcKFC5PHXbBgQTQ79thjk7U33XRTWWtiWfV4rnLb181N6vGbOXNmsrZoJGTKmDFjotnJJ5+crK3HfVCJxtwfnxgDAACQNY0xAAAAWdMYAwAAkDWNMQAAAFnTGAMAAJA1jTEAAABZ0xgDAACQNXOMYQWpx3lw9jVUxr6uvdT9reTxaYrnsR6fj/Uq9fg2NDSswJU0TlN8PtI4gwYNSua33HJLNGvTpk2y9tBDD41md955Z3phzYw5xgAAAFBAYwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNaMa4IVpB7HaDS3fV10f+rxMaBpq8fnVHPb17Ci2dfUk2qNpMuNcU0AAABQQGMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABkrXWtFwDNSVObM1ir2XjVOk+VrNkMZGKa2r6uR9U8h6m9Wat9XY+vrUW3W6vnebVet+v1/sL/z3Nx+an0XPrEGAAAgKxpjAEAAMiaxhgAAICsaYwBAADImsYYAACArGmMAQAAyFqLkhkkAAAAZMwnxgAAAGRNYwwAAEDWNMYAAABkTWMMAABA1jTGAAAAZE1jDAAAQNY0xgAAAGRNYwwAAEDWNMYAAABk7f8BRN0x3O1wENYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session = fo.launch_app(dataset, auto=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyTHF3R4L1jY",
        "outputId": "3226b296-c6e8-4247-c809-c1f674935db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.session.session:Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Welcome to\n",
            "\n",
            "âââââââââââââââââââââââââââââââ   âââ âââââââ ââââ   âââââââââââ\n",
            "ââââââââââââââââââââââââââââââââ ââââââââââââââââââ  âââââââââââ\n",
            "ââââââ  âââââââââ     âââ    âââââââ âââ   âââââââââ âââââââââ\n",
            "ââââââ  âââââââââ     âââ     âââââ  âââ   âââââââââââââââââââ\n",
            "âââ     ââââââ        âââ      âââ   ââââââââââââ ââââââââââââââ\n",
            "âââ     ââââââ        âââ      âââ    âââââââ âââ  âââââââââââââ v1.5.2\n",
            "\n",
            "If you're finding FiftyOne helpful, here's how you can get involved:\n",
            "\n",
            "|\n",
            "|  â­â­â­ Give the project a star on GitHub â­â­â­\n",
            "|  https://github.com/voxel51/fiftyone\n",
            "|\n",
            "|  ððð Join the FiftyOne Discord community ððð\n",
            "|  https://community.voxel51.com/\n",
            "|\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.session.session:\n",
            "Welcome to\n",
            "\n",
            "âââââââââââââââââââââââââââââââ   âââ âââââââ ââââ   âââââââââââ\n",
            "ââââââââââââââââââââââââââââââââ ââââââââââââââââââ  âââââââââââ\n",
            "ââââââ  âââââââââ     âââ    âââââââ âââ   âââââââââ âââââââââ\n",
            "ââââââ  âââââââââ     âââ     âââââ  âââ   âââââââââââââââââââ\n",
            "âââ     ââââââ        âââ      âââ   ââââââââââââ ââââââââââââââ\n",
            "âââ     ââââââ        âââ      âââ    âââââââ âââ  âââââââââââââ v1.5.2\n",
            "\n",
            "If you're finding FiftyOne helpful, here's how you can get involved:\n",
            "\n",
            "|\n",
            "|  â­â­â­ Give the project a star on GitHub â­â­â­\n",
            "|  https://github.com/voxel51/fiftyone\n",
            "|\n",
            "|  ððð Join the FiftyOne Discord community ððð\n",
            "|  https://community.voxel51.com/\n",
            "|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In FiftyOne, try filtering the output by the confidence of the classifier"
      ],
      "metadata": {
        "id": "2T0dxI6pOxQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(session.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "XmFf8X0oNTV9",
        "outputId": "391fa0c7-5ab8-419f-b780-32ee64d1fe73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5151-gpu-t4-hm-2a99y9azf59j9-c.us-east1-0.prod.colab.dev?polling=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYAyj4yuNXub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}